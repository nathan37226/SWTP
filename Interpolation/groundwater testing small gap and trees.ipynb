{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def makeNullRects(dates, y):\n",
    "    '''This function returns a list of matplotlib.patches.Rectangles where\n",
    "    np.nan values are present in the y array. If values are consecutive,\n",
    "    the rectangles will widen as needed.\n",
    "    Note that this function is made for a figure with an x-axis of dates\n",
    "    Input:\n",
    "        dates: x axis date time values\n",
    "        y: y axis range values as np.array, contains np.nan values\n",
    "\n",
    "    Returns:\n",
    "        list of matplotlib.patches.Rectangles located where\n",
    "        y has np.nan values.\n",
    "\n",
    "    Rectangle Parameters in function:\n",
    "        opacityCoeff: how solid rectangles appear\n",
    "        longRectColor: the color of the rectangles with >=7 width\n",
    "        shortRectColor: the color of the rectanges with <7 width\n",
    "    '''\n",
    "    # setting up rectangle parameters\n",
    "    opacityCoeff = 0.5\n",
    "    longRectColor = \"red\"\n",
    "    shortRectColor = \"magenta\"\n",
    "\n",
    "    # prep work for creating rectangles for nan values\n",
    "    index = 0\n",
    "    yMax = np.nanmax(y)\n",
    "    yMin = np.nanmin(y)\n",
    "    rectHeight = yMax - yMin\n",
    "    yRectCoor = yMin\n",
    "    allRects = []   # this is what will be returned\n",
    "\n",
    "    # creating rectangle patches\n",
    "    while index < len(y):\n",
    "\n",
    "        # if nan exists, then need to create a rectangle patch\n",
    "        if np.isnan(y[index]):\n",
    "            xRectCoorIndex = index - 1\n",
    "\n",
    "            # condition for if first y value is nan\n",
    "            if index == 0:\n",
    "                xRectCoorIndex += 1\n",
    "            \n",
    "            # condition for if last y value is nan, assumes y is not len 2\n",
    "            elif index + 1 == len(y):\n",
    "                xRectCoor = mdates.date2num(dates[xRectCoorIndex])\n",
    "                coords = (xRectCoor, yRectCoor)\n",
    "                width = mdates.date2num(dates[xRectCoorIndex + 1]) - mdates.date2num(dates[xRectCoorIndex])\n",
    "                allRects.append(mpatches.Rectangle(coords, width, rectHeight, color=shortRectColor, alpha=opacityCoeff))\n",
    "                break\n",
    "                \n",
    "            # all other cases\n",
    "            xRectCoor = mdates.date2num(dates[xRectCoorIndex])\n",
    "\n",
    "            # checking finding how long the rectangle needs to be--how many consecutive null values\n",
    "            index += 1\n",
    "            while np.isnan(y[index]):\n",
    "                index += 1\n",
    "            rightEdgeIndex = mdates.date2num(dates[index])\n",
    "\n",
    "            # making rectangle\n",
    "            coords = (xRectCoor, yRectCoor)\n",
    "            width = rightEdgeIndex - xRectCoor\n",
    "            color = shortRectColor\n",
    "            if index - xRectCoorIndex > 5:\n",
    "                color = longRectColor\n",
    "            allRects.append(mpatches.Rectangle(coords, width, rectHeight, color=color, alpha=opacityCoeff))\n",
    "\n",
    "        else:\n",
    "            index += 1\n",
    "\n",
    "    return allRects\n",
    "\n",
    "def visualizeMissingValues(dates, arr, fig, ax, wantToMakeNullRects = True):\n",
    "    '''This function plots an array of values with datetime x axis values onto\n",
    "    a given axis, showing patches of null values if present.\n",
    "\n",
    "    Input:\n",
    "        dates: a numpy array of datetime objs that are the x-axis for the array with missing data to plot\n",
    "        arr: a numpy array that has missing data\n",
    "        fig: a matplotlib figure that contains the axis with the plot\n",
    "        ax: a matplotlib axis that will be plotted upon\n",
    "\n",
    "    Returns:\n",
    "        fig: edited matplotlib figure\n",
    "        ax: edited matplotlib axis\n",
    "    '''\n",
    "    ax.plot(dates, arr)\n",
    "\n",
    "    if wantToMakeNullRects:\n",
    "        rects = makeNullRects(dates, arr)\n",
    "        for rect in rects:\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    formatter = mdates.ConciseDateFormatter(ax.xaxis.get_major_locator(), formats=[\"%Y\", \"%Y-%b\", \"%b-%d\", \"%d %H:%M\", \"%d %H:%M\", \"%H:%M\"])\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    return fig, ax\n",
    "\n",
    "def plotImputedData(dates, nullArr, imputedArr, ax):\n",
    "    '''This graph plots imputed data as a green dashed line on a given\n",
    "    matplotlib axis.\n",
    "\n",
    "    Input:\n",
    "        dates: a numpy array of datetime objs that are the x-axis for the array with missing data to plot\n",
    "        nullArr: a numpy array that has missing data\n",
    "        imputedArr: a numpy array that has some of the missing values imputed\n",
    "        ax: a matplotlib axis that will be plotted upon\n",
    "    \n",
    "    Returns:\n",
    "        ax: edited matplotlib axis\n",
    "    '''\n",
    "    index = 0\n",
    "    while index < len(nullArr):                                 # looping through arr since it has the null values\n",
    "        if np.isnan(nullArr[index]):\n",
    "            # getting the width of the null area\n",
    "            lenForward = 0\n",
    "            while np.isnan(nullArr[index + lenForward]):\n",
    "                lenForward += 1\n",
    "\n",
    "            # domain to plot is [index-1, index+lenforward]\n",
    "            domain = list(range(index-1, index+lenForward+1))\n",
    "            datesToPlot = [dates[i] for i in domain]\n",
    "            pointsToPlot = [imputedArr[i] for i in domain]\n",
    "            ax.plot(datesToPlot, pointsToPlot, \"g--\")       # green dashed line\n",
    "\n",
    "            # moving index forward past null gap\n",
    "            index += lenForward\n",
    "        else:\n",
    "            index += 1\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def findNulls(arr):\n",
    "    index = 0\n",
    "    pairs = []                  # formatted like [(start index, num values)]\n",
    "    while index < len(arr):\n",
    "        if np.isnan(arr[index]):\n",
    "            width = 1\n",
    "            try:\n",
    "                while np.isnan(arr[index + width]):\n",
    "                    width += 1\n",
    "            except IndexError:  # means end of array is null\n",
    "                break\n",
    "            pairs.append((index, width))\n",
    "            index += width\n",
    "        else:\n",
    "            index += 1\n",
    "\n",
    "    # for pair in pairs:\n",
    "    #     print(\"Null values starting at index: {i}. {w} total nulls\".format(i=pair[0], w=pair[1]))\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def imputeArrValues(index, arr, width):\n",
    "    '''Helper function for imputeArr, which is used inside smallGapImputation.\n",
    "    '''\n",
    "    smallGapFailed = False      # represents if there's a small enough gap but was just not able to fill it in due to missing data on either side\n",
    "    if width < 5 and index + width + 1 < len(arr):\n",
    "        # interpolate data!\n",
    "        # want 6 values before and after gap for cubic spline, but more might be better\n",
    "        # technically only 4 points are needed, but more might help impute better\n",
    "        lenForwards = 1\n",
    "        while lenForwards < 10 and not np.isnan(arr[index + width + lenForwards]):\n",
    "            lenForwards += 1\n",
    "\n",
    "        lenBackwards = 1\n",
    "        while lenBackwards < 10 and not np.isnan(arr[index - lenBackwards]):\n",
    "            lenBackwards += 1\n",
    "\n",
    "        # getting values set up for imputation\n",
    "        nullRange = list(range(index, index + width, 1))\n",
    "        totalRange = list(range(index - lenBackwards + 1, index + width + lenForwards, 1))\n",
    "        x = [x for x in totalRange if x not in nullRange]       # impution data points\n",
    "        y = [arr[i] for i in x]                                 # function values of impution data points\n",
    "        imputionRange = list(range(index, index + width, 1))\n",
    "\n",
    "        if lenForwards > 5 and lenBackwards > 5:\n",
    "            # cubic spline impution\n",
    "            cspline = CubicSpline(x, y)\n",
    "            for i in imputionRange:\n",
    "                arr[i] = cspline(i)         # replacing null values in array with interpolated values\n",
    "        \n",
    "        elif (lenForwards > 5 and lenBackwards > 2) or (lenForwards > 2 and lenBackwards > 5):\n",
    "            # cubic spline but data lies mostly on one end\n",
    "            # handles cases such as [x1, x2, x3, x4, x5, x6, nan, nan, nan, x7, x8, x9]\n",
    "            cspline = CubicSpline(x, y)\n",
    "            for i in imputionRange:\n",
    "                arr[i] = cspline(i)         # replacing null values in array with interpolated values\n",
    "        \n",
    "        elif width < 3:                     # not enough values preceeding and succeeding null gap for cubic spline, but null gap is small\n",
    "            # linear impution\n",
    "            linInterplator = interp1d(x, y)\n",
    "            for i in imputionRange:\n",
    "                arr[i] = linInterplator(i)         # replacing null values in array with interpolated values\n",
    "        else:\n",
    "            smallGapFailed = True\n",
    "\n",
    "    return arr, smallGapFailed\n",
    "\n",
    "def imputeArr(arr):\n",
    "    '''Helper function for smallGapImputation.\n",
    "    '''\n",
    "    # looping through each index in the array to find nulls\n",
    "    index = 0\n",
    "    reimputeColumn = False\n",
    "    while index < len(arr):\n",
    "        # a null has been found\n",
    "        if np.isnan(arr[index]):\n",
    "            # finding how many consecutive nulls are present\n",
    "            width = 1\n",
    "            while index + width < len(arr) and np.isnan(arr[index + width]):    #not reach the end and is still null\n",
    "                width += 1\n",
    "\n",
    "            # imputation happens with this helper function\n",
    "            arr, smallGapFailed = imputeArrValues(index, arr, width)\n",
    "            if smallGapFailed:\n",
    "                reimputeColumn = True\n",
    "\n",
    "            # move index forward past gap, continue searching and imputing\n",
    "            index += width\n",
    "\n",
    "        # no null gap, so continue searching\n",
    "        else:\n",
    "            index += 1\n",
    "\n",
    "    return arr, reimputeColumn\n",
    "\n",
    "def smallGapImputation(df):\n",
    "    '''This function takes in a dataframe that has null values present.\n",
    "    For each column, this function will attempt to fill in null gaps of size 5\n",
    "    or less with cubic spline impution or, if that's not available and the gap is < 3,\n",
    "    linear impution.\n",
    "\n",
    "    Input: pandas dataframe whose columns have null values, first column is timestamp\n",
    "    Returns: df with imputed data\n",
    "    '''\n",
    "    for col in df.columns[1:]:\n",
    "        # getting an array from the dataframe\n",
    "        arr = np.array(df[col])\n",
    "\n",
    "        # to run through a column multiple times if necessary\n",
    "        count = 0\n",
    "        while count < 5:\n",
    "            arr, reimputeColumn = imputeArr(arr)\n",
    "            if reimputeColumn:\n",
    "                count += 1\n",
    "                # print(\"Reimputing {col}\".format(col=col))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # replacing arr in df with arr with interpolated values\n",
    "        df[col] = arr\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def imputeSmallGaps():\n",
    "\n",
    "    # get original combined data with all null values\n",
    "    df = pd.read_csv(\"Joined Influent and Rainfall and Weather and Groundwater and Creek Gauge.csv\", parse_dates=[\"DateTime\"])\n",
    "    df[\"SWTP Total Influent Flow\"] = np.array([np.nan if x < 3.7 else x for x in df[\"SWTP Total Influent Flow\"]])\n",
    "\n",
    "    # imputing all small gaps with cubic splines and linear lines, gaps of size < 5\n",
    "    df = smallGapImputation(df)\n",
    "\n",
    "    # adding year, month, day, and hour columns\n",
    "    df[\"Year\"] = df[\"DateTime\"].dt.year\n",
    "    df[\"Month\"] = df[\"DateTime\"].dt.month\n",
    "    df[\"Week Day\"] = df[\"DateTime\"].dt.dayofweek\n",
    "    df[\"Hour\"] = df[\"DateTime\"].dt.hour\n",
    "    df[\"Week\"] = df[\"DateTime\"].dt.week\n",
    "\n",
    "    # saving imputed data\n",
    "    df.to_csv(\"Small Gap Imputed Data.csv\", index=False)\n",
    "\n",
    "imputeSmallGaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "def createIndicies(index, gapSize):\n",
    "    indicies = []\n",
    "    if gapSize % 2 != 0:\n",
    "        maxVal = int(gapSize/2) + index\n",
    "        minVal = int(gapSize/2) * -1 + index\n",
    "        indicies = [i for i in range(minVal, maxVal + 1)]\n",
    "    else:\n",
    "        maxVal = int(gapSize/2) + index\n",
    "        minVal = int(gapSize/2) * -1 + index + 1\n",
    "        indicies = [i for i in range(minVal, maxVal + 1)]\n",
    "    return indicies\n",
    "\n",
    "def testSmallSpot(arr, index, length):\n",
    "    if len(arr) - index > 25 and index > 25:    # not in last 25 or in first 25 indicies\n",
    "        # tests if 5 vals before and after index are null\n",
    "        # also if index is null\n",
    "        for i in range(length):\n",
    "            if np.isnan(arr[index + i]):\n",
    "                return False          \n",
    "            if np.isnan(arr[index - i]):\n",
    "                return False\n",
    "        return True                         # only if all are not null will this be hit\n",
    "    return False\n",
    "\n",
    "def testSmallGap(feature, count = 5, smallGapsPerTest = 30):\n",
    "    # getting data\n",
    "    df = pd.read_csv(\"Joined Influent and Rainfall and Weather and Groundwater and Creek Gauge.csv\", \n",
    "        usecols = [\"DateTime\", feature])\n",
    "    arr = np.array(df[feature])\n",
    "\n",
    "    # to remove sus values in a particular feature\n",
    "    if feature == \"SWTP Total Influent Flow\":\n",
    "        arr = np.array([np.nan if x < 3.7 else x for x in df[\"SWTP Total Influent Flow\"]])\n",
    "    nullArr = deepcopy(arr)                     # will be adding null values to here for validation\n",
    "\n",
    "    # starting validation\n",
    "    totalR = 0\n",
    "    breakCount = 0                              # in case not able to get as many desired spots\n",
    "    for i in range(count):                      # average of how many validation tests\n",
    "\n",
    "        spots = []                              # append all initial indicies to be turned null here\n",
    "        validationIndicies = []                 # append all indicies forced to null here\n",
    "        \n",
    "        # getting the spots to make null\n",
    "        while len(spots) < smallGapsPerTest:\n",
    "            randIndex = np.random.randint(0, len(arr))\n",
    "\n",
    "            # testing if the randomly generated index is a valid spot\n",
    "            if testSmallSpot(nullArr, randIndex, 7):\n",
    "                spots.append(randIndex)\n",
    "                nullGapWidth = np.random.randint(1, 5) # either 1, 2, 3, or 4\n",
    "\n",
    "                # making a null gap where data was previously\n",
    "                indiciesToTurnNull = createIndicies(randIndex, nullGapWidth)\n",
    "                for i in indiciesToTurnNull:\n",
    "                    nullArr[i] = np.nan\n",
    "                    validationIndicies.append(i)\n",
    "\n",
    "            # in case while loop is infinite\n",
    "            if breakCount > 5000000:             # just some large number\n",
    "                raise NotImplementedError(\"Failed to create all small null gaps\")\n",
    "        \n",
    "            breakCount += 1\n",
    "        \n",
    "        # inputing new array with created null values\n",
    "        df[feature] = nullArr\n",
    "\n",
    "        # imputing and getting r2 values\n",
    "        df = smallGapImputation(df)\n",
    "        imputedArr = df[feature]\n",
    "\n",
    "        prevValues = [arr[i] for i in validationIndicies]\n",
    "        imputedValues = [imputedArr[i] for i in validationIndicies]\n",
    "        totalR += r2_score(prevValues, imputedValues)\n",
    "    # print(\"Avg r^2 for {col} is: \\n{val}\".format(col = feature, val = totalR / count))\n",
    "\n",
    "    return totalR / count\n",
    "\n",
    "def performSmallGapValidation(featDf):\n",
    "    avgR2Vals = []\n",
    "    allFeatures = np.array(featDf[\"Feature\"])\n",
    "    for feature in allFeatures:\n",
    "    # for feature in [\"Blackman 96 Hour Rainfall Aggregate\", \"Blackman 120 Hour Rainfall Aggregate\"]:\n",
    "        print(feature)\n",
    "        avgR2Vals.append(testSmallGap(feature, 10, 100))       # r2 is avg of 15 tests, 25 small null gaps created per test\n",
    "    print(avgR2Vals)\n",
    "    # featDf[\"Avg R2\"] = avgR2Vals\n",
    "    # featDf.to_csv(\"Validated Features.csv\", index=False)\n",
    "\n",
    "    return featDf\n",
    "\n",
    "# featDf = pd.read_csv(\"Features.csv\")\n",
    "# featDf = performSmallGapValidation(featDf)\n",
    "# print(featDf)\n",
    "\n",
    "# df = pd.read_csv(\"Validated Features.csv\")\n",
    "df = pd.read_csv(\"Filtered Validated Features.csv\")\n",
    "df.plot.bar(x=\"Feature\", y=\"Avg R2\", rot=37)\n",
    "# plt.bar([i for i in range(len(df[\"Feature\"]))], df[\"Avg R2\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dcor import distance_correlation\n",
    "\n",
    "def getCorrelationPerFeature(df, targetFeature):\n",
    "    targetArr = np.array(df[targetFeature])\n",
    "    targetNullLocations = np.nonzero(np.isnan(targetArr))[0]\n",
    "\n",
    "    correlationList = []\n",
    "    cols = [col for col in df.columns if col not in [\"DateTime\", targetFeature]]\n",
    "    for col in cols:\n",
    "        # gettiing column as array and null locations\n",
    "        arr = np.array(df[col])\n",
    "        arrNullLocations = np.nonzero(np.isnan(arr))[0]\n",
    "        allNullLocations = np.unique(np.append(targetNullLocations, arrNullLocations))\n",
    "\n",
    "        # removing null indicies\n",
    "        currentTargetArr = np.delete(targetArr, allNullLocations)\n",
    "        arr = np.delete(arr, allNullLocations)\n",
    "\n",
    "        # computing \n",
    "        correlationValue = distance_correlation(currentTargetArr, arr)\n",
    "        correlationList.append((col, correlationValue))\n",
    "\n",
    "    correlationList.sort(key=lambda a: a[1])\n",
    "    correlationList = correlationList[::-1]\n",
    "    # corrDf = pd.DataFrame(np.array(correlationList), columns = [\"Feature\", \"Correlation with Target\"])\n",
    "    # print(corrDf)\n",
    "    \n",
    "    return correlationList\n",
    "\n",
    "targetFeature = \"Ozark Aquifer Depth to Water Level (ft)\"\n",
    "df = pd.read_csv(\"Small Gap Imputed Data.csv\")\n",
    "correlationList = getCorrelationPerFeature(df, targetFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlationList = [('Springfield Plateau Aquifer Depth to Water Level (ft)', 0.6307472041607479), ('Week', 0.5765419244542733), ('Month', 0.5726279226642098), ('James Gauge Height (ft)', 0.5636879927117522), ('SWTP Total Influent Flow', 0.533093275904417), ('SWTP Plant 2 Influent Flow', 0.5205061617966512), ('Wilsons Gauge Height (ft)', 0.43951761698951675), ('SWTP Plant 1 Influent Flow', 0.3815965878440867), ('Sequiota 168 Hour Rainfall Aggregate', 0.3790800777831508), ('Sequiota 144 Hour Rainfall Aggregate', 0.35859381995089634), ('Sequiota 120 Hour Rainfall Aggregate', 0.3361124951332317), ('Year', 0.33475684589322774), ('Sequiota 96 Hour Rainfall Aggregate', 0.30806467669381055), ('Republic 168 Hour Rainfall Aggregate', 0.2985548939998324), ('AT&T 168 Hour Rainfall Aggregate', 0.29750850751625374), ('Hiland 168 Hour Rainfall Aggregate', 0.29675139217766533), ('Field 168 Hour Rainfall Aggregate', 0.2924034951313501), ('Willard 168 Hour Rainfall Aggregate', 0.29100906059925974), ('NW 168 Hour Rainfall Aggregate', 0.2855386912527847), ('Pittman 168 Hour Rainfall Aggregate', 0.2831864693005713), ('Hiland 144 Hour Rainfall Aggregate', 0.2823723855497944), ('Westport 168 Hour Rainfall Aggregate', 0.2823710115408092), ('Republic 144 Hour Rainfall Aggregate', 0.28043329631822683), ('AT&T 144 Hour Rainfall Aggregate', 0.2795171504344931), ('Waste 168 Hour Rainfall Aggregate', 0.27939688488900466), ('Pleasant 168 Hour Rainfall Aggregate', 0.2785139833307677), ('Weller 168 Hour Rainfall Aggregate', 0.27460603981370146), ('Field 144 Hour Rainfall Aggregate', 0.27458970133859273), ('Willard 144 Hour Rainfall Aggregate', 0.27445281575912894), ('Millwood 168 Hour Rainfall Aggregate', 0.2744424049919476), ('Sequiota 72 Hour Rainfall Aggregate', 0.2721167731721901), ('Rutledge 168 Hour Rainfall Aggregate', 0.2707097493453225), ('Fire 168 Hour Rainfall Aggregate', 0.2702133951630731), ('Sherwood 168 Hour Rainfall Aggregate', 0.26970477387268893), ('NW 144 Hour Rainfall Aggregate', 0.26932598698918603), ('Strafford 168 Hour Rainfall Aggregate', 0.26896867391737117), ('Disney 168 Hour Rainfall Aggregate', 0.2686604900200778), ('James 168 Hour Rainfall Aggregate', 0.26831648686722814), ('Bingham 168 Hour Rainfall Aggregate', 0.26803556971653875), ('Hiland 120 Hour Rainfall Aggregate', 0.2662906398387919), ('Pittman 144 Hour Rainfall Aggregate', 0.26579546912939356), ('Westport 144 Hour Rainfall Aggregate', 0.2653438267516973), ('Total 168 Hour Rainfall Aggregate', 0.2651211732477124), ('Willard_Intermediate 168 Hour Rainfall Aggregate', 0.2646803359843156), ('Sunshine 168 Hour Rainfall Aggregate', 0.26411604203806427), ('Valley 168 Hour Rainfall Aggregate', 0.2640368782184995), ('Pleasant 144 Hour Rainfall Aggregate', 0.2628851990562218), ('Le 168 Hour Rainfall Aggregate', 0.2626605734648382), ('Waste 144 Hour Rainfall Aggregate', 0.2626302529722998), ('Jefferies 168 Hour Rainfall Aggregate', 0.26186429498121505), ('Republic 120 Hour Rainfall Aggregate', 0.2601934265389541), ('AT&T 120 Hour Rainfall Aggregate', 0.26014802067271886), ('Roundtree 168 Hour Rainfall Aggregate', 0.2599020729344486), ('Millwood 144 Hour Rainfall Aggregate', 0.2580556435308486), ('Weller 144 Hour Rainfall Aggregate', 0.25783907998665784), ('Willard 120 Hour Rainfall Aggregate', 0.25663979900923933), ('Rutledge 144 Hour Rainfall Aggregate', 0.25599928879469974), ('Field 120 Hour Rainfall Aggregate', 0.2553522837563216), ('Strafford 144 Hour Rainfall Aggregate', 0.25500714120086543), ('James 144 Hour Rainfall Aggregate', 0.2541510307111001), ('Fire 144 Hour Rainfall Aggregate', 0.25404192724171476), ('Sherwood 144 Hour Rainfall Aggregate', 0.2539436667284397), \n",
    "('Disney 144 Hour Rainfall Aggregate', 0.2530527935941957), ('Bingham 144 Hour Rainfall Aggregate', 0.25210047154875603), ('NW 120 Hour Rainfall Aggregate', 0.25176045049815754), ('Total 144 Hour Rainfall Aggregate', 0.2517075864653168), ('Valley 144 Hour Rainfall Aggregate', 0.24958346865522912), ('Willard_Intermediate 144 Hour Rainfall Aggregate', 0.2490556283200971), ('Sunshine 144 Hour Rainfall Aggregate', 0.24852355557803882), ('Le 144 Hour Rainfall Aggregate', 0.24810669887320116), ('Pittman 120 Hour Rainfall Aggregate', 0.24749707982179972), ('Westport 120 Hour Rainfall Aggregate', 0.24700697344444747), ('Jefferies 144 Hour Rainfall Aggregate', 0.24665109623495626), ('Hiland 96 Hour Rainfall Aggregate', 0.24637857176725605), ('Pleasant 120 Hour Rainfall Aggregate', 0.24621414284580512), ('Waste 120 Hour Rainfall Aggregate', 0.2448116890440284), ('Roundtree 144 Hour Rainfall Aggregate', 0.24385020938943144), ('Cherokee 168 Hour Rainfall Aggregate', 0.24153369976082234), ('Shady 168 Hour Rainfall Aggregate', 0.24067834451030065), ('Weller 120 Hour Rainfall Aggregate', 0.2403943416513641), ('Rutledge 120 Hour Rainfall Aggregate', 0.24035668519467035), \n",
    "('Millwood 120 Hour Rainfall Aggregate', 0.239524347840239), ('Strafford 120 Hour Rainfall Aggregate', 0.2390755449468274), ('James 120 Hour Rainfall Aggregate', 0.2375659021974559), ('Sherwood 120 Hour Rainfall Aggregate', 0.23720134611318294), ('Fire 120 Hour Rainfall Aggregate', 0.23660288180650696), ('Airport 168 Hour Rainfall Aggregate', 0.23638541268319654), ('AT&T 96 Hour Rainfall Aggregate', 0.23630281835000333), ('Total 120 Hour Rainfall Aggregate', 0.2362648049132383), ('Republic 96 Hour Rainfall Aggregate', 0.2361654773917245), ('Disney 120 Hour Rainfall Aggregate', 0.23612157117533117), ('Bingham 120 Hour Rainfall Aggregate', 0.23532812910093917), ('Willard 96 Hour Rainfall Aggregate', 0.23489417683946853), ('Valley 120 Hour Rainfall Aggregate', 0.2343995447840297), ('Le 120 Hour Rainfall Aggregate', 0.23261859952637387), ('Field 96 Hour Rainfall Aggregate', 0.23243894817469085), ('Willard_Intermediate 120 Hour Rainfall Aggregate', 0.23234485570461447), ('Sunshine 120 Hour Rainfall Aggregate', 0.23231825501152442), ('NW 96 Hour Rainfall Aggregate', 0.23099448165634265), ('English 168 Hour Rainfall Aggregate', 0.23014656557358623), ('Jefferies 120 Hour Rainfall Aggregate', 0.22987353316879236), ('Cherokee 144 Hour Rainfall Aggregate', 0.2270747669766974), ('Pleasant 96 Hour Rainfall Aggregate', 0.22659688376348727), ('Shady 144 Hour Rainfall Aggregate', 0.22651379330817617), ('Roundtree 120 Hour Rainfall Aggregate', 0.22635119554380428), ('Airport 144 Hour Rainfall Aggregate', 0.2250796367732096), ('Westport 96 Hour Rainfall Aggregate', 0.22483879809254997), ('Pittman 96 Hour Rainfall Aggregate', 0.22479551237034942), ('Waste 96 Hour Rainfall Aggregate', 0.22393427553415146), ('SWTP Plant 1 Gravity Flow', 0.22281688798394458), ('English 144 Hour Rainfall Aggregate', 0.22246424915152896), ('Rutledge 96 Hour Rainfall Aggregate', 0.22098227058024936), ('Hiland 72 Hour Rainfall Aggregate', 0.22007452895313404), ('Weller 96 Hour Rainfall Aggregate', 0.21919715915412416), ('Strafford 96 Hour Rainfall Aggregate', 0.21886583596037126), ('Total 96 Hour Rainfall Aggregate', 0.21742536634327664), ('Millwood 96 Hour Rainfall Aggregate', 0.2167716490321167), ('Sherwood 96 Hour Rainfall Aggregate', 0.216436510730393), ('James 96 Hour Rainfall Aggregate', 0.21592691132062838), ('Disney 96 Hour Rainfall Aggregate', 0.21571571232830455), ('Valley 96 Hour Rainfall Aggregate', 0.21539586767638869), ('Fire 96 Hour Rainfall Aggregate', 0.21509328189990626), ('Bingham 96 Hour Rainfall Aggregate', 0.2146857482148008), ('Le 96 Hour Rainfall Aggregate', 0.21357119459298754), ('Willard_Intermediate 96 Hour Rainfall Aggregate', 0.21300159517117412), ('Airport 120 Hour Rainfall Aggregate', 0.21244731066372874), ('Sunshine 96 Hour Rainfall Aggregate', 0.21240589104352825), ('Shady 120 Hour Rainfall Aggregate', 0.21158403461127104), ('English 120 Hour Rainfall Aggregate', 0.21125453984434878), ('Cherokee 120 Hour Rainfall Aggregate', 0.21068024178715256), ('Jefferies 96 Hour Rainfall Aggregate', 0.2093168536252737), ('Willard 72 Hour Rainfall Aggregate', 0.20760409983942765), ('AT&T 72 Hour Rainfall Aggregate', 0.2068192589103986), ('Republic 72 Hour Rainfall Aggregate', 0.20681367066403575), ('Roundtree 96 Hour Rainfall Aggregate', 0.2058601464175766), ('Field 72 Hour Rainfall Aggregate', 0.20437893840974616), ('NW 72 Hour Rainfall Aggregate', 0.20426445546022257), ('Airport_West 168 Hour Rainfall Aggregate', 0.2021557768059795), ('Pleasant 72 Hour Rainfall Aggregate', 0.20106420218329538), ('Waste 72 Hour Rainfall Aggregate', 0.197007686208034), ('Pittman 72 Hour Rainfall Aggregate', 0.1969301250353878), ('Westport 72 Hour Rainfall Aggregate', 0.19627575740380288), ('Airport 96 Hour Rainfall Aggregate', 0.19614977110207302), ('English 96 Hour Rainfall Aggregate', 0.1956702861951137), ('Rutledge 72 Hour Rainfall Aggregate', 0.19488770677348685), ('Strafford 72 Hour Rainfall Aggregate', 0.19386122089609864), ('Shady 96 Hour Rainfall Aggregate', 0.19328283360312926), ('Total 72 Hour Rainfall Aggregate', 0.1931401943341729), ('Weller 72 Hour Rainfall Aggregate', 0.19245475043649352), ('Cherokee 96 Hour Rainfall Aggregate', 0.19137376678409568), ('Valley 72 Hour Rainfall Aggregate', 0.19054834109648455), ('Airport_West 144 Hour Rainfall Aggregate', 0.19054201029338988), ('Disney 72 Hour Rainfall Aggregate', 0.19012695788202627), ('Sherwood 72 Hour Rainfall Aggregate', 0.1898180708397562), ('Le 72 Hour Rainfall Aggregate', 0.1891027941299336), ('Millwood 72 Hour Rainfall Aggregate', 0.18907934020622533), ('Willard_Intermediate 72 Hour Rainfall Aggregate', 0.1884987405617409), ('James 72 Hour Rainfall Aggregate', 0.1883429155017829), ('Bingham 72 Hour Rainfall Aggregate', 0.18832798282147675), ('Fire 72 Hour Rainfall Aggregate', 0.18801180220100128), ('Sunshine 72 Hour Rainfall Aggregate', 0.1867367317882274), ('Jefferies 72 Hour Rainfall Aggregate', 0.1835802593835982), ('Roundtree 72 Hour Rainfall Aggregate', 0.18108560664929352), ('Airport_West 120 Hour Rainfall Aggregate', 0.17803936392471822), ('Airport 72 Hour Rainfall Aggregate', 0.1753393818083737), ('English 72 Hour Rainfall Aggregate', 0.17399142700541345), ('Shady 72 Hour Rainfall Aggregate', 0.17001773181148486), ('Cherokee 72 Hour Rainfall Aggregate', 0.16759641015713841), \n",
    "('Airport_West 96 Hour Rainfall Aggregate', 0.16317252836079457), ('SW_Peak_Flow', 0.1616236444894937), ('HourlyStationPressure', 0.15271579902818724), ('HourlyAltimeterSetting', 0.1519876727661242), ('River 168 Hour Rainfall Aggregate', 0.1498452011767934), ('HourlySeaLevelPressure', 0.14718987868136907), ('Airport_West 72 Hour Rainfall Aggregate', 0.14531536260511274), ('River 144 Hour Rainfall Aggregate', 0.13970922022769972), ('Airport_Springfield 168 Hour Rainfall Aggregate', 0.13729891459175309), ('Mark 168 Hour Rainfall Aggregate', 0.13176908834283665), ('River 120 Hour Rainfall Aggregate', 0.12935352844065576), ('Airport_Springfield 144 Hour Rainfall Aggregate', 0.12910769031762823), ('Mark 144 Hour Rainfall Aggregate', 0.12396599583614991), ('Airport_Springfield 120 Hour Rainfall Aggregate', 0.11991303578479541), ('River 96 Hour Rainfall Aggregate', 0.11694787461798342), ('Mark 120 Hour Rainfall Aggregate', 0.11531315043345582), ('Airport_Springfield 96 Hour Rainfall Aggregate', 0.10988786858045745), ('Mark 96 Hour Rainfall Aggregate', 0.105340728223974), ('River 72 Hour Rainfall Aggregate', 0.10232202884542511), ('Airport_Springfield 72 Hour Rainfall Aggregate', 0.0967608660270247), ('Mark 72 Hour Rainfall Aggregate', 0.09337510242949196), ('HourlyWetBulbTemperature', 0.09059418198330822), ('HourlyDryBulbTemperature', 0.08819100260972183), ('HourlyDewPointTemperature', 0.08777041151979946), ('Blackman 168 Hour Rainfall Aggregate', 0.08765474719145999), ('Blackman 144 Hour Rainfall Aggregate', 0.083155716834374), ('Blackman 120 Hour Rainfall Aggregate', 0.07841367288398636), ('Blackman 96 Hour Rainfall Aggregate', 0.07257553890422525), ('Sequiota Rainfall (in)', 0.06750777069010394), ('Blackman 72 Hour Rainfall Aggregate', 0.0651078109755356), ('NW Rainfall (in)', 0.05543693777927901), ('Rutledge Rainfall (in)', 0.05381348224029626), ('Hiland Rainfall (in)', 0.053696110380969886), ('James Rainfall (in)', 0.05305243382951508), ('Willard Rainfall (in)', 0.05220670432854757), ('Total Rainfall (in)', 0.05184139433221512), ('Pleasant Rainfall (in)', 0.05145325676525972), ('Republic Rainfall (in)', 0.05105062435299764), ('Valley Rainfall (in)', 0.05014734318512089), ('AT&T Rainfall (in)', 0.049703125829297604), ('Westport Rainfall (in)', 0.04892283315594852), ('Willard_Intermediate Rainfall (in)', 0.04850189246638779), ('Strafford Rainfall (in)', 0.04846012476350796), ('Le Rainfall (in)', 0.0482686296380321), ('Field Rainfall (in)', 0.04806007936708993), ('Sherwood Rainfall (in)', 0.04750710701739062), ('Millwood Rainfall (in)', 0.047282632930919485), ('Fire Rainfall (in)', 0.04700055987379547), ('Waste Rainfall (in)', 0.046909445387607156), ('Bingham Rainfall (in)', 0.04670387814383212), ('Sunshine Rainfall (in)', 0.046362475221268816), ('Weller Rainfall (in)', 0.04605963344976909), ('Pittman Rainfall (in)', 0.04502803967879383), ('Jefferies Rainfall (in)', 0.044918085631140525), ('Disney Rainfall (in)', 0.04435527020946567), ('Roundtree Rainfall (in)', 0.04332548479514482), ('Airport Rainfall (in)', 0.04248003034330316), ('Shady Rainfall (in)', 0.04166010289902156), ('Williams 72 Hour Rainfall Aggregate', 0.041383981013486656), ('Williams 96 Hour Rainfall Aggregate', 0.04098745824144807), ('Williams 168 Hour Rainfall Aggregate', 0.04091676146336944), ('Williams 144 Hour Rainfall Aggregate', 0.04091676146336944), ('Williams 120 Hour Rainfall Aggregate', 0.04091676146336944), ('English Rainfall (in)', 0.03989512268847416), ('Cherokee Rainfall (in)', 0.038858059014430485), ('HourlyRelativeHumidity', 0.037892409365553645), ('Airport_West Rainfall (in)', 0.0358809369705908), ('Airport_Springfield Rainfall (in)', 0.03318206211893972), ('River Rainfall (in)', 0.03160756583078008), ('HourlyWindSpeed', 0.02884656752918485), ('HourlyVisibility', 0.025626272725796147), ('Mark Rainfall (in)', 0.020894563511283545), ('Blackman Rainfall (in)', 0.015501730294682148), ('Williams Rainfall (in)', 0.01328631789601823), ('HourlyPressureChange', 0.012152893951929169), ('HourlyPressureTendency', 0.009456778773684667), ('Week Day', 0.006847159600733715), ('Hour', 0.0018620492588146615)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "%matplotlib qt\n",
    "\n",
    "def findNulls(arr):\n",
    "    index = 0\n",
    "    pairs = []                  # formatted like [(start index, num values)]\n",
    "    while index < len(arr):\n",
    "        if np.isnan(arr[index]):\n",
    "            width = 1\n",
    "            try:\n",
    "                while np.isnan(arr[index + width]):\n",
    "                    width += 1\n",
    "            except IndexError:  # means end of array is null\n",
    "                break\n",
    "            pairs.append((index, width))\n",
    "            index += width\n",
    "        else:\n",
    "            index += 1\n",
    "\n",
    "    # for pair in pairs:\n",
    "    #     print(\"Null values starting at index: {i}. {w} total nulls\".format(i=pair[0], w=pair[1]))\n",
    "    return pairs\n",
    "\n",
    "def createIndicies(index, gapSize):\n",
    "    indicies = []\n",
    "    if gapSize % 2 != 0:\n",
    "        maxVal = int(gapSize/2) + index\n",
    "        minVal = int(gapSize/2) * -1 + index\n",
    "        indicies = [i for i in range(minVal, maxVal + 1)]\n",
    "    else:\n",
    "        maxVal = int(gapSize/2) + index\n",
    "        minVal = int(gapSize/2) * -1 + index + 1\n",
    "        indicies = [i for i in range(minVal, maxVal + 1)]\n",
    "    return indicies\n",
    "\n",
    "def testLargeSpot(arr, index, length):\n",
    "    if len(arr) - index > length and index > length: \n",
    "        # tests if 5 vals before and after index are null\n",
    "        # also if index is null\n",
    "        for i in range(length):\n",
    "            if np.isnan(arr[index + i]):\n",
    "                return False          \n",
    "            if np.isnan(arr[index - i]):\n",
    "                return False\n",
    "        return True                         # only if all are not null will this be hit\n",
    "    return False\n",
    " \n",
    "def createLargeGapIndicies(arr, test_size):\n",
    "    existingLargeNullGapLengths = [x[1] for x in findNulls(arr) if x[1] > 5]\n",
    "    minGapSize = 5\n",
    "    maxGapSize = max(existingLargeNullGapLengths, default=150)\n",
    "    spots = []                              # append all initial indicies to be turned null here\n",
    "    validationIndicies = []                 # append all indicies forced to null here\n",
    "    nullArr = deepcopy(arr)                     # will be adding null values to here for validation\n",
    "\n",
    "    # getting the spots to make null\n",
    "    breakout = 0\n",
    "    totalCreatedNulls = 0\n",
    "    hasLargestGap = False\n",
    "    while totalCreatedNulls / len(arr) < test_size and breakout < 500000:\n",
    "        # randomly getting index and how large of gap to create\n",
    "        randIndex = np.random.randint(0, len(arr))\n",
    "        randGapSize = np.random.randint(minGapSize, maxGapSize)\n",
    "        if not hasLargestGap:\n",
    "            randGapSize = maxGapSize\n",
    "            hasLargestGap = True\n",
    "\n",
    "        # testing if spot is valid\n",
    "        if testLargeSpot(nullArr, randIndex, randGapSize):\n",
    "            spots.append(randIndex)\n",
    "            totalCreatedNulls += randGapSize\n",
    "            \n",
    "            # making a null gap where data was previously\n",
    "            indiciesToTurnNull = createIndicies(randIndex, randGapSize)\n",
    "            for i in indiciesToTurnNull:\n",
    "                nullArr[i] = np.nan\n",
    "                validationIndicies.append(i)\n",
    "\n",
    "        breakout += 1\n",
    "    return validationIndicies\n",
    "\n",
    "def train_test_split_largeGap(data, target, test_size = 0.1):\n",
    "    trainX, trainY = deepcopy(data), deepcopy(target)\n",
    "    testX, testY = [], []\n",
    "    validationIndicies = np.sort(np.array(createLargeGapIndicies(target, test_size)))\n",
    "    for index in validationIndicies:\n",
    "        testX.append(data[index])\n",
    "        testY.append(target[index])\n",
    "    testX, testY = np.array(testX), np.array(testY)\n",
    "    trainX = np.delete(trainX, validationIndicies, 0)\n",
    "    trainY = np.delete(trainY, validationIndicies)\n",
    "    return trainX, testX, trainY, testY, validationIndicies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33817, 19) (8496, 19) (33817,) (8496,)\n",
      "10.27143141503484\n",
      "0.6648923481717854 with 9 features, 10 max depth, 75 trees, and a scale factor of 0.25\n",
      "0.6685057521237845 with 9 features, 10 max depth, 75 trees, and a scale factor of 0.5\n",
      "0.6703947765715775 with 9 features, 10 max depth, 75 trees, and a scale factor of 0.75\n",
      "0.6704403089134804 with 9 features, 10 max depth, 75 trees, and a scale factor of 1\n",
      "0.8686363960074946 with 9 features, 10 max depth, 75 trees, and a scale factor of 0.1\n",
      "Best hyperparas are: 9 features, 10 depth, and 75 trees\n",
      "With an MSE of: 0.6648923481717854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def saveForestPredictions(originFilename, predFilename, feature):\n",
    "    # building dataframes\n",
    "    originDf = pd.read_csv(originFilename, parse_dates=[\"DateTime\"])\n",
    "    predDf = pd.read_csv(predFilename, parse_dates=[\"DateTime\"])\n",
    "\n",
    "    # getting arrays from dataframes\n",
    "    predArr = np.array(predDf[predDf.columns[-1]])\n",
    "    predDates = np.array(predDf[\"DateTime\"])\n",
    "    originArr = np.array(originDf[\"Ozark Aquifer Depth to Water Level (ft)\"])\n",
    "    originDates = np.array(originDf[\"DateTime\"])\n",
    "\n",
    "    # copying over predicted values\n",
    "    for i in range(len(predDates)):\n",
    "        locations = np.nonzero(originDates == predDates[i])\n",
    "        originArr[locations[0]] = predArr[i]\n",
    "\n",
    "    # replacing array in df and saving to csv\n",
    "    df[feature] = originArr\n",
    "    df.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "def findSharedNullValueFeatures(df, targetFeature, tol = 0.5):\n",
    "    # getting location of target nulls\n",
    "    target = np.array(df[targetFeature])\n",
    "    targetNulls = np.nonzero(np.isnan(target))[0]\n",
    "\n",
    "    # getting columns\n",
    "    columnList = [x for x in df.columns if x not in [\"DateTime\", targetFeature]]\n",
    "    badColumns = []\n",
    "    for col in columnList:\n",
    "        # getting how many null values are shared between target and each feature\n",
    "        arr = np.array(df[col])\n",
    "        arrNulls = np.nonzero(np.isnan(arr))[0]\n",
    "        sharedNullIncidiesCount = len(np.intersect1d(targetNulls, arrNulls))\n",
    "        \n",
    "        # if share tol% or more null features, then leave that feature out\n",
    "        if sharedNullIncidiesCount > tol * len(targetNulls):\n",
    "            badColumns.append(col)\n",
    "\n",
    "    return badColumns\n",
    "\n",
    "def separateDataIntoSets(target, data, dates):\n",
    "    # getting null locations from target feature array\n",
    "    targetNulls = np.nonzero(np.isnan(target))[0]\n",
    "\n",
    "    # getting data corresponding to where the target feature is null\n",
    "    testData = []\n",
    "    testDates = []\n",
    "    for i in targetNulls:\n",
    "        testData.append(data[i])\n",
    "        testDates.append(dates[i])\n",
    "    testData = np.array(testData)\n",
    "    testDates = np.array(testDates)\n",
    "\n",
    "    # deleting all indicies that are null from target and data\n",
    "    trainTarget = np.delete(target, targetNulls)\n",
    "    trainData = np.delete(data, targetNulls, 0)     # the 0 means delete a row\n",
    "\n",
    "    # finding where null values are present in the data that has a not null target corresponding to it\n",
    "    badIndicies = []\n",
    "    for col in range(len(trainData[0])):\n",
    "        for row in range(len(trainData)):\n",
    "            if np.isnan(trainData[row][col]):\n",
    "                badIndicies.append(row)\n",
    "\n",
    "    # removing those indicies so that rand forest can train\n",
    "    if len(badIndicies) > 0:\n",
    "        badIndicies = np.unique(np.array(badIndicies))\n",
    "        trainData = np.delete(trainData, badIndicies, 0)\n",
    "        trainTarget = np.delete(trainTarget, badIndicies)\n",
    "\n",
    "    # finding where null values are present in the data that has a null target corresponding to it\n",
    "    badIndicies = []\n",
    "    for col in range(len(testData[0])):\n",
    "        for row in range(len(testData)):\n",
    "            if np.isnan(testData[row][col]):\n",
    "                badIndicies.append(row)\n",
    "    \n",
    "    # removing those indicies so that rand forest can predict\n",
    "    if len(badIndicies) > 0:\n",
    "        badIndicies = np.unique(np.array(badIndicies))\n",
    "        testData = np.delete(testData, badIndicies, 0)\n",
    "        testDates = np.delete(testDates, badIndicies)\n",
    "    \n",
    "    return trainData, trainTarget, testData, testDates\n",
    "\n",
    "def scalePredictedValues(dates, fullTarget, predictedValues, predictedIndicies, scaleFactor):\n",
    "    # copying over predicted values\n",
    "    fullPredTarget = deepcopy(fullTarget)\n",
    "    validNullTarget = deepcopy(fullTarget)                 # used in linear scaling step to find where to scale\n",
    "    predDates = [dates[i] for i in predictedIndicies]\n",
    "    for i in range(len(predDates)):\n",
    "        locations = np.nonzero(dates == predDates[i])\n",
    "        fullPredTarget[locations[0]] = predictedValues[i]\n",
    "        validNullTarget[locations[0]] = np.nan\n",
    "\n",
    "    # linear scaling to trendline\n",
    "    scalingSpots = findNulls(validNullTarget)               # formatted as [(index, gap size)]\n",
    "    count = 0\n",
    "    for tup in scalingSpots:\n",
    "        # getting points to make linear trendline\n",
    "        points = []\n",
    "        for i in range(10):\n",
    "            xCorBefore = tup[0] - i\n",
    "            yCorBefore = fullPredTarget[xCorBefore]\n",
    "            xCorAfter = tup[0] + tup[1] + i\n",
    "            yCorAfter = fullPredTarget[xCorAfter]\n",
    "            points.append((xCorBefore, yCorBefore))\n",
    "            points.append((xCorAfter, yCorAfter))\n",
    "        \n",
    "        # creating trendline\n",
    "        trendlineCoeffs = np.polyfit([p[0] for p in points],[p[1] for p in points], 1)\n",
    "        trendline = np.poly1d(trendlineCoeffs)\n",
    "        \n",
    "        # scaling predicted values\n",
    "        scalePredictedValues = deepcopy(predictedValues)\n",
    "        for i in range(tup[1]):         # for how many indicies in gap\n",
    "            predictedValues[count] = trendline(tup[0] + i) + scaleFactor * (fullPredTarget[tup[0] + i] - trendline(tup[0] + i))\n",
    "            count += 1\n",
    "        \n",
    "    return scalePredictedValues\n",
    "\n",
    "def tuneForest(df, targetFeature):\n",
    "    # getting data to use\n",
    "    target = np.array(df[targetFeature])\n",
    "    dates = np.array(df[\"DateTime\"])\n",
    "    badFeaturesToUse = findSharedNullValueFeatures(df, targetFeature)\n",
    "    badFeaturesToUse += [targetFeature, \"DateTime\"]\n",
    "    df = df.drop(columns=badFeaturesToUse)\n",
    "    data = df.to_numpy()\n",
    "\n",
    "    # splitting data up into respective datasets\n",
    "    validData, validTarget, nullData, nullDates = separateDataIntoSets(target, data, dates)\n",
    "\n",
    "    # splitting up known data into training and validation sets\n",
    "    # XTrain, XTest, YTrain, YTest = train_test_split(validData, validTarget, test_size=0.2)\n",
    "    XTrain, XTest, YTrain, YTest, testIndicies = train_test_split_largeGap(validData, validTarget, test_size=0.20)\n",
    "    print(np.shape(XTrain), np.shape(XTest), np.shape(YTrain), np.shape(YTest))\n",
    "\n",
    "    # setting up possible hyperparameter values\n",
    "    # maxNumFeatures = list(range(3, int(len(df.columns)/1.75) + 1, 2))\n",
    "    # maxDepths = [3, 5, 7, 10]\n",
    "    # numTrees = [50, 75, 100]\n",
    "    maxNumFeatures = [9]\n",
    "    maxDepths = [10]\n",
    "    numTrees = [75]\n",
    "    scaleFactors = [.1, .25, .5, .75, 1]\n",
    "\n",
    "    # grid searching for best combination\n",
    "    combos = []\n",
    "    for numFeats in maxNumFeatures:\n",
    "        for maxDepth in maxDepths:\n",
    "            for trees in numTrees:\n",
    "                # only previous for loops impact the random forest's performance\n",
    "                imputer = RandomForestRegressor(n_estimators=trees, max_depth=maxDepth, max_features=numFeats)\n",
    "                imputer.fit(XTrain, YTrain)\n",
    "                predictedValues = imputer.predict(XTest)\n",
    "                print(mean_squared_error(YTest, predictedValues))   # original mse without scaling\n",
    "\n",
    "                for scale in scaleFactors:\n",
    "                    # linear trendline scaling\n",
    "                    scaledPredictedValues = scalePredictedValues(dates, validTarget, predictedValues, testIndicies, scale)\n",
    "                    # r2 = r2_score(predictedValues, YTest)\n",
    "                    mse = mean_squared_error(YTest, scaledPredictedValues)\n",
    "                    # combos.append((r2, (numFeats, maxDepth, trees)))\n",
    "                    combos.append((mse, (numFeats, maxDepth, trees, scale)))\n",
    "\n",
    "    combos.sort(key=lambda a: a[0])\n",
    "    # combos = combos[::-1]\n",
    "    for tup in combos:\n",
    "        print(tup[0], \"with {f} features, {d} max depth, {t} trees, and a scale factor of {s}\".format(\n",
    "            f = tup[1][0], d = tup[1][1], t = tup[1][2], s = tup[1][3]))\n",
    "\n",
    "    # creating best tree predictions\n",
    "    print(\"Best hyperparas are: {f} features, {d} depth, and {t} trees\".format(f = combos[0][1][0], d = combos[0][1][1], t = combos[0][1][2]))\n",
    "    # print(\"With an r2 of: {r}\".format(r = combos[0][0]))\n",
    "    print(\"With an MSE of: {m}\".format(m = combos[0][0]))\n",
    "    imputer = RandomForestRegressor(max_features=combos[0][1][0], max_depth=combos[0][1][1], n_estimators=combos[0][1][2])\n",
    "    # imputer = RandomForestRegressor(max_features=9, max_depth=10, n_estimators=75)\n",
    "    imputer.fit(validData, validTarget)\n",
    "    imputedValues = imputer.predict(nullData)\n",
    "    imputedValues = scalePredictedValues(dates, target, imputedValues, \n",
    "        [i for i in range(len(target)) if np.isnan(target[i])], combos[0][1][3])\n",
    "\n",
    "    # saving to a dataframe\n",
    "    predData = np.array((np.array(nullDates), imputedValues)).T\n",
    "    newDf = pd.DataFrame(predData, columns=[\"DateTime\", \"Predicted Ozark Groundwater Depth (ft)\"])\n",
    "    newDf.to_csv(\"predicted groundwater.csv\", index=False)\n",
    "\n",
    "\n",
    "targetFeature = \"Ozark Aquifer Depth to Water Level (ft)\"\n",
    "# correlationList = getCorrelationPerFeature(df, targetFeature)\n",
    "topCorrelatedFeatures = [x[0] for x in correlationList[:20]] + [\"DateTime\", targetFeature]\n",
    "df = pd.read_csv(\"Small Gap Imputed Data.csv\", usecols=topCorrelatedFeatures)\n",
    "tuneForest(df, targetFeature)\n",
    "# best was 9 features, 10 max depth, and 75 trees, in like 7 mins\n",
    "saveForestPredictions(\"Small Gap Imputed Data.csv\", \"predicted groundwater.csv\", targetFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "%matplotlib qt\n",
    "\n",
    "# feature = \"SWTP Total Influent Flow\"\n",
    "feature = \"Ozark Aquifer Depth to Water Level (ft)\"\n",
    "# feature = \"Springfield Plateau Aquifer Depth to Water Level (ft)\"\n",
    "# feature = \"James Gauge Height (ft)\"\n",
    "# feature = \"Wilsons Gauge Height (ft)\"\n",
    "# feature = \"Fire 168 Hour Rainfall Aggregate\"\n",
    "# feature = \"HourlyPressureChange\"\n",
    "\n",
    "\n",
    "# df = pd.read_csv(\"Joined Influent and Rainfall and Weather and Groundwater and Creek Gauge.csv\", parse_dates=[\"DateTime\"])\n",
    "df = pd.read_csv(\"Small Gap Imputed Data.csv\", parse_dates=[\"DateTime\"])\n",
    "# df = pd.read_csv(\"Small Gap Imputed Data Editted.csv\", parse_dates=[\"DateTime\"])\n",
    "# df[\"SWTP Total Influent Flow\"] = np.array([np.nan if x < 3.7 else x for x in df[\"SWTP Total Influent Flow\"]])\n",
    "\n",
    "\n",
    "# imputedDf = pd.read_csv(\"Small Gap Imputed Data.csv\")\n",
    "# imputedDf = pd.read_csv(\"Small Gap Imputed Data Editted.csv\")\n",
    "imputedDf = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "dates = np.array(df[\"DateTime\"])\n",
    "imputedArr = np.array(imputedDf[feature])\n",
    "nullArr = deepcopy(np.array(df[feature]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig, ax = visualizeMissingValues(dates, nullArr, fig, ax)\n",
    "ax = plotImputedData(dates, nullArr, imputedArr, ax)\n",
    "# ax.scatter(testDf[\"DateTime\"], testDf[testDf.columns[-1]], s=8, color=\"red\", marker=\"x\")\n",
    "ax.set_ylabel(feature)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18d1565e3dd2a1a1180dd629712b39ff168054eb513fda549cd851c01d6423bb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
