{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def makeNullRects(dates, y):\n",
    "    '''This function returns a list of matplotlib.patches.Rectangles where\n",
    "    np.nan values are present in the y array. If values are consecutive,\n",
    "    the rectangles will widen as needed.\n",
    "    Note that this function is made for a figure with an x-axis of dates\n",
    "    Input:\n",
    "        dates: x axis date time values\n",
    "        y: y axis range values as np.array, contains np.nan values\n",
    "\n",
    "    Returns:\n",
    "        list of matplotlib.patches.Rectangles located where\n",
    "        y has np.nan values.\n",
    "\n",
    "    Rectangle Parameters in function:\n",
    "        opacityCoeff: how solid rectangles appear\n",
    "        longRectColor: the color of the rectangles with >=7 width\n",
    "        shortRectColor: the color of the rectanges with <7 width\n",
    "    '''\n",
    "    # setting up rectangle parameters\n",
    "    opacityCoeff = 0.5\n",
    "    longRectColor = \"red\"\n",
    "    shortRectColor = \"magenta\"\n",
    "\n",
    "    # prep work for creating rectangles for nan values\n",
    "    index = 0\n",
    "    yMax = np.nanmax(y)\n",
    "    yMin = np.nanmin(y)\n",
    "    rectHeight = yMax - yMin\n",
    "    yRectCoor = yMin\n",
    "    allRects = []   # this is what will be returned\n",
    "\n",
    "    # creating rectangle patches\n",
    "    while index < len(y):\n",
    "\n",
    "        # if nan exists, then need to create a rectangle patch\n",
    "        if np.isnan(y[index]):\n",
    "            xRectCoorIndex = index - 1\n",
    "\n",
    "            # condition for if first y value is nan\n",
    "            if index == 0:\n",
    "                xRectCoorIndex += 1\n",
    "            \n",
    "            # condition for if last y value is nan, assumes y is not len 2\n",
    "            elif index + 1 == len(y):\n",
    "                xRectCoor = mdates.date2num(dates[xRectCoorIndex])\n",
    "                coords = (xRectCoor, yRectCoor)\n",
    "                width = mdates.date2num(dates[xRectCoorIndex + 1]) - mdates.date2num(dates[xRectCoorIndex])\n",
    "                allRects.append(mpatches.Rectangle(coords, width, rectHeight, color=shortRectColor, alpha=opacityCoeff))\n",
    "                break\n",
    "                \n",
    "            # all other cases\n",
    "            xRectCoor = mdates.date2num(dates[xRectCoorIndex])\n",
    "\n",
    "            # checking finding how long the rectangle needs to be--how many consecutive null values\n",
    "            index += 1\n",
    "            while np.isnan(y[index]):\n",
    "                index += 1\n",
    "            rightEdgeIndex = mdates.date2num(dates[index])\n",
    "\n",
    "            # making rectangle\n",
    "            coords = (xRectCoor, yRectCoor)\n",
    "            width = rightEdgeIndex - xRectCoor\n",
    "            color = shortRectColor\n",
    "            if index - xRectCoorIndex > 5:\n",
    "                color = longRectColor\n",
    "            allRects.append(mpatches.Rectangle(coords, width, rectHeight, color=color, alpha=opacityCoeff))\n",
    "\n",
    "        else:\n",
    "            index += 1\n",
    "\n",
    "    return allRects\n",
    "\n",
    "def visualizeMissingValues(dates, arr, fig, ax):\n",
    "    '''This function plots an array of values with datetime x axis values onto\n",
    "    a given axis, showing patches of null values if present.\n",
    "\n",
    "    Input:\n",
    "        dates: a numpy array of datetime objs that are the x-axis for the array with missing data to plot\n",
    "        arr: a numpy array that has missing data\n",
    "        fig: a matplotlib figure that contains the axis with the plot\n",
    "        ax: a matplotlib axis that will be plotted upon\n",
    "\n",
    "    Returns:\n",
    "        fig: edited matplotlib figure\n",
    "        ax: edited matplotlib axis\n",
    "    '''\n",
    "    ax.plot(dates, arr)\n",
    "\n",
    "    rects = makeNullRects(dates, arr)\n",
    "    for rect in rects:\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    formatter = mdates.ConciseDateFormatter(ax.xaxis.get_major_locator(), formats=[\"%Y\", \"%Y-%b\", \"%b-%d\", \"%d %H:%M\", \"%d %H:%M\", \"%H:%M\"])\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    return fig, ax\n",
    "\n",
    "def plotImputedData(dates, nullArr, imputedArr, ax):\n",
    "    '''This graph plots imputed data as a green dashed line on a given\n",
    "    matplotlib axis.\n",
    "\n",
    "    Input:\n",
    "        dates: a numpy array of datetime objs that are the x-axis for the array with missing data to plot\n",
    "        nullArr: a numpy array that has missing data\n",
    "        imputedArr: a numpy array that has some of the missing values imputed\n",
    "        ax: a matplotlib axis that will be plotted upon\n",
    "    \n",
    "    Returns:\n",
    "        ax: edited matplotlib axis\n",
    "    '''\n",
    "    index = 0\n",
    "    while index < len(nullArr):                                 # looping through arr since it has the null values\n",
    "        if np.isnan(nullArr[index]):\n",
    "            # getting the width of the null area\n",
    "            lenForward = 0\n",
    "            while np.isnan(nullArr[index + lenForward]):\n",
    "                lenForward += 1\n",
    "\n",
    "            # domain to plot is [index-1, index+lenforward]\n",
    "            domain = list(range(index-1, index+lenForward+1))\n",
    "            datesToPlot = [dates[i] for i in domain]\n",
    "            pointsToPlot = [imputedArr[i] for i in domain]\n",
    "            ax.plot(datesToPlot, pointsToPlot, \"g--\")       # green dashed line\n",
    "\n",
    "            # moving index forward past null gap\n",
    "            index += lenForward\n",
    "        else:\n",
    "            index += 1\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def getImputationData(nullFilename, targetFeature, correlatedFeatures):\n",
    "    '''For a given feature and set of correlated features,\n",
    "    this function returns a set of training data and target values\n",
    "    and testing data.\n",
    "    The training data is all the data that has no null values that correspond\n",
    "    to the non-null target data, which is the training target.\n",
    "    The testing data is all the data with non-null values that correspond\n",
    "    to a null target value.\n",
    "    Note that if a null target value has a set of features that includes a null,\n",
    "    that value will be left out.\n",
    "    '''\n",
    "    df = pd.read_csv(nullFilename, usecols=correlatedFeatures)   # selected areas that seem believeable and kept them from large gap impution\n",
    "    target = np.array(df[targetFeature])\n",
    "    dates = np.array(df[\"DateTime\"])\n",
    "    df = df.drop(columns=[targetFeature, \"DateTime\"])\n",
    "    data = df.to_numpy()\n",
    "    targetNulls = np.nonzero(np.isnan(target))[0]\n",
    "\n",
    "    testData = []\n",
    "    testDates = []\n",
    "    for i in targetNulls:\n",
    "        testData.append(data[i])\n",
    "        testDates.append(dates[i])\n",
    "    testData = np.array(testData)\n",
    "\n",
    "    trainTarget = np.delete(target, targetNulls)\n",
    "    trainData = np.delete(data, targetNulls, 0)     # the 0 means delete a row\n",
    "\n",
    "    badIndicies = []\n",
    "    for col in range(len(trainData[0])):\n",
    "        for row in range(len(trainData)):\n",
    "            if np.isnan(trainData[row][col]):\n",
    "                badIndicies.append(row)\n",
    "    if len(badIndicies) > 0:\n",
    "        badIndicies = np.unique(np.array(badIndicies))\n",
    "        trainData = np.delete(trainData, badIndicies, 0)\n",
    "        trainTarget = np.delete(trainTarget, badIndicies)\n",
    "\n",
    "    badIndicies = []\n",
    "    for col in range(len(testData[0])):\n",
    "        for row in range(len(testData)):\n",
    "            if np.isnan(testData[row][col]):\n",
    "                badIndicies.append(row)\n",
    "    if len(badIndicies) > 0:\n",
    "        badIndicies = np.unique(np.array(badIndicies))\n",
    "        testData = np.delete(testData, badIndicies, 0)\n",
    "    \n",
    "    return trainData, trainTarget, testData, testDates\n",
    "\n",
    "def joinImputedData(imputedFilename, nullFilename, targetFeature, filename=\"test.csv\"):\n",
    "    '''This function takes a csv of imputated data for null values for a single feature\n",
    "    and joins it to the original dataset with null values under the filename test.csv.\n",
    "    '''\n",
    "    testDf = pd.read_csv(imputedFilename, parse_dates=[\"DateTime\"])\n",
    "    df = pd.read_csv(nullFilename, parse_dates=[\"DateTime\"])\n",
    "    testArr = np.array(testDf[testDf.columns[-1]])\n",
    "    testDates = np.array(testDf[\"DateTime\"])\n",
    "    actualArr = np.array(df[targetFeature])\n",
    "    actualDates = np.array(df[\"DateTime\"])\n",
    "\n",
    "    for i in range(len(testDates)):\n",
    "        locations = np.nonzero(actualDates == testDates[i])\n",
    "        actualArr[locations[0]] = testArr[i]\n",
    "\n",
    "    df[targetFeature] = actualArr\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def normalize1D(arr):\n",
    "    return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
    "\n",
    "def normalize2D(data):\n",
    "    for col in range(np.shape(data)[1]):\n",
    "        ithCol = data[:, col]\n",
    "        ithCol = normalize1D(ithCol)\n",
    "        data[:, col] = ithCol\n",
    "    return data\n",
    "\n",
    "def scaleImputedData(trainData, trainTarget, testData):\n",
    "    return normalize2D(trainData), normalize1D(trainTarget), normalize2D(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "\n",
    "correlatedFeatures = ['Ozark Aquifer Depth to Water Level (ft)', \"Hour\", \"Month\", \"Year\", \"Week\"] + [\"DateTime\"]\n",
    "correlatedFeatures += [\"Sequiota 168 Hour Rainfall Aggregate\"] + [\"SWTP Total Influent Flow\"]\n",
    "correlatedFeatures += [\"James Gauge Height (ft)\"] + [\"Wilsons Gauge Height (ft)\"]\n",
    "\n",
    "imputedFilename = \"predicted groundwater test.csv\"\n",
    "# nullFilename = \"Small Gap Imputed Data Editted.csv\"\n",
    "nullFilename = \"Small Gap Imputed Data.csv\"\n",
    "targetFeature = \"Ozark Aquifer Depth to Water Level (ft)\"\n",
    "\n",
    "trainData, trainTarget, testData, testDates = getImputationData(nullFilename, targetFeature, correlatedFeatures)\n",
    "imputer = RandomForestRegressor(min_samples_split=5)\n",
    "# imputer = DecisionTreeRegressor(min_samples_split=5)\n",
    "imputer.fit(trainData, trainTarget)\n",
    "predictedValues = imputer.predict(testData)\n",
    "\n",
    "\n",
    "# normTrainData, normTrainTarget, normTestData = scaleImputedData(trainData, trainTarget, testData)\n",
    "# # imputer = LinearRegression()\n",
    "# imputer = Lasso()\n",
    "# # imputer = Ridge()\n",
    "# imputer.fit(trainData, trainTarget)\n",
    "# predictedValues = imputer.predict(testData)\n",
    "\n",
    "\n",
    "l = np.array((np.array(testDates), predictedValues)).T\n",
    "newDf = pd.DataFrame(l, columns=[\"DateTime\", \"Predicted Ozark Groundwater Depth (ft)\"])\n",
    "newDf.to_csv(\"predicted groundwater test.csv\", index=False)\n",
    "\n",
    "joinImputedData(imputedFilename, nullFilename, targetFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# initial starting points\n",
    "# p1 = (25806, 199.75)    # first large gap\n",
    "# p2 = (26487, 189.66)\n",
    "# p1 = (27465, 181.0573)  # second large gap\n",
    "# p2 = (27584, 182.5138)\n",
    "# p1 = (27746, 181.1897)  # third large gap\n",
    "# p2 = (27781, 179.1884)\n",
    "# p1 = (27895, 179.9647)  # fourth large gap\n",
    "# p2 = (27984, 177.9796)\n",
    "p1 = (28040, 178.8463)  # fifth large gap\n",
    "p2 = (28118, 173.26)\n",
    "\n",
    "# create a linear line from start of null gap to end\n",
    "x = np.linspace(p1[0], p2[0] - 1, num=(p2[0]-p1[0]), dtype=int)\n",
    "m = (p2[1] - p1[1]) / (p2[0] - p1[0])\n",
    "b = p2[1] - m * p2[0]\n",
    "xLinear = m * x + b\n",
    "\n",
    "# scaling and adjustment factors, depends on which gap as differences might be larger or smaller\n",
    "# the mean and st dev for each were hand chosen for these by trial and error\n",
    "# scale = np.random.normal(loc=.1, scale=0.04, size=np.shape(xLinear))     # for first small gap  \n",
    "# adjust = np.random.normal(loc=0, scale=0.25, size=np.shape(xLinear))\n",
    "# scale = np.random.normal(loc=.02, scale=0.04, size=np.shape(xLinear))    # for second small gap  \n",
    "# adjust = np.random.normal(loc=0, scale=0.04, size=np.shape(xLinear))\n",
    "# scale = np.random.normal(loc=.05, scale=0.04, size=np.shape(xLinear))    # for third small gap  \n",
    "# adjust = np.random.normal(loc=0, scale=0.2, size=np.shape(xLinear))\n",
    "# scale = np.random.normal(loc=.04, scale=0.04, size=np.shape(xLinear))    # for fourth small gap  \n",
    "# adjust = np.random.normal(loc=0, scale=0.15, size=np.shape(xLinear))\n",
    "scale = np.random.normal(loc=.038, scale=0.03, size=np.shape(xLinear))    # for fifth and last small gap  \n",
    "adjust = np.random.normal(loc=0, scale=0.125, size=np.shape(xLinear))\n",
    "\n",
    "# get original data with large null gaps and then the random forest predictions for those gaps\n",
    "# originDf = pd.read_csv(\"Small Gap Imputed Data.csv\", usecols=[\"Ozark Aquifer Depth to Water Level (ft)\", \"DateTime\"])\n",
    "originDf = pd.read_csv(\"Small Gap Imputed Data Editted.csv\", usecols=[\"Ozark Aquifer Depth to Water Level (ft)\", \"DateTime\"])\n",
    "predDf = pd.read_csv(\"test.csv\", usecols=[\"Ozark Aquifer Depth to Water Level (ft)\"])\n",
    "depths = np.array(originDf[\"Ozark Aquifer Depth to Water Level (ft)\"])\n",
    "predDepths = np.array(predDf[\"Ozark Aquifer Depth to Water Level (ft)\"])\n",
    "\n",
    "# linear line shifted toward the prediction value with gaussian noise added\n",
    "newDepths = np.ones_like(xLinear)\n",
    "for i in x:\n",
    "    if i % 6 == 0:\n",
    "        newDepths[i - p1[0]] = xLinear[i - p1[0]] + scale[i - p1[0]] * (predDepths[i] - xLinear[i - p1[0]]) + adjust[i - p1[0]]\n",
    "        # depths[i] = xLinear[i - p1[0]] + scale[i - p1[0]] * (predDepths[i] - xLinear[i - p1[0]]) + adjust[i - p1[0]]\n",
    "    else:\n",
    "        newDepths[i - p1[0]] = xLinear[i - p1[0]] + scale[i - p1[0]] * (predDepths[i] - xLinear[i - p1[0]])\n",
    "        # depths[i] = xLinear[i - p1[0]] + scale[i - p1[0]] * (predDepths[i] - xLinear[i - p1[0]])\n",
    "\n",
    "# since the new depth values oscillate a lot, we are going to smooth it out\n",
    "# take every few points and then cubic spline interpolate using those few points\n",
    "interpX = [p1[0] - 1]       # starting values, right before null gap\n",
    "interpY = [depths[p1[0] - 1]]\n",
    "# n = 10          # for first large gap\n",
    "# n = 6           # for second large gap\n",
    "# n = 2           # for third large gap  \n",
    "# n = 3           # for fourth large gap \n",
    "n = 2           # for fifth and last large gap\n",
    "for i in x:\n",
    "    if i % n == 0 and i != (p2[0] - 1):     # taking essentially every n values from the linear shift thingy\n",
    "        interpX.append(i)\n",
    "        interpY.append(newDepths[i - p1[0]])\n",
    "interpX.append(p2[0] - 1)   # ending values, right after null gap\n",
    "interpY.append(depths[p2[0] - 1])\n",
    "\n",
    "cs = CubicSpline(interpX, interpY)\n",
    "indicies = list(range(p1[0] - 1, p2[0]))\n",
    "newDepths = np.array([cs(i) for i in indicies])\n",
    "for i in indicies:\n",
    "    depths[i] = newDepths[i - p1[0] + 1]    # replacing values with cubic spline values\n",
    "\n",
    "# save new data into a file called \"test2.csv\"\n",
    "originDf[\"Ozark Aquifer Depth to Water Level (ft)\"] = depths\n",
    "originDf.to_csv(\"linear line test.csv\")\n",
    "joinImputedData(\"linear line test.csv\", \"test.csv\", \"Ozark Aquifer Depth to Water Level (ft)\", \"test2.csv\")\n",
    "\n",
    "# after this, pick out the wanted data by hand and put it in a master file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "feature = \"Ozark Aquifer Depth to Water Level (ft)\"\n",
    "\n",
    "# df = pd.read_csv(\"Small Gap Imputed Data Editted.csv\", parse_dates=[\"DateTime\"])\n",
    "df = pd.read_csv(\"Small Gap Imputed Data.csv\", parse_dates=[\"DateTime\"])\n",
    "nullArr = deepcopy(np.array(df[feature]))\n",
    "dates = np.array(df[\"DateTime\"])\n",
    "imputedDf = pd.read_csv(\"Small Gap Imputed Data Editted.csv\")\n",
    "# imputedDf = pd.read_csv(\"test.csv\")\n",
    "# imputedDf = pd.read_csv(\"test2.csv\")\n",
    "imputedArr = np.array(imputedDf[feature])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig, ax = visualizeMissingValues(dates, nullArr, fig, ax)\n",
    "ax = plotImputedData(dates, nullArr, imputedArr, ax)\n",
    "# ax.scatter(testDf[\"DateTime\"], testDf[testDf.columns[-1]], s=8, color=\"red\", marker=\"x\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18d1565e3dd2a1a1180dd629712b39ff168054eb513fda549cd851c01d6423bb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
