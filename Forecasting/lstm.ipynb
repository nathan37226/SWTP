{"cells":[{"cell_type":"markdown","metadata":{"id":"OGiE2ObcUVEV"},"source":["#### Roughly following the guide in:\n","###### https://pangkh98.medium.com/multi-step-multivariate-time-series-forecasting-using-lstm-92c6d22cd9c2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1257,"status":"ok","timestamp":1650565123501,"user":{"displayName":"Slade Gunter","userId":"04979545897422635730"},"user_tz":300},"id":"SafpVR2_UVEY"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","\n","features = ['SWTP Total Influent Flow', 'SWTP Plant 1 Influent Flow', 'SWTP Plant 2 Influent Flow',\n","            'Wilsons Gauge Height (ft)', 'James Gauge Height (ft)', \n","            'Fire 120 Hour Rainfall Aggregate', 'Bingham 120 Hour Rainfall Aggregate', 'Field 120 Hour Rainfall Aggregate', \n","            'Springfield Plateau Aquifer Depth to Water Level (ft)', 'Ozark Aquifer Depth to Water Level (ft)']\n","\n","# features = [\"Springfield Plateau Aquifer Depth to Water Level (ft)\", \"Fire 120 Hour Rainfall Aggregate\", \"SWTP Total Influent Flow\"]\n","dataset = pd.read_csv(\"Train and Test Data.csv\", usecols=features)\n","# dataset = pd.read_csv(\"Imputed Data.csv\", usecols=features)\n","arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n","dataset[\"Target\"] = arr         # adding another influent flow feature so that past values can be used to predict future values\n","values = dataset.values\n","\n","# linear transformation of each feature from [min, max] to [0, 1]\n","scaler = MinMaxScaler()\n","scaled = scaler.fit_transform(values)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow import keras\n","from sklearn.metrics import mean_squared_error\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Activation\n","from keras_tuner.tuners import BayesianOptimization\n","import os\n","\n","# path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\keras_tuner_attempt4\\\\model\"\n","\n","# split a multivariate sequence into samples\n","def split_sequences(sequences, n_steps_in, n_steps_out):\n","    X, y = list(), list()\n","    for i in range(len(sequences)):\n","        # find the end of this pattern\n","        end_ix = i + n_steps_in\n","        out_end_ix = end_ix + n_steps_out-1\n","        # check if we are beyond the dataset\n","        if out_end_ix > len(sequences):\n","            break\n","        # gather input and output parts of the pattern\n","        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","    return np.array(X), np.array(y)\n","\n","# for testing on different parts of the total year testing set\n","# use slide = 15 for 8 different windows will cover 24 days of 72 hour forecasts\n","def sliding_window(X, y, n_test, slide):\n","    split_point = X.shape[0] - n_test + slide\n","    train_X , train_y = X[:split_point, :] , y[:split_point, :]\n","    test_X , test_y = X[split_point:, :] , y[split_point:, :]\n","    return train_X, train_y, test_X, test_y\n","\n","def build_model(hp):\n","    model = Sequential()\n","    model.add(LSTM(units=hp.Int('units', min_value = 40, max_value = 200, step = 5), \n","               activation = 'tanh', return_sequences = True, input_shape = (n_steps_in, len(features))))\n","    model.add(LSTM(units = hp.Int('units', min_value = 40, max_value = 200, step = 5)))\n","    model.add(Dense(24))   # for predicting 24 hours -- if desire more, change\n","    model.add(Activation('linear'))\n","    model.compile(loss = 'mse', metrics = 'mse', optimizer = keras.optimizers.Adam(\n","        hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4, 1e-5])))\n","    return model\n","\n","def invNormalize(arr, minimum, maximum):\n","    return (maximum - minimum) * arr + minimum\n","\n","def predict(model, test_X, test_y, fullData = False):\n","    # to be able to inverse scale predictions\n","    df = pd.read_csv(\"Train and Test Data.csv\", usecols=[\"SWTP Total Influent Flow\"])\n","    if fullData:\n","        df = pd.read_csv(\"Imputed Data.csv\", usecols=[\"SWTP Total Influent Flow\"])\n","    arr = np.array(df[\"SWTP Total Influent Flow\"])\n","    maximum = np.max(arr)\n","    minimum = np.min(arr)\n","\n","    #predictions and rescaling to [min, max]\n","    y_pred = model.predict(test_X)\n","    y_pred_inv = np.array([invNormalize(x, minimum, maximum) for x in y_pred])\n","    test_y_inv = np.array([invNormalize(x, minimum, maximum) for x in test_y])\n","    print(\"y_pred_inv:\",y_pred_inv.shape)\n","    print(\"test_y_inv:\",y_pred_inv.shape)\n","    \n","    return y_pred_inv, test_y_inv\n","\n","def mseForecast(y, y_pred):\n","    # change so is only for a range, i.e. first 72 days of test set\n","    # mse is gonna scale really badly the further out it goes\n","    # msut fix! cannot calculate mse from a full year, must use a rolling window\n","    # that forecasts 3 days in advance, has new next hour put into it, then has next 3 day forecast one hour after\n","    totalMSE = 0\n","    for i in range(y.shape[0]):\n","        totalMSE += mean_squared_error(y[i], y_pred[i])\n","    avgMSE = totalMSE / y.shape[0]\n","    print(\"Total Avg MSE:\", avgMSE)\n","    return avgMSE\n","\n","def saveResults(path, firstMSE, avgMSE, n_epochs, hours=36):\n","    txt = \"n_steps_in = \" + str(hours)\n","    txt += \"\\nepochs = \" + str(n_epochs)\n","    txt += \"\\nFirst 10 Avg MSE: \" + str(round(firstMSE, 4))\n","    txt += \"\\nTotal Avg MSE: \" + str(round(avgMSE, 4))\n","    txt += \"\\n\\nForm:\\nLSTM\\nLSTM\\nDense(24)\\nActivation('linear')\"\n","    with open(path + \"\\\\results.txt\", 'w') as f:\n","        f.write(txt)\n","\n","def getValidationData(features, n_steps_in):\n","    # getting data\n","    dataset = pd.read_csv(\"Imputed Data.csv\", usecols=features)\n","    arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n","    dataset[\"Target\"] = arr         # adding another influent flow feature so that past values can be used to predict future values\n","    values = dataset.values\n","    scaler = MinMaxScaler()\n","    scaled = scaler.fit_transform(values)\n","\n","    n_steps_out = 24\n","    X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n","    train_X, train_y, validate_X, validate_y = sliding_window(X, y, 8760, 0)    # 8760 means last year of data, for validation\n","    return validate_X, validate_y\n","\n","def saveValidationResults(path, testMSE, validationMSE, n_epochs, hours=36):\n","    txt = \"n_steps_in = \" + str(hours)\n","    txt += \"\\nepochs = \" + str(n_epochs)\n","    txt += \"\\nAvg Test MSE: \" + str(round(testMSE, 4))\n","    txt += \"\\nAvg Validation MSE: \" + str(round(validationMSE, 4))\n","    txt += \"\\n\\nForm:\\nLSTM\\nLSTM\\nDense(24)\\nActivation('linear')\"\n","    with open(path + \"\\\\validation results.txt\", 'w') as f:\n","        f.write(txt)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137,"status":"ok","timestamp":1650565126149,"user":{"displayName":"Slade Gunter","userId":"04979545897422635730"},"user_tz":300},"id":"BX4vfwfKUVEZ","outputId":"c143e53a-899c-410d-b2fa-2917d2e4d7d0"},"outputs":[],"source":["# choose a number of time steps \n","# n_steps_in = 36\n","# n_steps_in = 48\n","n_steps_out = 24\n","\n","# splitting into training and testing\n","n_tests = [8760, 6552, 4344, 2280]                      # for different validation windows throughout the year\n","# epochList = [3, 5, 8, 10, 12, 15, 20, 25, 30]\n","# epochList = [4, 6, 7, 9, 11, 13, 14]\n","# epochList = [10, 5, 15, 30, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 20, 25]   # for 36 hour inupt\n","epochList = [3, 5, 8, 10, 12, 15, 20, 25, 30, 4, 6, 7, 9, 11, 13, 14]   # for 48 hour input\n","hourList = [48, 72, 36, 60, 42, 54, 24, 12, 18, 30, 45]\n","indicies = [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n","# epochList = [20, 25]\n","\n","# for i in n_tests:               # number of testing points per year\n","#     for s in range(7):          # number of sliding windows per testing point\n","lst = []\n","# for i in range(len(epochList)):\n","for n_steps_in, index in zip(hourList, indicies):\n","    # covert into input/output\n","    X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n","    print (\"X.shape\" , X.shape)                             # [rows, time lags backward, features]\n","    print (\"y.shape\" , y.shape)                             # [rows, future time values]\n","\n","    path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\hour tuning\\\\keras_tuner_attempt\"\n","    path += str(index)\n","    modelPath = path + \"\\\\model\"\n","    project_title = \"keras_tuner_attempt\" + str(index)\n","    n_epochs = 20\n","    print(\"Number epochs:\", n_epochs)\n","    for n in n_tests[:1]:               \n","        for s in range(1):\n","            train_X, train_y, test_X, test_y = sliding_window(X, y, n, 15 * s)\n","            # print(\"\\ntrain_X.shape\", train_X.shape)\n","            # print(\"train_y.shape\", train_y.shape)\n","            # print(\"test_X.shape\", test_X.shape)\n","            # print(\"test_y.shape\", test_y.shape)\n","\n","            # tuning model with keras tuner\n","            # bayesian_opt_tuner = BayesianOptimization(\n","            #     build_model,\n","            #     objective='mse',\n","            #     max_trials=3,\n","            #     executions_per_trial=1,\n","            #     directory=os.path.normpath('C:/Users/natha/Desktop/Undergrad/Spring2022/MTH 596 PIC Math/Project - Group 2/Project/Forecasting'),\n","            #     project_name=project_title,\n","            #     overwrite=True)\n","            # bayesian_opt_tuner.search(train_X, train_y, epochs=n_epochs,\n","            #     validation_data=(test_X, test_y),\n","            #     validation_split=0.2, verbose=1)\n","            # bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=1)\n","            # model = bayes_opt_model_best_model[0]\n","            # model.save(modelPath)\n","\n","            # string = \"Number epochs = \" + str(n_epochs)\n","            # with open(path + \"\\\\note.txt\", 'w') as f:\n","            #     f.write(string)\n","\n","            model = keras.models.load_model(modelPath)\n","\n","            # fitting model and predicting\n","            pred_y_inv, test_y_inv = predict(model, test_X, test_y)\n","            totalAvgMSE = mseForecast(test_y_inv, pred_y_inv)\n","\n","            validate_X, validate_y = getValidationData(features, n_steps_in)\n","            y_pred_inv, y_validate_inv = predict(model, validate_X, validate_y, True)\n","            validationAvgMSE = mseForecast(y_validate_inv, y_pred_inv)\n","\n","            saveValidationResults(path, totalAvgMSE, validationAvgMSE, n_epochs, n_steps_in)\n","\n","            lst.append((n_epochs, round(totalAvgMSE, 4), round(validationAvgMSE, 4)))\n","print(lst)\n","    "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","%matplotlib qt\n","\n","# This cell displays results of models created using above\n","\n","bestHours = [(48, 73.4145, 68.161), (72, 74.0058, 64.4983), (36, 85.1316, 66.0576), (60, 82.8813, 69.0653), \n","             (42, 110.3751, 80.0528), (54, 121.7316, 81.2496), (24, 99.3262, 72.6822), (12, 81.604, 68.1418), \n","             (18, 94.3174, 72.4802), (30, 92.6211, 76.5908), (45, 87.9457, 77.9525)]    # hour, testMSE, validateMSE\n","bestHours.sort(key = lambda x:x[0])\n","\n","bestEpochs36 = [(10, 67.2062, 62.3418), (5, 74.1717, 66.4878), (15, 77.8016, 64.5557), (30, 148.8478, 99.5409), \n","                (3, 108.7818, 99.7956), (4, 67.6975, 61.946), (6, 66.4358, 61.5656), (7, 70.8634, 59.2995), (8, 71.261, 64.6968), \n","                (9, 73.3233, 63.315), (11, 77.3612, 67.0248), (12, 80.6395, 78.4578), (13, 74.4024, 70.7845), \n","                (14, 85.5936, 71.134), (20, 87.708, 72.1959), (25, 86.8419, 82.2874)]   # epoch, testMSE, validateMSE\n","bestEpochs36.sort(key = lambda x:x[0])\n","\n","bestEpochs48 = [(3, 66.25, 62.099), (5, 70.0581, 59.1067), (8, 68.3578, 59.3903), (10, 74.559, 69.4547), \n","                (12, 79.9994, 69.8428), (15, 86.8468, 68.2357), (20, 106.8292, 71.1984), (25, 93.9291, 79.7969), \n","                (30, 94.9882, 82.1036), (4, 69.0089, 61.9953), (6, 73.2355, 63.6687), (7, 69.012, 61.0523), (9, 79.0518, 65.1099), \n","                (11, 75.885, 65.9066), (13, 73.6373, 63.8709), (14, 102.5217, 84.5292)]   # epoch, testMSE, validateMSE\n","bestEpochs48.sort(key = lambda x:x[0])\n","\n","# graphing hours\n","fig1, ax1 = plt.subplots()\n","x = [tup[0] for tup in bestHours]\n","y = [tup[2] for tup in bestHours]\n","ax1.plot(x, y)\n","ax1.scatter(x, y)\n","ax1.grid()\n","ax1.set_title(\"Past Hours of Data and the MSE of an LSTM model with 20 Epochs\", fontsize=20)\n","ax1.set_xlabel(\"Past Hours of Data\", fontsize=16)\n","ax1.set_ylabel(\"Validation MSE\", fontsize=16)\n","\n","# graphing epochs\n","fig2, ax2 = plt.subplots()\n","for results, label, color in zip([bestEpochs36, bestEpochs48], [\"36 Hour\", \"48 Hour\"], [\"red\", \"blue\"]):\n","    x = [tup[0] for tup in results]     # number of epochs\n","    y = [tup[2] for tup in results]     # validation mse\n","    ax2.plot(x, y, label=label)\n","    ax2.scatter(x, y)\n","ax2.legend(loc=\"upper right\")\n","ax2.grid()\n","ax2.set_title(\"Epoch tuning and MSE on Validation Set\", fontsize=20)\n","ax2.set_xlabel(\"Epochs\", fontsize=16)\n","ax2.set_ylabel(\"Validation MSE\", fontsize=16)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","\n","# choose a number of time steps \n","n_steps_in = 36\n","# n_steps_in = 48\n","n_steps_out = 24\n","\n","# covert into input/output\n","X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n","print (\"X.shape\" , X.shape)                             # [rows, time lags backward, features]\n","print (\"y.shape\" , y.shape)                             # [rows, future time values]\n","\n","\n","n_tests = [8760, 6552, 4344, 2280]                      # for different validation windows throughout the year\n","epochList36 = [10, 5, 15, 30, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 20, 25]\n","epochList48 = [3, 5, 8, 10, 12, 15, 20, 25, 30, 4, 6, 7, 9, 11, 13, 14]\n","epoch48ValidationsIndicies = [13, 4]\n","epoch36ValidationsIndicies = [6, 0]\n","# 48 - 11, 12 epochs best\n","# 36 - 6, 10 epochs best\n","\n","epochsToUse = [epochList36[i] for i in epoch36ValidationsIndicies]\n","for n_epochs, folderIndex in zip(epochsToUse, epoch36ValidationsIndicies):\n","    path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\36 hour epoch tuning\\\\keras_tuner_attempt\"\n","    path += str(folderIndex+1)\n","    modelPath = path + \"\\\\model\"\n","    print(path)\n","    print(\"Number epochs:\", n_epochs)\n","\n","    testMSE = [[0, 0, 0], [1, 0, 0], [2, 0, 0], [3, 0, 0]]      # n_test period, first 96 hour prediction mse, shifted 480 hours forward 96 hour pred\n","    for n in range(len(n_tests)):               \n","        for s in range(2):\n","            # getting data for validation test\n","            train_X, train_y, test_X, test_y = sliding_window(X, y, n_tests[n], 480 * s)\n","\n","            # fitting model and predicting\n","            model = keras.models.load_model(modelPath)\n","            model.fit(train_X, train_y, epochs=n_epochs, verbose=1, validation_data=(test_X, test_y), validation_split=0.2)\n","            pred_y_inv, test_y_inv = predict(model, test_X, test_y)\n","            totalMSE = 0\n","            numHoursForward = 96\n","            for k in range(numHoursForward):\n","                totalMSE += mean_squared_error(pred_y_inv[k], test_y_inv[k])\n","            # print(\"First {n} Avg MSE: {mse}\".format(n=numHoursForward, mse=totalMSE/numHoursForward))\n","\n","            testMSE[n][s+1] = totalMSE/numHoursForward\n","\n","    # saving validation results\n","    with open(path + \"\\\\validation results.json\", \"w\") as f:\n","        json.dump(testMSE, f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def combineValidationResults(results):\n","    sum = 0\n","    for l in results:\n","        for i in range(1, 3):\n","            sum += l[i]\n","    print(\"MSE = \", sum/8)\n","\n","epoch12Hour48Results = [[0, 13.265085957764384, 5.664549392861499], [1, 322.14254964814444, 89.00012063018733], \n","                        [2, 30.94971330178527, 15.442300796366476], [3, 23.369238580467254, 21.40317229597237]]\n","epoch11Hour48Results = [[0, 18.972753015342423, 5.010667427249979], [1, 275.55779577332555, 133.88350945889738], \n","                        [2, 33.828710449148744, 12.727969694411149], [3, 15.03309853350242, 24.127845879349817]]\n","epoch10Hour36Results = [[0, 15.57648872532529, 7.567939042658903], [1, 292.8620620735798, 49.715468543092896], \n","                        [2, 37.71236929791596, 13.61932757675911], [3, 36.1392324544318, 35.98589133229877]]\n","epoch6Hour36Results = [[0, 13.016783481550883, 6.828164035876674], [1, 314.32799463859845, 23.358751010248636], \n","                        [2, 31.608841918622772, 12.704988196358912], [3, 17.563685872196917, 21.91560674564487]]\n","combineValidationResults(epoch11Hour48Results)\n","combineValidationResults(epoch12Hour48Results)\n","combineValidationResults(epoch6Hour36Results)\n","combineValidationResults(epoch10Hour36Results)"]}],"metadata":{"colab":{"name":"lstm2.ipynb","provenance":[]},"interpreter":{"hash":"18d1565e3dd2a1a1180dd629712b39ff168054eb513fda549cd851c01d6423bb"},"kernelspec":{"display_name":"Python 3.8.13 ('ML')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
