{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in = 36\n",
    "# n_steps_out = 24\n",
    "n_steps_out = 5\n",
    "# n_steps_out = 48\n",
    "# n_steps_out = 1\n",
    "\n",
    "features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)', \n",
    "            \"Ozark Aquifer Depth to Water Level (ft)\", \"James Gauge Height (ft)\", 'HourlyStationPressure']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "import os\n",
    "\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def sliding_window(X, y, n_test, slide):\n",
    "    split_point = X.shape[0] - n_test + slide\n",
    "    train_X , train_y = X[:split_point, :] , y[:split_point, :]\n",
    "    test_X , test_y = X[split_point:, :] , y[split_point:, :]\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "def getValidationData(filename, features, n_steps_in):\n",
    "    # getting data\n",
    "    dataset = pd.read_csv(filename, usecols=features)\n",
    "    arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "    dataset[\"Target\"] = arr         # adding another influent flow feature so that past values can be used to predict future values\n",
    "    values = dataset.values\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(values)\n",
    "\n",
    "    X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "    train_X, train_y, validate_X, validate_y = sliding_window(X, y, 8760, 0)    # 8760 means last year of data, for validation\n",
    "    return validate_X, validate_y\n",
    "\n",
    "def invNormalize(arr, minimum, maximum):\n",
    "    return (maximum - minimum) * arr + minimum\n",
    "\n",
    "def mseForecast(y, y_pred):\n",
    "    totalMSE = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        totalMSE += mean_squared_error(y[i], y_pred[i])\n",
    "    avgMSE = totalMSE / y.shape[0]\n",
    "    return avgMSE\n",
    "\n",
    "def predict(model, test_X, test_y, fullData = False):\n",
    "    # to be able to inverse scale predictions\n",
    "    df = pd.read_csv(\"Train and Test Data.csv\", usecols=[\"SWTP Total Influent Flow\"])\n",
    "    if fullData:\n",
    "        df = pd.read_csv(\"Imputed Data.csv\", usecols=[\"SWTP Total Influent Flow\"])\n",
    "    arr = np.array(df[\"SWTP Total Influent Flow\"])\n",
    "    maximum = np.max(arr)\n",
    "    minimum = np.min(arr)\n",
    "\n",
    "    #predictions and rescaling to [min, max]\n",
    "    y_pred = model.predict(test_X)\n",
    "    y_pred_inv = np.array([invNormalize(x, minimum, maximum) for x in y_pred])\n",
    "    test_y_inv = np.array([invNormalize(x, minimum, maximum) for x in test_y])\n",
    "    print(\"y_pred_inv:\",y_pred_inv.shape)\n",
    "    print(\"test_y_inv:\",y_pred_inv.shape)\n",
    "    \n",
    "    return y_pred_inv, test_y_inv\n",
    "\n",
    "def saveValidationResults(path, testMSE, validationMSE, n_epochs, batch_size, hours=36, linexLossA = 3):\n",
    "    txt = \"n_steps_in = \" + str(hours)\n",
    "    txt += \"\\nepochs = \" + str(n_epochs)\n",
    "    txt += \"\\nbatch = \" + str(batch_size)\n",
    "    txt += \"\\nlinex loss a = \" + str(linexLossA)\n",
    "    txt += \"\\nAvg Test MSE: \" + str(round(testMSE, 4))\n",
    "    txt += \"\\nAvg Validation MSE: \" + str(round(validationMSE, 4))\n",
    "    txt += \"\\n\\nForm:\\nLSTM\\nLSTM\\nDense(24)\\nActivation('linear')\"\n",
    "    with open(path + \"\\\\validation results.txt\", 'w') as f:\n",
    "        f.write(txt)\n",
    "\n",
    "\n",
    "def sign_ae(x, y):\n",
    "    sign_x = tf.math.sign(x)\n",
    "    sign_y = tf.math.sign(y)\n",
    "    delta = x - y\n",
    "    return sign_x * sign_y * tf.math.abs(delta)\n",
    "    \n",
    "def linex_loss(delta, a=3., b=3.):\n",
    "    if a != 0 and b > 0:\n",
    "        # loss = 1/(a*a) * (tf.exp(a * delta) - a * delta - 1)\n",
    "        loss = b * (tf.exp(a * delta) - a * delta - 1)\n",
    "        return loss\n",
    "    else:\n",
    "        raise ValueError(\"linex_loss error with a or b\")\n",
    "          \n",
    "def linex_loss_val(y_true, y_pred):\n",
    "    delta = sign_ae(y_true, y_pred)\n",
    "    res = linex_loss(delta)\n",
    "    return tf.math.reduce_mean(res)\n",
    "\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        neuronTuning = hp.Int('units', min_value = 200, max_value = 350, step = 8)\n",
    "        learningRateTuning = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "        dropoutRateTuning = hp.Choice('rate', values = [0.0, 0.1, 0.2, 0.3])\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(LSTM(units = neuronTuning, dropout = dropoutRateTuning, \n",
    "               activation = 'tanh', input_shape = (n_steps_in, len(features)), return_sequences = True))\n",
    "        model.add(LSTM(units = neuronTuning, dropout = dropoutRateTuning))\n",
    "        model.add(Dense(24))\n",
    "        model.add(Activation('linear'))\n",
    "        model.compile(loss = 'mse', metrics = 'mse', optimizer = keras.optimizers.Adam(learningRateTuning))\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,  \n",
    "                         batch_size = hp.Choice(\"batch_size\", [2, 4, 8, 12, 16, 32, 64]), \n",
    "                         **kwargs)\n",
    "\n",
    "def buildModel():\n",
    "    neurons = 135\n",
    "    learningRate = 1e-4\n",
    "    # dropoutRate = 0.2\n",
    "    dropoutRate = 0\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(units = neurons, dropout = dropoutRate, \n",
    "            activation = 'tanh', input_shape = (n_steps_in, len(features)), return_sequences = True))\n",
    "    model.add(LSTM(units = neurons, dropout = dropoutRate))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss = linex_loss_val, metrics = linex_loss_val, optimizer = keras.optimizers.Adam(learningRate))\n",
    "    # model.compile(loss = 'mse', metrics = 'mse', optimizer = keras.optimizers.Adam(learningRate))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5341/5341 [==============================] - 117s 21ms/step - loss: 0.0183 - linex_loss_val: 0.0183 - val_loss: 0.0075 - val_linex_loss_val: 0.0075\n",
      "Epoch 2/5\n",
      "5341/5341 [==============================] - 107s 20ms/step - loss: 0.0028 - linex_loss_val: 0.0028 - val_loss: 8.7994e-04 - val_linex_loss_val: 8.7963e-04\n",
      "Epoch 3/5\n",
      "5341/5341 [==============================] - 108s 20ms/step - loss: 3.6860e-04 - linex_loss_val: 3.6860e-04 - val_loss: 3.0227e-04 - val_linex_loss_val: 3.0235e-04\n",
      "Epoch 4/5\n",
      "5341/5341 [==============================] - 111s 21ms/step - loss: 1.3420e-04 - linex_loss_val: 1.3420e-04 - val_loss: 1.3036e-04 - val_linex_loss_val: 1.3035e-04\n",
      "Epoch 5/5\n",
      "5341/5341 [==============================] - 108s 20ms/step - loss: 7.7698e-05 - linex_loss_val: 7.7698e-05 - val_loss: 1.5994e-05 - val_linex_loss_val: 1.5997e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\natha\\Desktop\\Undergrad\\Spring2022\\MTH 596 PIC Math\\Project - Group 2\\Project\\Forecasting\\loss\\linex loss 1 hour\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\natha\\Desktop\\Undergrad\\Spring2022\\MTH 596 PIC Math\\Project - Group 2\\Project\\Forecasting\\loss\\linex loss 1 hour\\model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023BF41FE730> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023BF45658E0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_inv: (8760, 1)\n",
      "test_y_inv: (8760, 1)\n",
      "y_pred_inv: (8760, 1)\n",
      "test_y_inv: (8760, 1)\n",
      "0.026083060895557397\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\"\n",
    "path += \"\\\\loss\\\\linex loss 1 hour\"\n",
    "modelPath = path + \"\\\\model\"\n",
    "\n",
    "dataset = pd.read_csv(\"Train and Test Data.csv\", usecols=features)\n",
    "arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "dataset[\"Target\"] = arr\n",
    "values = dataset.values\n",
    "scaler = MinMaxScaler()                     # automatically [0, 1]\n",
    "scaled = scaler.fit_transform(values)\n",
    "X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "train_X, train_y, test_X, test_y = sliding_window(X, y, 365*24, 0)\n",
    "\n",
    "model = buildModel()\n",
    "hist = model.fit(train_X, train_y, epochs = n_epochs, validation_data = (test_X, test_y), \n",
    "                    validation_split = 0.2, verbose = 1, batch_size = 4)\n",
    "model.save(modelPath)\n",
    "# model = keras.models.load_model(modelPath, custom_objects={'linex_loss_val': linex_loss_val})\n",
    "\n",
    "pred_y_inv, test_y_inv = predict(model, test_X, test_y)\n",
    "avgTestMSE = mseForecast(test_y_inv, pred_y_inv)\n",
    "validate_X, validate_y = getValidationData(\"Imputed Data.csv\", features, n_steps_in)\n",
    "y_pred_inv, y_validate_inv = predict(model, validate_X, validate_y, True)\n",
    "validationAvgMSE = mseForecast(y_validate_inv, y_pred_inv)\n",
    "print(validationAvgMSE)\n",
    "\n",
    "saveValidationResults(path, avgTestMSE, validationAvgMSE, n_epochs, 4, n_steps_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochList = [5]\n",
    "lastAttempt = 3\n",
    "attemptList = list(range(lastAttempt+1, len(epochList) + lastAttempt+1))\n",
    "\n",
    "for attempt, n_epochs in zip(attemptList, epochList):\n",
    "    # getting data and splitting it up\n",
    "    dataset = pd.read_csv(\"Train and Test Data.csv\", usecols=features)\n",
    "    arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "    dataset[\"Target\"] = arr\n",
    "    values = dataset.values\n",
    "    scaler = MinMaxScaler()     # automatically [0, 1]\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "    train_X, train_y, test_X, test_y = sliding_window(X, y, 365*24, 0)\n",
    "\n",
    "\n",
    "    # getting strings set up\n",
    "    path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\"\n",
    "    path += \"\\\\feature tuning\\\\rainfall subset\\\\keras_tuner_attempt\"\n",
    "    path += str(attempt)\n",
    "    modelPath = path + \"\\\\model\"\n",
    "    project_title = \"keras_tuner_attempt\" + str(attempt)\n",
    "    dirPath = 'C:/Users/natha/Desktop/Undergrad/Spring2022/MTH 596 PIC Math/Project - Group 2/Project/Forecasting/feature tuning/rainfall subset'\n",
    "\n",
    "    # creating hyperparameter tuner and tuning with it\n",
    "    bayesian_opt_tuner = BayesianOptimization(\n",
    "            MyHyperModel(),\n",
    "            objective='mse',\n",
    "            max_trials=3,\n",
    "            executions_per_trial=1,\n",
    "            directory=os.path.normpath(dirPath),\n",
    "            project_name=project_title,\n",
    "            overwrite=False)                  # switch to false if want to resume from where last left off\n",
    "            # overwrite=True)\n",
    "    bayesian_opt_tuner.search(train_X, train_y, epochs=n_epochs,\n",
    "        validation_data=(test_X, test_y),\n",
    "        verbose=1,\n",
    "        callbacks = [keras.callbacks.EarlyStopping('val_loss', patience=2)])        # this will stop if 3 epochs have gone by with no improvement\n",
    "    bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=1)\n",
    "    model = bayes_opt_model_best_model[0]\n",
    "    model.save(modelPath)\n",
    "\n",
    "    pred_y_inv, test_y_inv = predict(model, test_X, test_y)\n",
    "    avgTestMSE = mseForecast(test_y_inv, pred_y_inv)\n",
    "\n",
    "    validate_X, validate_y = getValidationData(\"Imputed Data.csv\", features, n_steps_in)\n",
    "    y_pred_inv, y_validate_inv = predict(model, validate_X, validate_y, True)\n",
    "    validationAvgMSE = mseForecast(y_validate_inv, y_pred_inv)\n",
    "\n",
    "    saveValidationResults(path, avgTestMSE, validationAvgMSE, n_epochs, n_steps_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochList = [5, 6, 7, 8, 9]\n",
    "attempts = [1, 2, 3, 4, 5]\n",
    "batches = [4, 16, 32, 64]\n",
    "subAttempts = [1, 2, 3, 4]\n",
    "\n",
    "for n_epochs, attempt in zip(epochList, attempts):\n",
    "    for n_batches, subAttempt in zip(batches, subAttempts):\n",
    "\n",
    "        # getting data and splitting it up\n",
    "        dataset = pd.read_csv(\"Train and Test Data.csv\", usecols=features)\n",
    "        arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "        dataset[\"Target\"] = arr\n",
    "        values = dataset.values\n",
    "        scaler = MinMaxScaler()     # automatically [0, 1]\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "        train_X, train_y, test_X, test_y = sliding_window(X, y, 365*24, 0)\n",
    "\n",
    "\n",
    "        # getting strings set up\n",
    "        path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\"\n",
    "        path += \"\\\\loss\\\\linex loss \" + str(attempt) + \".\" + str(subAttempt)\n",
    "        modelPath = path + \"\\\\model\"\n",
    "        project_title = \"linex loss \" + str(attempt) + \".\" + str(subAttempt)\n",
    "        dirPath = 'C:/Users/natha/Desktop/Undergrad/Spring2022/MTH 596 PIC Math/Project - Group 2/Project/Forecasting/loss'\n",
    "\n",
    "        # creating hyperparameter tuner and tuning with it\n",
    "        model = buildModel()\n",
    "        hist = model.fit(train_X, train_y, epochs = n_epochs, validation_data = (test_X, test_y), \n",
    "                         validation_split = 0.2, verbose = 1, batch_size = n_batches)\n",
    "        model.save(modelPath)\n",
    "\n",
    "        pred_y_inv, test_y_inv = predict(model, test_X, test_y)\n",
    "        avgTestMSE = mseForecast(test_y_inv, pred_y_inv)\n",
    "\n",
    "        validate_X, validate_y = getValidationData(\"Imputed Data.csv\", features, n_steps_in)\n",
    "        y_pred_inv, y_validate_inv = predict(model, validate_X, validate_y, True)\n",
    "        validationAvgMSE = mseForecast(y_validate_inv, y_pred_inv)\n",
    "        print(validationAvgMSE)\n",
    "\n",
    "        saveValidationResults(path, avgTestMSE, validationAvgMSE, n_epochs, n_batches, n_steps_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b104c07bfaa35248860e50ba1cfb46ca5e69d0db6474176440e1466f68c081"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
