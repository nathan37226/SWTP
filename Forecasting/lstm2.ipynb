{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# features = ['SWTP Total Influent Flow', 'SWTP Plant 1 Influent Flow', 'SWTP Plant 2 Influent Flow',\n",
    "#             'Wilsons Gauge Height (ft)', 'James Gauge Height (ft)', \n",
    "#             'Fire 120 Hour Rainfall Aggregate', 'Bingham 120 Hour Rainfall Aggregate', 'Field 120 Hour Rainfall Aggregate', \n",
    "#             'Springfield Plateau Aquifer Depth to Water Level (ft)', 'Ozark Aquifer Depth to Water Level (ft)']\n",
    "\n",
    "# features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)']    # subset 1\n",
    "# features = ['Springfield Plateau Aquifer Depth to Water Level (ft)', 'Fire Rainfall (in)', \n",
    "#             'Bingham Rainfall (in)', 'SWTP Total Influent Flow']      # subset 2\n",
    "# features = ['Springfield Plateau Aquifer Depth to Water Level (ft)', 'Fire Rainfall (in)', 'Bingham Rainfall (in)', \n",
    "#             'SWTP Plant 1 Gravity Flow', 'SWTP Plant 2 Influent Flow', 'SWTP Total Influent Flow']  # subset 3\n",
    "# features = ['SWTP Plant 1 Gravity Flow', 'SWTP Plant 2 Influent Flow', 'SW_Peak_Flow', 'SWTP Total Influent Flow']\n",
    "# features = [\"Ozark Aquifer Depth to Water Level (ft)\", \"Fire 168 Hour Rainfall Aggregate\", \n",
    "#             \"English 72 Hour Rainfall Aggregate\", \"Hour\", 'SWTP Total Influent Flow']   # SFS with no influent flow stuff\n",
    "# features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)', 'Field Rainfall (in)',\n",
    "#             'Pittman Rainfall (in)', 'Waste Rainfall (in)', 'Ozark Aquifer Depth to Water Level (ft)']    # subset 4\n",
    "# features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)', 'SWTP Plant 1 Gravity Flow']\n",
    "# features = ['SWTP Total Influent Flow', 'Fire 120 Hour Rainfall Aggregate', 'Bingham 120 Hour Rainfall Aggregate', 'SWTP Plant 1 Gravity Flow']\n",
    "features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)', 'SWTP Plant 1 Gravity Flow', \"HourlySeaLevelPressure\"]\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"Train and Test Data.csv\", usecols=features)\n",
    "# dataset = pd.read_csv(\"Imputed Data.csv\", usecols=features)\n",
    "arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "# dataset.drop(columns=['SWTP Total Influent Flow'], inplace=True)\n",
    "# features.remove(\"SWTP Total Influent Flow\")\n",
    "dataset[\"Target\"] = arr         # adding another influent flow feature so that past values can be used to predict future values\n",
    "values = dataset.values\n",
    "\n",
    "# linear transformation of each feature from [min, max] to [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "import os\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def sliding_window(X, y, n_test, slide):\n",
    "    split_point = X.shape[0] - n_test + slide\n",
    "    train_X , train_y = X[:split_point, :] , y[:split_point, :]\n",
    "    test_X , test_y = X[split_point:, :] , y[split_point:, :]\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('units', min_value = 5, max_value = 150, step = 5), \n",
    "               activation = 'tanh', return_sequences = True, input_shape = (n_steps_in, len(features))))\n",
    "    model.add(LSTM(units = hp.Int('units', min_value = 5, max_value = 150, step = 5)))\n",
    "    model.add(Dense(24))   # for predicting 24 hours -- if desire more, change\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss = 'mse', metrics = 'mse', optimizer = keras.optimizers.Adam(\n",
    "        hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4, 1e-5])))\n",
    "    return model\n",
    "\n",
    "def build_model_new(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('units', min_value = 5, max_value = 150, step = 5), \n",
    "               activation = 'tanh', input_shape = (n_steps_in, len(features))))\n",
    "    model.add(LSTM(units = hp.Int('units', min_value = 5, max_value = 150, step = 5)))\n",
    "    model.add(Dense(24))   # for predicting 24 hours -- if desire more, change\n",
    "    model.add(Activation('tanh'))\n",
    "    model.compile(loss = 'mse', metrics = 'mse', optimizer = keras.optimizers.Adam(\n",
    "        hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4, 1e-5])))\n",
    "    return model\n",
    "\n",
    "def invNormalize(arr, minimum, maximum):\n",
    "    return (maximum - minimum) * arr + minimum\n",
    "\n",
    "def predict(model, test_X):\n",
    "    # to be able to inverse scale predictions\n",
    "    df = pd.read_csv(\"Train and Test Data.csv\", usecols=[\"SWTP Total Influent Flow\"])\n",
    "    arr = np.array(df[\"SWTP Total Influent Flow\"])\n",
    "    maximum = np.max(arr)\n",
    "    minimum = np.min(arr)\n",
    "\n",
    "    #predictions and rescaling to [min, max]\n",
    "    y_pred = model.predict(test_X)\n",
    "    y_pred_inv = np.array([invNormalize(x, minimum, maximum) for x in y_pred])\n",
    "    test_y_inv = np.array([invNormalize(x, minimum, maximum) for x in test_y])\n",
    "    print(\"y_pred_inv:\",y_pred_inv.shape)\n",
    "    print(\"test_y_inv:\",y_pred_inv.shape)\n",
    "    \n",
    "    return y_pred_inv, test_y_inv\n",
    "\n",
    "def saveResults(path, firstMSE, avgMSE, n_epochs, features, hours=36):\n",
    "    txt = \"n_steps_in = \" + str(hours)\n",
    "    txt += \"\\nepochs = \" + str(n_epochs)\n",
    "    txt += \"\\nFirst 96 Avg MSE: \" + str(round(firstMSE, 4))\n",
    "    txt += \"\\nTotal Avg MSE: \" + str(round(avgMSE, 4))\n",
    "    txt += \"\\n\\nForm:\\nLSTM\\nLSTM\\nDense(24)\\nActivation('linear')\\n\\n\"\n",
    "    txt += str(features)\n",
    "    with open(path + \"\\\\results.txt\", 'w') as f:\n",
    "        f.write(txt)\n",
    "\n",
    "def saveResultsSimple(path, firstMSE, avgMSE, n_epochs, features, hours=36):\n",
    "    txt = \"n_steps_in = \" + str(hours)\n",
    "    txt += \"\\nepochs = \" + str(n_epochs)\n",
    "    txt += \"\\nFirst 96 Avg MSE: \" + str(round(firstMSE, 4))\n",
    "    txt += \"\\nTotal Avg MSE: \" + str(round(avgMSE, 4))\n",
    "    txt += \"\\n\\nForm:\\nLSTM\\nDense(24)\\nActivation('linear')\\n\\n\"\n",
    "    txt += str(features)\n",
    "    # txt += \"\\n\\nRainfall weight *= 2\"\n",
    "    with open(path + \"\\\\results.txt\", 'w') as f:\n",
    "        f.write(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in = 36\n",
    "n_steps_out = 24\n",
    "\n",
    "# covert into input/output\n",
    "X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "print (\"X.shape\" , X.shape)                             # [rows, time lags backward, features]\n",
    "print (\"y.shape\" , y.shape)                             # [rows, future time values]\n",
    "\n",
    "epochList = [4, 5, 6, 7]\n",
    "n_tests = [8760, 6552, 4344, 2280]                      \n",
    "lst = []\n",
    "\n",
    "start = 3\n",
    "for i in range(start, len(epochList[start:start+1]) + start):\n",
    "# for i in range(len(epochList)):\n",
    "    # path = \"feature tuning\\\\keras_tuner_attempt\"\n",
    "    path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\feature tuning\\\\keras_tuner_attempt\"\n",
    "    path += str(i+1+6)\n",
    "    modelPath = path + \"\\\\model\"\n",
    "    project_title = \"keras_tuner_attempt\" + str(i+1+6)\n",
    "    n_epochs = epochList[i]\n",
    "    print(\"Number epochs:\", n_epochs)\n",
    "    for n in n_tests[:1]:               \n",
    "        for s in range(1):\n",
    "            train_X, train_y, test_X, test_y = sliding_window(X, y, n, 15 * s)\n",
    "\n",
    "            # tuning model with keras tuner\n",
    "            bayesian_opt_tuner = BayesianOptimization(\n",
    "                build_model,\n",
    "                # build_model_new,\n",
    "                objective='mse',\n",
    "                max_trials=3,\n",
    "                executions_per_trial=1,\n",
    "                directory=os.path.normpath('C:/Users/natha/Desktop/Undergrad/Spring2022/MTH 596 PIC Math/Project - Group 2/Project/Forecasting/feature tuning'),\n",
    "                project_name=project_title,\n",
    "                overwrite=True)\n",
    "            bayesian_opt_tuner.search(train_X, train_y, epochs=n_epochs,\n",
    "                validation_data=(test_X, test_y),\n",
    "                validation_split=0.2, verbose=1)\n",
    "            bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=1)\n",
    "            model = bayes_opt_model_best_model[0]\n",
    "            model.save(modelPath)\n",
    "            # model = keras.models.load_model(modelPath)\n",
    "\n",
    "            string = \"Number epochs = \" + str(n_epochs)\n",
    "            with open(path + \"\\\\note.txt\", 'w') as f:\n",
    "                f.write(string)\n",
    "\n",
    "            pred_y_inv, test_y_inv = predict(model, test_X)\n",
    "            firstMSE = 0\n",
    "            for k in range(96):\n",
    "                firstMSE += mean_squared_error(pred_y_inv[k], test_y_inv[k])\n",
    "            # print(\"First 10 Avg MSE:\", firstMSE/10)\n",
    "\n",
    "            totalAvgMSE = 0\n",
    "            for i in range(test_y.shape[0]):\n",
    "                totalAvgMSE += mean_squared_error(test_y_inv[i], pred_y_inv[i])\n",
    "            totalAvgMSE = totalAvgMSE / test_y.shape[0]\n",
    "            # saveResultsSimple(path, firstMSE/96, totalAvgMSE, n_epochs, features)\n",
    "            saveResults(path, firstMSE/96, totalAvgMSE, n_epochs, features)\n",
    "\n",
    "            lst.append((n_epochs, round(firstMSE/96, 4), round(totalAvgMSE, 4)))\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\feature tuning\\\\keras_tuner_attempt10\\\\model\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib qt\n",
    "\n",
    "def weightedMSE(arr):\n",
    "    results = []\n",
    "    x1 = 0\n",
    "    A = np.array([[x1, 1-x1]])\n",
    "    for tup in arr:\n",
    "        x = np.array([[tup[1]], [tup[2]]])\n",
    "        b = np.matmul(A, x)\n",
    "        results.append((tup[0], b[0][0]))\n",
    "    results.sort(key = lambda x:x[0])\n",
    "    return results\n",
    "\n",
    "bestEpochsfeatSubset1 = [(3, 14.4393, 63.0615), (4, 11.7961, 63.7118), (5, 12.4332, 62.5518), (6, 27.4915, 62.4562)]\n",
    "bestEpochsfeatSubset2 = [(3, 35.0325, 217.5524), (4, 37.7734, 166.1138), (5, 162.7366, 117.081), (6, 77.5023, 155.0186)]\n",
    "bestEpochsfeatSubset3 = [(3, 34.8093, 86.7812), (4, 24.0673, 66.3212), (5, 37.6574, 74.5868), (6, 39.6214, 80.9921)]\n",
    "originalSubset = [(3, 201.0433, 108.7818), (4, 93.3019, 67.6975), (5, 27.4284, 74.1717), (6, 24.6906, 66.4358)]\n",
    "originalResults = weightedMSE(originalSubset)\n",
    "results1 = weightedMSE(bestEpochsfeatSubset1)\n",
    "results2 = weightedMSE(bestEpochsfeatSubset2)\n",
    "results3 = weightedMSE(bestEpochsfeatSubset3)\n",
    "\n",
    "# graphing\n",
    "resultList = [originalResults, results1, results2, results3]\n",
    "subsets = [\"original\"] + [\"subset\" + str(i) for i in range(1, len(resultList))]\n",
    "for results, label in zip(resultList, subsets):\n",
    "    x = [tup[0] for tup in results]\n",
    "    y = [tup[1] for tup in results]\n",
    "    plt.plot(x, y, label=label)\n",
    "    plt.scatter(x, y)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Weighted MSE Sum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at feature tuning\\subset 5\\keras_tuner_attempt8\\model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\natha\\Desktop\\Undergrad\\Spring2022\\MTH 596 PIC Math\\Project - Group 2\\Project\\Forecasting\\lstm2.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Undergrad/Spring2022/MTH%20596%20PIC%20Math/Project%20-%20Group%202/Project/Forecasting/lstm2.ipynb#ch0000005?line=44'>45</a>\u001b[0m \u001b[39m# modelPath = \"48 hour epoch tuning\\\\keras_tuner_attempt14\\\\model\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Undergrad/Spring2022/MTH%20596%20PIC%20Math/Project%20-%20Group%202/Project/Forecasting/lstm2.ipynb#ch0000005?line=45'>46</a>\u001b[0m \u001b[39m# modelPath = \"36 hour epoch tuning\\\\keras_tuner_attempt7\\\\model\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Undergrad/Spring2022/MTH%20596%20PIC%20Math/Project%20-%20Group%202/Project/Forecasting/lstm2.ipynb#ch0000005?line=46'>47</a>\u001b[0m \u001b[39m# modelPath = \"feature tuning\\\\subset 1\\\\keras_tuner_attempt3\\\\model\" # did v well in long term\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Undergrad/Spring2022/MTH%20596%20PIC%20Math/Project%20-%20Group%202/Project/Forecasting/lstm2.ipynb#ch0000005?line=47'>48</a>\u001b[0m \u001b[39m# modelPath = \"feature tuning\\\\extraneous\\\\keras_tuner_attempt8\\\\model\" # avg mse of 58.5759\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Undergrad/Spring2022/MTH%20596%20PIC%20Math/Project%20-%20Group%202/Project/Forecasting/lstm2.ipynb#ch0000005?line=48'>49</a>\u001b[0m modelPath \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfeature tuning\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39msubset 5\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mkeras_tuner_attempt8\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# avg mse of 60.5839 with low initial\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Undergrad/Spring2022/MTH%20596%20PIC%20Math/Project%20-%20Group%202/Project/Forecasting/lstm2.ipynb#ch0000005?line=49'>50</a>\u001b[0m model \u001b[39m=\u001b[39m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(modelPath)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Undergrad/Spring2022/MTH%20596%20PIC%20Math/Project%20-%20Group%202/Project/Forecasting/lstm2.ipynb#ch0000005?line=50'>51</a>\u001b[0m y_pred_inv, y_test_inv \u001b[39m=\u001b[39m predict(model, test_X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Undergrad/Spring2022/MTH%20596%20PIC%20Math/Project%20-%20Group%202/Project/Forecasting/lstm2.ipynb#ch0000005?line=52'>53</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y_pred_inv)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/natha/anaconda3/envs/ML/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/natha/anaconda3/envs/ML/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/natha/anaconda3/envs/ML/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/natha/anaconda3/envs/ML/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/natha/anaconda3/envs/ML/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\keras\\saving\\save.py:204\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/natha/anaconda3/envs/ML/lib/site-packages/keras/saving/save.py?line=201'>202</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath_str, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/natha/anaconda3/envs/ML/lib/site-packages/keras/saving/save.py?line=202'>203</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> <a href='file:///c%3A/Users/natha/anaconda3/envs/ML/lib/site-packages/keras/saving/save.py?line=203'>204</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNo file or directory found at \u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/natha/anaconda3/envs/ML/lib/site-packages/keras/saving/save.py?line=205'>206</a>\u001b[0m   \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    <a href='file:///c%3A/Users/natha/anaconda3/envs/ML/lib/site-packages/keras/saving/save.py?line=206'>207</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39mload(filepath_str, \u001b[39mcompile\u001b[39m, options)\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at feature tuning\\subset 5\\keras_tuner_attempt8\\model"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib qt\n",
    "\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)']\n",
    "# features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)', 'SWTP Plant 1 Gravity Flow']\n",
    "features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)', 'SWTP Plant 1 Gravity Flow', \"HourlySeaLevelPressure\"]\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"Imputed Data.csv\", usecols=features)\n",
    "arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "dataset[\"Target\"] = arr         # adding another influent flow feature so that past values can be used to predict future values\n",
    "values = dataset.values\n",
    "\n",
    "# linear transformation of each feature from [min, max] to [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(values)\n",
    "\n",
    "\n",
    "# n_steps_in = 48\n",
    "n_steps_in = 36\n",
    "n_steps_out = 24\n",
    "X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "train_X, train_y, test_X, test_y = sliding_window(X, y, 8760, 0)    # a full year of data\n",
    "\n",
    "# modelPath = \"48 hour epoch tuning\\\\keras_tuner_attempt14\\\\model\"\n",
    "# modelPath = \"36 hour epoch tuning\\\\keras_tuner_attempt7\\\\model\"\n",
    "# modelPath = \"feature tuning\\\\subset 1\\\\keras_tuner_attempt3\\\\model\" # did v well in long term\n",
    "# modelPath = \"feature tuning\\\\extraneous\\\\keras_tuner_attempt8\\\\model\" # avg mse of 58.5759\n",
    "modelPath = \"feature tuning\\\\subset 5\\\\keras_tuner_attempt8\\\\model\" # avg mse of 60.5839 with low initial\n",
    "model = model = keras.models.load_model(modelPath)\n",
    "y_pred_inv, y_test_inv = predict(model, test_X)\n",
    "\n",
    "x = list(range(len(y_pred_inv)))\n",
    "y = [mean_squared_error(y_pred_inv[i], y_test_inv[i]) for i in range(len(y_pred_inv))]\n",
    "\n",
    "\n",
    "\n",
    "# plotting the mse of 24 hour future predictions\n",
    "df = pd.read_csv(\"Imputed Data.csv\", usecols=[\"DateTime\", \"Fire Rainfall (in)\", \"Fire 168 Hour Rainfall Aggregate\"], parse_dates=[\"DateTime\"])\n",
    "dates = np.array(df[\"DateTime\"])\n",
    "dates = dates[-1*test_y.shape[0]-24:-24]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "formatter = mdates.ConciseDateFormatter(ax.xaxis.get_major_locator(), formats=[\"%Y\", \"%Y-%b\", \"%b-%d\", \"%d %H:%M\", \"%d %H:%M\", \"%H:%M\"])\n",
    "locator = mdates.AutoDateLocator()\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "ax.set_title(\"RMSE of 24 Hour Predictions\")\n",
    "ax.plot(dates, np.sqrt(y))  # root mean square error\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18d1565e3dd2a1a1180dd629712b39ff168054eb513fda549cd851c01d6423bb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
