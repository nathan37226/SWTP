{"cells":[{"cell_type":"markdown","metadata":{"id":"OGiE2ObcUVEV"},"source":["#### Roughly following the guide in:\n","###### https://pangkh98.medium.com/multi-step-multivariate-time-series-forecasting-using-lstm-92c6d22cd9c2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1257,"status":"ok","timestamp":1650565123501,"user":{"displayName":"Slade Gunter","userId":"04979545897422635730"},"user_tz":300},"id":"SafpVR2_UVEY"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","\n","features = ['SWTP Total Influent Flow', 'SWTP Plant 1 Influent Flow', 'SWTP Plant 2 Influent Flow',\n","            'Wilsons Gauge Height (ft)', 'James Gauge Height (ft)', \n","            'Fire 120 Hour Rainfall Aggregate', 'Bingham 120 Hour Rainfall Aggregate', 'Field 120 Hour Rainfall Aggregate', \n","            'Springfield Plateau Aquifer Depth to Water Level (ft)', 'Ozark Aquifer Depth to Water Level (ft)']\n","\n","# features = [\"Springfield Plateau Aquifer Depth to Water Level (ft)\", \"Fire 120 Hour Rainfall Aggregate\", \"SWTP Total Influent Flow\"]\n","# dataset = pd.read_csv(\"Train and Test Data.csv\", usecols=features)\n","dataset = pd.read_csv(\"Imputed Data.csv\", usecols=features)\n","arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n","dataset[\"Target\"] = arr         # adding another influent flow feature so that past values can be used to predict future values\n","values = dataset.values\n","\n","# linear transformation of each feature from [min, max] to [0, 1]\n","scaler = MinMaxScaler()\n","scaled = scaler.fit_transform(values)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow import keras\n","from sklearn.metrics import mean_squared_error\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Activation\n","from keras_tuner.tuners import BayesianOptimization\n","import os\n","\n","# path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\keras_tuner_attempt4\\\\model\"\n","\n","# split a multivariate sequence into samples\n","def split_sequences(sequences, n_steps_in, n_steps_out):\n","    X, y = list(), list()\n","    for i in range(len(sequences)):\n","        # find the end of this pattern\n","        end_ix = i + n_steps_in\n","        out_end_ix = end_ix + n_steps_out-1\n","        # check if we are beyond the dataset\n","        if out_end_ix > len(sequences):\n","            break\n","        # gather input and output parts of the pattern\n","        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","    return np.array(X), np.array(y)\n","\n","# for testing on different parts of the total year testing set\n","# use slide = 15 for 8 different windows will cover 24 days of 72 hour forecasts\n","def sliding_window(X, y, n_test, slide):\n","    split_point = X.shape[0] - n_test + slide\n","    train_X , train_y = X[:split_point, :] , y[:split_point, :]\n","    test_X , test_y = X[split_point:, :] , y[split_point:, :]\n","    return train_X, train_y, test_X, test_y\n","\n","def build_model(hp):\n","    model = Sequential()\n","    model.add(LSTM(units=hp.Int('units', min_value = 40, max_value = 200, step = 5), \n","               activation = 'tanh', return_sequences = True, input_shape = (n_steps_in, len(features))))\n","    model.add(LSTM(units = hp.Int('units', min_value = 40, max_value = 200, step = 5)))\n","    model.add(Dense(24))   # for predicting 24 hours -- if desire more, change\n","    model.add(Activation('linear'))\n","    model.compile(loss = 'mse', metrics = 'mse', optimizer = keras.optimizers.Adam(\n","        hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4, 1e-5])))\n","    return model\n","\n","def invNormalize(arr, minimum, maximum):\n","    return (maximum - minimum) * arr + minimum\n","\n","def predict(model, test_X):\n","    # to be able to inverse scale predictions\n","    df = pd.read_csv(\"Train and Test Data.csv\", usecols=[\"SWTP Total Influent Flow\"])\n","    arr = np.array(df[\"SWTP Total Influent Flow\"])\n","    maximum = np.max(arr)\n","    minimum = np.min(arr)\n","\n","    #predictions and rescaling to [min, max]\n","    y_pred = model.predict(test_X)\n","    y_pred_inv = np.array([invNormalize(x, minimum, maximum) for x in y_pred])\n","    test_y_inv = np.array([invNormalize(x, minimum, maximum) for x in test_y])\n","    print(\"y_pred_inv:\",y_pred_inv.shape)\n","    print(\"test_y_inv:\",y_pred_inv.shape)\n","    \n","    return y_pred_inv, test_y_inv\n","\n","def mseForecast(y, y_pred):\n","    # change so is only for a range, i.e. first 72 days of test set\n","    # mse is gonna scale really badly the further out it goes\n","    # msut fix! cannot calculate mse from a full year, must use a rolling window\n","    # that forecasts 3 days in advance, has new next hour put into it, then has next 3 day forecast one hour after\n","    totalMSE = 0\n","    for i in range(y.shape[0]):\n","        totalMSE += mean_squared_error(y[i], y_pred[i])\n","    avgMSE = totalMSE / y.shape[0]\n","    print(\"Total Avg MSE:\", avgMSE)\n","    return avgMSE\n","\n","def saveResults(path, firstMSE, avgMSE, n_epochs, hours=36):\n","    txt = \"n_steps_in = \" + str(hours)\n","    txt += \"\\nepochs = \" + str(n_epochs)\n","    txt += \"\\nFirst 10 Avg MSE: \" + str(round(firstMSE, 4))\n","    txt += \"\\nTotal Avg MSE: \" + str(round(avgMSE, 4))\n","    txt += \"\\n\\nForm:\\nLSTM\\nLSTM\\nDense(24)\\nActivation('linear')\"\n","    with open(path + \"\\\\results.txt\", 'w') as f:\n","        f.write(txt)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137,"status":"ok","timestamp":1650565126149,"user":{"displayName":"Slade Gunter","userId":"04979545897422635730"},"user_tz":300},"id":"BX4vfwfKUVEZ","outputId":"c143e53a-899c-410d-b2fa-2917d2e4d7d0"},"outputs":[],"source":["# choose a number of time steps \n","n_steps_in = 36\n","# n_steps_in = 48\n","n_steps_out = 24\n","\n","# covert into input/output\n","X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n","print (\"X.shape\" , X.shape)                             # [rows, time lags backward, features]\n","print (\"y.shape\" , y.shape)                             # [rows, future time values]\n","\n","\n","# splitting into training and testing\n","n_tests = [8760, 6552, 4344, 2280]                      # for different validation windows throughout the year\n","# epochList = [3, 5, 8, 10, 12, 15, 20, 25, 30]\n","# epochList = [4, 6, 7, 9, 11, 13, 14]\n","epochList = [20, 25]\n","\n","# for i in n_tests:               # number of testing points per year\n","#     for s in range(7):          # number of sliding windows per testing point\n","lst = []\n","for i in range(len(epochList)):\n","    path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\keras_tuner_attempt\"\n","    path += str(i+15)\n","    modelPath = path + \"\\\\model\"\n","    project_title = \"keras_tuner_attempt\" + str(i+15)\n","    n_epochs = epochList[i]\n","    print(\"Number epochs:\", n_epochs)\n","    for n in n_tests[:1]:               \n","        for s in range(1):\n","            train_X, train_y, test_X, test_y = sliding_window(X, y, n, 15 * s)\n","            # print(\"\\ntrain_X.shape\", train_X.shape)\n","            # print(\"train_y.shape\", train_y.shape)\n","            # print(\"test_X.shape\", test_X.shape)\n","            # print(\"test_y.shape\", test_y.shape)\n","\n","            # tuning model with keras tuner\n","            bayesian_opt_tuner = BayesianOptimization(\n","                build_model,\n","                objective='mse',\n","                max_trials=3,\n","                executions_per_trial=1,\n","                directory=os.path.normpath('C:/Users/natha/Desktop/Undergrad/Spring2022/MTH 596 PIC Math/Project - Group 2/Project/Forecasting'),\n","                project_name=project_title,\n","                overwrite=True)\n","            bayesian_opt_tuner.search(train_X, train_y, epochs=n_epochs,\n","                validation_data=(test_X, test_y),\n","                validation_split=0.2, verbose=1)\n","            bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=1)\n","            model = bayes_opt_model_best_model[0]\n","            model.save(modelPath)\n","\n","            string = \"Number epochs = \" + str(n_epochs)\n","            with open(path + \"\\\\note.txt\", 'w') as f:\n","                f.write(string)\n","\n","            # path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\keras_tuner_attempt1\\\\model\"\n","            # model = keras.models.load_model(modelPath)\n","\n","            # fitting model and predicting\n","            pred_y_inv, test_y_inv = predict(model, test_X)\n","            totalMSE = 0\n","            for k in range(10):\n","                totalMSE += mean_squared_error(pred_y_inv[k], test_y_inv[k])\n","            print(\"First 10 Avg MSE:\", totalMSE/10)\n","            totalAvgMSE = mseForecast(test_y_inv, pred_y_inv)\n","            saveResults(path, totalMSE/10, totalAvgMSE, n_epochs)\n","\n","            lst.append((n_epochs, round(totalMSE/10, 4), round(totalAvgMSE, 4)))\n","print(lst)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","%matplotlib qt\n","\n","# of form (hyperparameter, First MSE, Total Avg MSE) for a LSTM, LSTM, Dense(24), Activation('linear') model with 20 epochs\n","def weightedMSE(arr):\n","    results = []\n","    x1 = 0.3\n","    A = np.array([[x1, 1-x1]])\n","    for tup in arr:\n","        x = np.array([[tup[1]], [tup[2]]])\n","        b = np.matmul(A, x)\n","        results.append((tup[0], b[0][0]))\n","    results.sort(key = lambda x:x[0])\n","    return results\n","\n","bestHours = [(12, 158.3091, 81.6040), (18, 63.3076, 94.3174), (24, 44.8115, 99.3262), (30, 39.1650, 92.6211), (36, 36.1072, 85.1316), \n","    (42, 48.3943, 110.375), (45, 191.6520, 87.9457), (48, 23.7618, 73.4414), (54, 26.0222, 121.7316), \n","    (60, 42.7132, 82.8813), (72, 80.9572, 74.0058)]\n","# results = weightedMSE(bestHours)\n","\n","bestEpochs36 = [(5, 27.4284, 74.1717), (10, 22.9803, 67.2062), (15, 57.3269, 77.8016), (30, 56.5516, 148.8478),\n","    (3, 201.0433, 108.7818), (4, 93.3019, 67.6975), (6, 24.6906, 66.4358), (7, 39.6959, 70.8634), (8, 55.2934, 71.261), \n","    (9, 30.4693, 73.3233), (11, 56.3759, 77.3612), (12, 79.1437, 80.6395), (13, 84.1152, 74.4024), (14, 26.3952, 85.5936), \n","    (20, 104.5808, 87.708), (25, 78.8238, 86.8419)]\n","results36 = weightedMSE(bestEpochs36)\n","\n","bestEpochs48 = [(3, 45.6318, 66.2500), (4, 59.9746, 69.0089), (5, 66.8563, 70.0581), (6, 33.0507, 73.2355), \n","    (7, 35.5758, 69.0120), (8, 35.6232, 68.3578), (9, 42.2172, 79.0518), (10, 168.4794, 74.5590), (11, 13.3794, 75.8850),\n","    (12, 13.7495, 79.9994), (13, 84.1156, 73.6373), (14, 28.7678, 102.5217), (15, 51.4311, 86.8468), \n","    (20, 30.0797, 106.8292), (25, 20.7012, 93.9292), (30, 69.5697, 94.9882)]\n","results48 = weightedMSE(bestEpochs48)\n","\n","# graphing\n","for results, label, color in zip([results36, results48], [\"36 Hour\", \"48 Hour\"], [\"red\", \"blue\"]):\n","    x = [tup[0] for tup in results]\n","    y = [tup[1] for tup in results]\n","    plt.plot(x, y, label=label, color=color)\n","    plt.scatter(x, y, color=\"green\")\n","plt.legend()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","\n","# choose a number of time steps \n","# n_steps_in = 36\n","n_steps_in = 48\n","n_steps_out = 24\n","\n","# covert into input/output\n","X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n","print (\"X.shape\" , X.shape)                             # [rows, time lags backward, features]\n","print (\"y.shape\" , y.shape)                             # [rows, future time values]\n","\n","\n","n_tests = [8760, 6552, 4344, 2280]                      # for different validation windows throughout the year\n","epochList36 = [10, 5, 15, 30, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 20, 25]\n","epochList48 = [3, 5, 8, 10, 12, 15, 20, 25, 30, 4, 6, 7, 9, 11, 13, 14]\n","epoch48ValidationsIndicies = [13, 4]\n","epoch36ValidationsIndicies = [6, 0]\n","# 48 - 11, 12 epochs best\n","# 36 - 6, 10 epochs best\n","\n","epochsToUse = [epochList48[i] for i in epoch48ValidationsIndicies]\n","for n_epochs, folderIndex in zip(epochsToUse, epoch48ValidationsIndicies):\n","    path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\48 hour epoch tuning\\\\keras_tuner_attempt\"\n","    path += str(folderIndex+1)\n","    modelPath = path + \"\\\\model\"\n","    print(path)\n","    print(\"Number epochs:\", n_epochs)\n","\n","    testMSE = [[0, 0, 0], [1, 0, 0], [2, 0, 0], [3, 0, 0]]      # n_test period, first 96 hour prediction mse, shifted 480 hours forward 96 hour pred\n","    for n in range(len(n_tests)):               \n","        for s in range(2):\n","            # getting data for validation test\n","            train_X, train_y, test_X, test_y = sliding_window(X, y, n_tests[n], 480 * s)\n","\n","            # fitting model and predicting\n","            model = keras.models.load_model(modelPath)\n","            model.fit(train_X, train_y, epochs=n_epochs, verbose=1, validation_data=(test_X, test_y), validation_split=0.2)\n","            pred_y_inv, test_y_inv = predict(model, test_X)\n","            totalMSE = 0\n","            numHoursForward = 96\n","            for k in range(numHoursForward):\n","                totalMSE += mean_squared_error(pred_y_inv[k], test_y_inv[k])\n","            # print(\"First {n} Avg MSE: {mse}\".format(n=numHoursForward, mse=totalMSE/numHoursForward))\n","\n","            testMSE[n][s+1] = totalMSE/numHoursForward\n","\n","    # saving validation results\n","    with open(path + \"\\\\validation results.json\", \"w\") as f:\n","        json.dump(testMSE, f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def combineValidationResults(results):\n","    sum = 0\n","    for l in results:\n","        for i in range(1, 3):\n","            sum += l[i]\n","    print(\"MSE = \", sum/8)\n","    \n","epoch12Hour48Results = [[0, 13.265085957764384, 5.664549392861499], [1, 322.14254964814444, 89.00012063018733], \n","                        [2, 30.94971330178527, 15.442300796366476], [3, 23.369238580467254, 21.40317229597237]]\n","epoch11Hour48Results = [[0, 18.972753015342423, 5.010667427249979], [1, 275.55779577332555, 133.88350945889738], \n","                        [2, 33.828710449148744, 12.727969694411149], [3, 15.03309853350242, 24.127845879349817]]\n","combineValidationResults(epoch12Hour48Results)\n","combineValidationResults(epoch11Hour48Results)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["y_pred_inv: (8760, 24)\n","test_y_inv: (8760, 24)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.dates as mdates\n","from tensorflow import keras\n","%matplotlib qt\n","\n","# getting \n","n_steps_in = 48\n","n_steps_out = 24\n","X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n","train_X, train_y, test_X, test_y = sliding_window(X, y, 8760, 0)    # a full year of data\n","\n","modelPath = \"48 hour epoch tuning\\\\keras_tuner_attempt14\\\\model\"\n","model = model = keras.models.load_model(modelPath)\n","y_pred_inv, y_test_inv = predict(model, test_X)\n","\n","x = list(range(len(y_pred_inv)))\n","y = [mean_squared_error(y_pred_inv[i], y_test_inv[i]) for i in range(len(y_pred_inv))]\n","\n","\n","\n","# plotting the mse of 24 hour future predictions\n","df = pd.read_csv(\"Imputed Data.csv\", usecols=[\"DateTime\", \"Fire Rainfall (in)\", \"Fire 168 Hour Rainfall Aggregate\"], parse_dates=[\"DateTime\"])\n","dates = np.array(df[\"DateTime\"])\n","dates = dates[-1*test_y.shape[0]-24:-24]\n","\n","fig, ax = plt.subplots()\n","formatter = mdates.ConciseDateFormatter(ax.xaxis.get_major_locator(), formats=[\"%Y\", \"%Y-%b\", \"%b-%d\", \"%d %H:%M\", \"%d %H:%M\", \"%H:%M\"])\n","locator = mdates.AutoDateLocator()\n","ax.xaxis.set_major_formatter(formatter)\n","ax.xaxis.set_major_locator(locator)\n","fig.autofmt_xdate()\n","\n","ax.set_title(\"RMSE of 24 Hour Predictions\")\n","ax.plot(dates, np.sqrt(y))  # root mean square error\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# change date to cheek forecasted predictions vs actual values on that date\n","dateDesired = \"2021-10-23T00:00:00.000000000\"\n","index = 0\n","for i in range(len(dates)):\n","    if dateDesired == str(dates[i]):\n","        index = i\n","        break\n","print(dates[index])\n","print(y_pred_inv[index])\n","print(y_test_inv[index])\n","fig, ax = plt.subplots()\n","ax.plot(list(range(24)), y_test_inv[index], label=\"Actual\")\n","ax.plot(list(range(24)), y_pred_inv[index], label=\"Prediction\")\n","ax.legend()\n","ax.set_title(\"Results for \" + dateDesired[:10] + \" at \" + dateDesired[11:16])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from dcor import distance_correlation\n","from scipy.stats import pearsonr\n","\n","rmseArr = np.sqrt(y)\n","df = pd.read_csv(\"Imputed Data.csv\")\n","lst = []\n","for col in df.columns[1:]:\n","    arr = np.array(df[col])\n","    arr = arr[-1*test_y.shape[0]-24:-24]\n","    correlationValue = distance_correlation(arr, rmseArr)\n","    # correlationValue, _ = pearsonr(arr, rmseArr)\n","    lst.append((col, correlationValue))\n","\n","lst.sort(key = lambda x:x[1])\n","lst = lst[::-1]\n","for x in lst:\n","    print(x)"]}],"metadata":{"colab":{"name":"lstm2.ipynb","provenance":[]},"interpreter":{"hash":"18d1565e3dd2a1a1180dd629712b39ff168054eb513fda549cd851c01d6423bb"},"kernelspec":{"display_name":"Python 3.8.13 ('ML')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
