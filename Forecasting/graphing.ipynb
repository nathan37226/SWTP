{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def makeNullRects(dates, y):\n",
    "    '''This function returns a list of matplotlib.patches.Rectangles where\n",
    "    np.nan values are present in the y array. If values are consecutive,\n",
    "    the rectangles will widen as needed.\n",
    "    Note that this function is made for a figure with an x-axis of dates\n",
    "    Input:\n",
    "        dates: x axis date time values\n",
    "        y: y axis range values as np.array, contains np.nan values\n",
    "\n",
    "    Returns:\n",
    "        list of matplotlib.patches.Rectangles located where\n",
    "        y has np.nan values.\n",
    "\n",
    "    Rectangle Parameters in function:\n",
    "        opacityCoeff: how solid rectangles appear\n",
    "        longRectColor: the color of the rectangles with >=7 width\n",
    "        shortRectColor: the color of the rectanges with <7 width\n",
    "    '''\n",
    "    # setting up rectangle parameters\n",
    "    opacityCoeff = 0.5\n",
    "    longRectColor = \"red\"\n",
    "    shortRectColor = \"magenta\"\n",
    "\n",
    "    # prep work for creating rectangles for nan values\n",
    "    index = 0\n",
    "    yMax = np.nanmax(y)\n",
    "    yMin = np.nanmin(y)\n",
    "    rectHeight = yMax - yMin\n",
    "    yRectCoor = yMin\n",
    "    allRects = []   # this is what will be returned\n",
    "\n",
    "    # creating rectangle patches\n",
    "    while index < len(y):\n",
    "\n",
    "        # if nan exists, then need to create a rectangle patch\n",
    "        if np.isnan(y[index]):\n",
    "            xRectCoorIndex = index - 1\n",
    "\n",
    "            # condition for if first y value is nan\n",
    "            if index == 0:\n",
    "                xRectCoorIndex += 1\n",
    "            \n",
    "            # condition for if last y value is nan, assumes y is not len 2\n",
    "            elif index + 1 == len(y):\n",
    "                xRectCoor = mdates.date2num(dates[xRectCoorIndex])\n",
    "                coords = (xRectCoor, yRectCoor)\n",
    "                width = mdates.date2num(dates[xRectCoorIndex + 1]) - mdates.date2num(dates[xRectCoorIndex])\n",
    "                allRects.append(mpatches.Rectangle(coords, width, rectHeight, color=shortRectColor, alpha=opacityCoeff))\n",
    "                break\n",
    "                \n",
    "            # all other cases\n",
    "            xRectCoor = mdates.date2num(dates[xRectCoorIndex])\n",
    "\n",
    "            # checking finding how long the rectangle needs to be--how many consecutive null values\n",
    "            index += 1\n",
    "            while np.isnan(y[index]):\n",
    "                index += 1\n",
    "            rightEdgeIndex = mdates.date2num(dates[index])\n",
    "\n",
    "            # making rectangle\n",
    "            coords = (xRectCoor, yRectCoor)\n",
    "            width = rightEdgeIndex - xRectCoor\n",
    "            color = shortRectColor\n",
    "            if index - xRectCoorIndex > 5:\n",
    "                color = longRectColor\n",
    "            allRects.append(mpatches.Rectangle(coords, width, rectHeight, color=color, alpha=opacityCoeff))\n",
    "\n",
    "        else:\n",
    "            index += 1\n",
    "\n",
    "    return allRects\n",
    "\n",
    "def visualizeMissingValues(dates, arr, fig, ax, wantToMakeNullRects = True):\n",
    "    '''This function plots an array of values with datetime x axis values onto\n",
    "    a given axis, showing patches of null values if present.\n",
    "\n",
    "    Input:\n",
    "        dates: a numpy array of datetime objs that are the x-axis for the array with missing data to plot\n",
    "        arr: a numpy array that has missing data\n",
    "        fig: a matplotlib figure that contains the axis with the plot\n",
    "        ax: a matplotlib axis that will be plotted upon\n",
    "\n",
    "    Returns:\n",
    "        fig: edited matplotlib figure\n",
    "        ax: edited matplotlib axis\n",
    "    '''\n",
    "    ax.plot(dates, arr)\n",
    "\n",
    "    if wantToMakeNullRects:\n",
    "        rects = makeNullRects(dates, arr)\n",
    "        for rect in rects:\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    formatter = mdates.ConciseDateFormatter(ax.xaxis.get_major_locator(), formats=[\"%Y\", \"%Y-%b\", \"%b-%d\", \"%d %H:%M\", \"%d %H:%M\", \"%H:%M\"])\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    return fig, ax\n",
    "\n",
    "def plotImputedData(dates, nullArr, imputedArr, ax):\n",
    "    '''This graph plots imputed data as a green dashed line on a given\n",
    "    matplotlib axis.\n",
    "\n",
    "    Input:\n",
    "        dates: a numpy array of datetime objs that are the x-axis for the array with missing data to plot\n",
    "        nullArr: a numpy array that has missing data\n",
    "        imputedArr: a numpy array that has some of the missing values imputed\n",
    "        ax: a matplotlib axis that will be plotted upon\n",
    "    \n",
    "    Returns:\n",
    "        ax: edited matplotlib axis\n",
    "    '''\n",
    "    index = 0\n",
    "    while index < len(nullArr):                                 # looping through arr since it has the null values\n",
    "        if np.isnan(nullArr[index]):\n",
    "            # getting the width of the null area\n",
    "            lenForward = 0\n",
    "            while np.isnan(nullArr[index + lenForward]):\n",
    "                lenForward += 1\n",
    "\n",
    "            # domain to plot is [index-1, index+lenforward]\n",
    "            domain = list(range(index-1, index+lenForward+1))\n",
    "            datesToPlot = [dates[i] for i in domain]\n",
    "            pointsToPlot = [imputedArr[i] for i in domain]\n",
    "            ax.plot(datesToPlot, pointsToPlot, \"g--\")       # green dashed line\n",
    "\n",
    "            # moving index forward past null gap\n",
    "            index += lenForward\n",
    "        else:\n",
    "            index += 1\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "feature = \"SWTP Total Influent Flow\"\n",
    "# feature = \"SWTP Plant 2 Influent Flow\"\n",
    "# feature = \"SW_Peak_Flow\"\n",
    "# feature = \"SWTP Plant 1 Gravity Flow\"\n",
    "# feature = \"Total 168 Hour Rainfall Aggregate\"\n",
    "# feature = \"Ozark Aquifer Depth to Water Level (ft)\"\n",
    "# feature = \"Springfield Plateau Aquifer Depth to Water Level (ft)\"\n",
    "# feature = \"James Gauge Height (ft)\"\n",
    "# feature = \"Wilsons Gauge Height (ft)\"\n",
    "# feature = \"Fire 168 Hour Rainfall Aggregate\"\n",
    "# feature = \"Fire Rainfall (in)\"\n",
    "# feature = \"HourlyPressureChange\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Joined Influent and Rainfall and Weather and Groundwater and Creek Gauge.csv\", parse_dates=[\"DateTime\"])\n",
    "# df = pd.read_csv(\"Small Gap Imputed Data.csv\", parse_dates=[\"DateTime\"])\n",
    "# df = pd.read_csv(\"Small Gap Imputed Data Editted.csv\", parse_dates=[\"DateTime\"])\n",
    "# df[\"SWTP Total Influent Flow\"] = np.array([np.nan if x < 3.7 else x for x in df[\"SWTP Total Influent Flow\"]])\n",
    "\n",
    "\n",
    "# imputedDf = pd.read_csv(\"Small Gap Imputed Data.csv\")\n",
    "# imputedDf = pd.read_csv(\"Small Gap Imputed Data Editted.csv\")\n",
    "# imputedDf = pd.read_csv(\"test.csv\")\n",
    "imputedDf = pd.read_csv(\"Imputed Data.csv\")\n",
    "# imputedDf = pd.read_csv(\"New Imputed Data.csv\")\n",
    "\n",
    "\n",
    "dates = np.array(df[\"DateTime\"])\n",
    "imputedArr = np.array(imputedDf[feature])\n",
    "nullArr = deepcopy(np.array(df[feature]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig, ax = visualizeMissingValues(dates, nullArr, fig, ax)\n",
    "ax = plotImputedData(dates, nullArr, imputedArr, ax)\n",
    "# ax = plotValidatedValues(ax)\n",
    "# ax.scatter(testDf[\"DateTime\"], testDf[testDf.columns[-1]], s=8, color=\"red\", marker=\"x\")\n",
    "ax.set_ylabel(feature, fontsize=16)\n",
    "ax.set_title(\"Missing Data in the \" + feature, fontsize=20)\n",
    "# ax.set_xlim([18789.35639453603, 18838.599498146727])\n",
    "# ax.set_ylim([3.6382304053667287, 10.719097953664527])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_inv: (8760, 24)\n",
      "test_y_inv: (8760, 24)\n",
      "The average relative error on each forecast is: 0.11388417376972226\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "%matplotlib qt\n",
    "\n",
    "def sign_ae(x, y):\n",
    "    sign_x = tf.math.sign(x)\n",
    "    sign_y = tf.math.sign(y)\n",
    "    delta = x - y\n",
    "    return sign_x * sign_y * tf.math.abs(delta)\n",
    "    \n",
    "def linex_loss(delta, a=3., b=3.):\n",
    "    if a != 0 and b > 0:\n",
    "        # loss = 1/(a*a) * (tf.exp(a * delta) - a * delta - 1)\n",
    "        loss = b * (tf.exp(a * delta) - a * delta - 1)\n",
    "        return loss\n",
    "    else:\n",
    "        raise ValueError(\"linex_loss error with a or b\")\n",
    "          \n",
    "def linex_loss_val(y_true, y_pred):\n",
    "    delta = sign_ae(y_true, y_pred)\n",
    "    res = linex_loss(delta)\n",
    "    return tf.math.reduce_mean(res)\n",
    "\n",
    "\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def sliding_window(X, y, n_test, slide):\n",
    "    split_point = X.shape[0] - n_test + slide\n",
    "    train_X , train_y = X[:split_point, :] , y[:split_point, :]\n",
    "    test_X , test_y = X[split_point:, :] , y[split_point:, :]\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "def invNormalize(arr, minimum, maximum):\n",
    "    return (maximum - minimum) * arr + minimum\n",
    "\n",
    "def predict(model, test_X, test_y, fullData = False):\n",
    "    # to be able to inverse scale predictions\n",
    "    df = pd.read_csv(\"Train and Test Data.csv\", usecols=[\"SWTP Total Influent Flow\"])\n",
    "    if fullData:\n",
    "        df = pd.read_csv(\"Imputed Data.csv\", usecols=[\"SWTP Total Influent Flow\"])\n",
    "    arr = np.array(df[\"SWTP Total Influent Flow\"])\n",
    "    maximum = np.max(arr)\n",
    "    minimum = np.min(arr)\n",
    "\n",
    "    #predictions and rescaling to [min, max]\n",
    "    y_pred = model.predict(test_X)\n",
    "    y_pred_inv = np.array([invNormalize(x, minimum, maximum) for x in y_pred])\n",
    "    test_y_inv = np.array([invNormalize(x, minimum, maximum) for x in test_y])\n",
    "    print(\"y_pred_inv:\",y_pred_inv.shape)\n",
    "    print(\"test_y_inv:\",y_pred_inv.shape)\n",
    "    \n",
    "    return y_pred_inv, test_y_inv\n",
    "\n",
    "def nnse(predictions, targets):\n",
    "    term1 = np.sum((predictions - targets) ** 2)\n",
    "    term2 = np.sum((predictions - np.average(targets)) ** 2)\n",
    "    nse = 1 - (term1 / term2)\n",
    "    return 1 / (2 - nse)\n",
    "\n",
    "def nseAbs(predictions, targets):\n",
    "    term1 = np.sum(np.abs(predictions - targets))\n",
    "    term2 = np.sum(np.abs((predictions - np.average(targets))))\n",
    "    nse = 1 - (term1 / term2)\n",
    "    return nse\n",
    "\n",
    "def nse(predictions, targets):\n",
    "    term1 = np.sum((predictions - targets)**2)\n",
    "    term2 = np.sum((predictions - np.average(targets))**2)\n",
    "    nse = 1 - (term1 / term2)\n",
    "    return nse\n",
    "\n",
    "features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)', \n",
    "            \"Ozark Aquifer Depth to Water Level (ft)\", \"James Gauge Height (ft)\", 'HourlyStationPressure']\n",
    "dataset = pd.read_csv(\"Imputed Data.csv\", usecols=features)\n",
    "arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "dataset[\"Target\"] = arr         # adding another influent flow feature so that past values can be used to predict future values\n",
    "values = dataset.values\n",
    "scaler = MinMaxScaler()         # linear transformation of each feature from [min, max] to [0, 1]\n",
    "scaled = scaler.fit_transform(values)\n",
    "\n",
    "n_steps_in = 36\n",
    "n_steps_out = 24\n",
    "# n_steps_out = 5\n",
    "X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "train_X, train_y, test_X, test_y = sliding_window(X, y, 8760, 0)    # a full year of data\n",
    "\n",
    "\n",
    "modelPath = \"loss\\\\linex loss 1.1 best\\\\model\"\n",
    "# modelPath = \"loss\\\\linex loss 5 hours\\\\model\"\n",
    "model = keras.models.load_model(modelPath, custom_objects={'linex_loss_val': linex_loss_val})\n",
    "\n",
    "# modelPath = \"loss\\\\mse loss\\\\model\"\n",
    "# model = keras.models.load_model(modelPath)\n",
    "\n",
    "y_pred_inv, y_test_inv = predict(model, test_X, test_y, True)\n",
    "x = list(range(len(y_pred_inv)))\n",
    "y = [np.abs(y_pred_inv[i] - y_test_inv[i]) / y_test_inv[i] for i in range(len(y_pred_inv))]\n",
    "y = [np.average(yi) for yi in y]\n",
    "print(\"The average relative error on each forecast is:\", np.average(y))\n",
    "# y = [mean_squared_error(y_pred_inv[i], y_test_inv[i]) for i in range(len(y_pred_inv))]\n",
    "\n",
    "\n",
    "# plotting metric of future predictions\n",
    "df = pd.read_csv(\"Imputed Data.csv\", usecols=[\"DateTime\"], parse_dates=[\"DateTime\"])\n",
    "dates = np.array(df[\"DateTime\"])\n",
    "dates = dates[-1*test_y.shape[0]-n_steps_out:-1*n_steps_out]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "formatter = mdates.ConciseDateFormatter(ax.xaxis.get_major_locator(), formats=[\"%Y\", \"%Y-%b\", \"%b-%d\", \"%d %H:%M\", \"%d %H:%M\", \"%H:%M\"])\n",
    "locator = mdates.AutoDateLocator()\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# ax.set_title(\"RMSE of 24 Hour Forecasts per Hour\", fontsize=20)\n",
    "# ax.set_ylabel(\"RMSE\", fontsize=16)\n",
    "fig.suptitle(\"Absolute Relative Error of {n} Hour Forecasts\".format(n=n_steps_out), fontsize=20)\n",
    "ax.set_title(\"Average Absolute Relative Error = {avg}\".format(avg = round(np.average(y), 4)), fontsize=20)\n",
    "ax.set_ylabel(\"Relative Error\", fontsize=16)\n",
    "# ax.plot(dates, np.sqrt(y))  # root mean square error\n",
    "ax.plot(dates, y) \n",
    "# ax.set_xlim([18975.997366488264, 19011.15631337849])\n",
    "# ax.set_ylim([0, 0.45])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_inv: (8760, 1)\n",
      "test_y_inv: (8760, 1)\n",
      "The average relative error on each forecast is: 0.0034403925321362964\n",
      "The nse for inv y is: 0.9999228377545633\n",
      "The nnse for inv y is: 0.999922843708116\n",
      "The nse1 for inv y is: 0.9917735314212088\n",
      "The r2 for inv y is: 0.9999228371667168\n"
     ]
    }
   ],
   "source": [
    "features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)', \n",
    "            \"Ozark Aquifer Depth to Water Level (ft)\", \"James Gauge Height (ft)\", 'HourlyStationPressure']\n",
    "dataset = pd.read_csv(\"Imputed Data.csv\", usecols=features)\n",
    "arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "dataset[\"Target\"] = arr         # adding another influent flow feature so that past values can be used to predict future values\n",
    "values = dataset.values\n",
    "scaler = MinMaxScaler()         # linear transformation of each feature from [min, max] to [0, 1]\n",
    "scaled = scaler.fit_transform(values)\n",
    "\n",
    "n_steps_in = 36\n",
    "# n_steps_out = 24\n",
    "# n_steps_out = 5\n",
    "n_steps_out = 1\n",
    "X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "train_X, train_y, test_X, test_y = sliding_window(X, y, 8760, 0)    # a full year of data\n",
    "\n",
    "\n",
    "# modelPath = \"loss\\\\linex loss 1.1 best\\\\model\"\n",
    "# modelPath = \"loss\\\\linex loss 5 hours\\\\model\"\n",
    "modelPath = \"loss\\\\linex loss 1 hour\\\\model\"\n",
    "model = keras.models.load_model(modelPath, custom_objects={'linex_loss_val': linex_loss_val})\n",
    "\n",
    "y_pred_inv, y_test_inv = predict(model, test_X, test_y, True)\n",
    "x = list(range(len(y_pred_inv)))\n",
    "y = [np.abs(y_pred_inv[i] - y_test_inv[i]) / y_test_inv[i] for i in range(len(y_pred_inv))]\n",
    "y = [np.average(yi) for yi in y]\n",
    "print(\"The average relative error on each forecast is:\", np.average(y))\n",
    "# y = [mean_squared_error(y_pred_inv[i], y_test_inv[i]) for i in range(len(y_pred_inv))]\n",
    "\n",
    "print(\"The nse for inv y is:\", nse(y_pred_inv.flatten(), y_test_inv.flatten()))\n",
    "print(\"The nnse for inv y is:\", nnse(y_pred_inv.flatten(), y_test_inv.flatten()))\n",
    "print(\"The nse1 for inv y is:\", nseAbs(y_pred_inv.flatten(), y_test_inv.flatten()))\n",
    "print(\"The r2 for inv y is:\", r2_score(y_pred_inv.flatten(), y_test_inv.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8760\n",
      "8760\n"
     ]
    }
   ],
   "source": [
    "# plotting metric of future predictions\n",
    "df = pd.read_csv(\"Imputed Data.csv\", usecols=[\"DateTime\", \"SWTP Total Influent Flow\"], parse_dates=[\"DateTime\"])\n",
    "dates = np.array(df[\"DateTime\"])\n",
    "target = np.array(df[\"SWTP Total Influent Flow\"])\n",
    "dates = dates[-1*test_y.shape[0]-n_steps_out:-1*n_steps_out]\n",
    "target = target[-1*test_y.shape[0]-n_steps_out:-1*n_steps_out]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(dates, target, label=\"True\")\n",
    "for i in range(len(y_pred_inv)):\n",
    "    correspondingDates = dates[i:i+24]\n",
    "\n",
    "print(len(y_pred_inv))\n",
    "print(len(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date to cheek forecasted predictions vs actual values on that date\n",
    "year = \"2021\"\n",
    "month = \"11\"\n",
    "day = \"14\"\n",
    "hour = \"23\"\n",
    "dateDesired = year + \"-\" + month + \"-\" + day + \"T\" + hour + \":00:00.000000000\"  # string format of how timestamps are stored\n",
    "\n",
    "# getting index in dates array corresponding to the string dateDesired\n",
    "index = -1\n",
    "for i in range(len(dates)):\n",
    "    if dateDesired == str(dates[i]):\n",
    "        index = i\n",
    "        break\n",
    "if index == -1:\n",
    "    print(\"Could not find date!\")\n",
    "\n",
    "rmseValue = np.sqrt(mean_squared_error(y_test_inv[index], y_pred_inv[index]))\n",
    "relError = np.average(np.abs(y_test_inv[index] - y_pred_inv[index]) / y_test_inv[index])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(list(range(1, n_steps_out+1)), y_test_inv[index], label=\"Actual\")\n",
    "ax.plot(list(range(1, n_steps_out+1)), y_pred_inv[index], label=\"Prediction\", color=\"red\")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"Hours in Advance\", fontsize=14)\n",
    "ax.set_ylabel(\"Total Influent Flow Rate\", fontsize=14)\n",
    "fig.suptitle(\"Results for \" + dateDesired[:10] + \" at \" + dateDesired[11:16], fontsize=18)\n",
    "# ax.set_title(\"RMSE=\" + str(round(rmseValue, 4)), fontsize=18)\n",
    "ax.set_title(\"Relative Error=\" + str(round(relError, 4)), fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlyRMSE = np.sqrt(y)\n",
    "# plt.hist(hourlyRMSE, bins=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40, 50, 60], density=True)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(hourlyRMSE, bins = [0, 1, 2, 3, 5, 8, 10, 15, 20, 30, 40, 50, 60],density=False)\n",
    "\n",
    "for rect in ax.patches:\n",
    "    height = rect.get_height()\n",
    "    ax.annotate(f'{int(height)}', xy=(rect.get_x()+rect.get_width()/2, height), \n",
    "                xytext=(0, 5), textcoords='offset points', ha='center', va='bottom')\n",
    "ax.set_xticks([0, 1, 2, 3, 5, 8, 10, 15, 20, 30, 40, 50, 60])\n",
    "ax.set_ylabel(\"Frequency\", fontsize=16)\n",
    "ax.set_xlabel(\"RMSE Bins\", fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b104c07bfaa35248860e50ba1cfb46ca5e69d0db6474176440e1466f68c081"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
