{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "n_steps_in = 36\n",
    "n_steps_out = 24\n",
    "features = ['SWTP Total Influent Flow', 'Fire Rainfall (in)', 'Bingham Rainfall (in)', \n",
    "            \"Ozark Aquifer Depth to Water Level (ft)\", \"James Gauge Height (ft)\", 'HourlyStationPressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getShiftedRainfallData(t):\n",
    "    df = pd.read_csv(\"Imputed Data.csv\")\n",
    "    features = list(df.columns)\n",
    "    newFeatures = []\n",
    "    array2D = []\n",
    "    for feat in features:\n",
    "        arr = np.array(df[feat])\n",
    "        if feat.find(\"Rain\") > 1:\n",
    "            # remove last t hours\n",
    "            arr = arr[:-1*t]\n",
    "            newFeatures.append(feat + \" (t-\" + str(t) + \")\")\n",
    "        else:\n",
    "            # remove first t hours\n",
    "            arr = arr[t:]\n",
    "            newFeatures.append(feat)\n",
    "        array2D.append(arr)\n",
    "    newDf = pd.DataFrame(np.array(array2D).T, columns=newFeatures)        \n",
    "    return newDf\n",
    "\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def sliding_window(X, y, n_test, slide):\n",
    "    split_point = X.shape[0] - n_test + slide\n",
    "    train_X , train_y = X[:split_point, :] , y[:split_point, :]\n",
    "    test_X , test_y = X[split_point:, :] , y[split_point:, :]\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('units', min_value = 20, max_value = 180, step = 5), \n",
    "               activation = 'tanh', return_sequences = True, input_shape = (n_steps_in, len(features))))\n",
    "    model.add(LSTM(units = hp.Int('units', min_value = 20, max_value = 180, step = 5)))\n",
    "    model.add(Dense(24))   # for predicting 24 hours -- if desire more, change\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss = 'mse', metrics = 'mse', optimizer = keras.optimizers.Adam(\n",
    "        hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4, 1e-5])))\n",
    "    return model\n",
    "\n",
    "def invNormalize(arr, minimum, maximum):\n",
    "    return (maximum - minimum) * arr + minimum\n",
    "\n",
    "def predict(model, test_X, test_y, fullData = False):\n",
    "    # to be able to inverse scale predictions\n",
    "    df = pd.read_csv(\"Train and Test Data.csv\", usecols=[\"SWTP Total Influent Flow\"])\n",
    "    if fullData:\n",
    "        df = pd.read_csv(\"Imputed Data.csv\", usecols=[\"SWTP Total Influent Flow\"])\n",
    "    arr = np.array(df[\"SWTP Total Influent Flow\"])\n",
    "    maximum = np.max(arr)\n",
    "    minimum = np.min(arr)\n",
    "\n",
    "    #predictions and rescaling to [min, max]\n",
    "    y_pred = model.predict(test_X)\n",
    "    y_pred_inv = np.array([invNormalize(x, minimum, maximum) for x in y_pred])\n",
    "    test_y_inv = np.array([invNormalize(x, minimum, maximum) for x in test_y])\n",
    "    print(\"y_pred_inv:\",y_pred_inv.shape)\n",
    "    print(\"test_y_inv:\",y_pred_inv.shape)\n",
    "    \n",
    "    return y_pred_inv, test_y_inv\n",
    "\n",
    "def mseForecast(y, y_pred):\n",
    "    # change so is only for a range, i.e. first 72 days of test set\n",
    "    # mse is gonna scale really badly the further out it goes\n",
    "    # msut fix! cannot calculate mse from a full year, must use a rolling window\n",
    "    # that forecasts 3 days in advance, has new next hour put into it, then has next 3 day forecast one hour after\n",
    "    totalMSE = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        totalMSE += mean_squared_error(y[i], y_pred[i])\n",
    "    avgMSE = totalMSE / y.shape[0]\n",
    "    print(\"Total Avg MSE:\", avgMSE)\n",
    "    return avgMSE\n",
    "\n",
    "def getValidationData(filename, features, n_steps_in):\n",
    "    # getting data\n",
    "    dataset = pd.read_csv(filename, usecols=features)\n",
    "    arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "    dataset[\"Target\"] = arr         # adding another influent flow feature so that past values can be used to predict future values\n",
    "    values = dataset.values\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(values)\n",
    "\n",
    "    n_steps_out = 24\n",
    "    X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "    train_X, train_y, validate_X, validate_y = sliding_window(X, y, 8760, 0)    # 8760 means last year of data, for validation\n",
    "    return validate_X, validate_y\n",
    "\n",
    "def saveValidationResults(path, testMSE, validationMSE, n_epochs, hours=36):\n",
    "    txt = \"n_steps_in = \" + str(hours)\n",
    "    txt += \"\\nepochs = \" + str(n_epochs)\n",
    "    txt += \"\\nAvg Test MSE: \" + str(round(testMSE, 4))\n",
    "    txt += \"\\nAvg Validation MSE: \" + str(round(validationMSE, 4))\n",
    "    txt += \"\\n\\nForm:\\nLSTM\\nLSTM\\nDense(24)\\nActivation('linear')\"\n",
    "    with open(path + \"\\\\validation results.txt\", 'w') as f:\n",
    "        f.write(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shiftList = [36, 42]\n",
    "attemptList = [5, 6]\n",
    "n_epochs = 5\n",
    "n_steps_in = 36\n",
    "results = []\n",
    "\n",
    "for shift, attempt in zip(shiftList, attemptList):\n",
    "    # getting shifted rainfall data\n",
    "    dataset = getShiftedRainfallData(shift)\n",
    "    filename = \"Shifted Rainfall\\\\Imputed Data Shifted \" + str(shift) + \" Hours.csv\"\n",
    "    dataset.to_csv(filename, index=False)\n",
    "    newFeatures = [feat + \" (t-\" + str(shift) + \")\" if feat.find(\"Rain\") > 1 else feat for feat in features]\n",
    "    dropFeatures = [feat for feat in dataset.columns if feat not in newFeatures]\n",
    "    dataset = dataset.drop(columns=dropFeatures)\n",
    "    arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "    dataset[\"Target\"] = arr\n",
    "    dataset = dataset.drop(dataset.tail(364*25).index)\n",
    "\n",
    "    # making train and test datasets\n",
    "    values = dataset.values\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "    train_X, train_y, test_X, test_y = sliding_window(X, y, 365*24, 0)\n",
    "    path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\Shifted Rainfall\\\\keras_tuner_attempt\"\n",
    "    path += str(attempt)\n",
    "    modelPath = path + \"\\\\model\"\n",
    "    project_title = \"keras_tuner_attempt\" + str(attempt)\n",
    "\n",
    "    # tuning model with keras tuner\n",
    "    bayesian_opt_tuner = BayesianOptimization(\n",
    "        build_model,\n",
    "        objective='mse',\n",
    "        max_trials=3,\n",
    "        executions_per_trial=1,\n",
    "        directory=os.path.normpath('C:/Users/natha/Desktop/Undergrad/Spring2022/MTH 596 PIC Math/Project - Group 2/Project/Forecasting/Shifted Rainfall'),\n",
    "        project_name=project_title,\n",
    "        overwrite=True)\n",
    "    bayesian_opt_tuner.search(train_X, train_y, epochs=n_epochs,\n",
    "        validation_data=(test_X, test_y),\n",
    "        validation_split=0.2, verbose=1)\n",
    "    bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=1)\n",
    "    model = bayes_opt_model_best_model[0]\n",
    "    model.save(modelPath)\n",
    "    # model = keras.models.load_model(modelPath)\n",
    "\n",
    "    # getting predictions on test and validation set from created model\n",
    "    pred_y_inv, test_y_inv = predict(model, test_X, test_y)\n",
    "    totalAvgMSE = mseForecast(test_y_inv, pred_y_inv)\n",
    "\n",
    "    validate_X, validate_y = getValidationData(filename, newFeatures, n_steps_in)\n",
    "    y_pred_inv, y_validate_inv = predict(model, validate_X, validate_y, True)\n",
    "    validationAvgMSE = mseForecast(y_validate_inv, y_pred_inv)\n",
    "\n",
    "    saveValidationResults(path, totalAvgMSE, validationAvgMSE, n_epochs, n_steps_in)\n",
    "\n",
    "    results.append((shift, n_epochs, round(totalAvgMSE, 4), round(validationAvgMSE, 4)))\n",
    "\n",
    "\n",
    "print(results)\n",
    "with open(\"Shifted Rainfall\\\\results list.json\", 'w') as f:\n",
    "    json.dumps(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shiftResults = [(20, 5, 91.8418, 82.3813), (24, 5, 73.2098, 67.1881), (28, 5, 85.0717, 80.1233), (32, 5, 72.9596, 63.533), \n",
    "                (36, 5, 71.0614, 63.7691), (42, 5, 75.4345, 67.1984)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data with shifted rainfall\n",
    "hourList = [72]\n",
    "epochList = [5, 6, 7, 8]\n",
    "attemptList = list(range(2, len(epochList)+1))\n",
    "results = []\n",
    "\n",
    "for n_steps_in in hourList:\n",
    "    for attempt, n_epochs in zip(attemptList, epochList):\n",
    "\n",
    "        dataset = pd.read_csv(\"Train and Test Data.csv\", usecols=features)\n",
    "        arr = np.array(dataset[\"SWTP Total Influent Flow\"])\n",
    "\n",
    "\n",
    "        dataset[\"Target\"] = arr         # adding another influent flow feature so that past values can be used to predict future values\n",
    "        values = dataset.values\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        X, y = split_sequences(scaled, n_steps_in, n_steps_out)\n",
    "        train_X, train_y, test_X, test_y = sliding_window(X, y, 365*24, 0)\n",
    "\n",
    "\n",
    "        path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\\\\Shifted Rainfall\\\\keras_tuner_attempt\"\n",
    "        path += str(attempt) + \" \" + str(n_steps_in) + \" hours\"\n",
    "        modelPath = path + \"\\\\model\"\n",
    "        project_title = \"keras_tuner_attempt\" + str(attempt) + \" \" + str(n_steps_in) + \" hours\"\n",
    "\n",
    "\n",
    "        # tuning model with keras tuner\n",
    "        bayesian_opt_tuner = BayesianOptimization(\n",
    "            build_model,\n",
    "            objective='mse',\n",
    "            max_trials=3,\n",
    "            executions_per_trial=1,\n",
    "            directory=os.path.normpath('C:/Users/natha/Desktop/Undergrad/Spring2022/MTH 596 PIC Math/Project - Group 2/Project/Forecasting/Shifted Rainfall'),\n",
    "            project_name=project_title,\n",
    "            overwrite=True)\n",
    "        bayesian_opt_tuner.search(train_X, train_y, epochs=n_epochs,\n",
    "            validation_data=(test_X, test_y),\n",
    "            validation_split=0.2, verbose=1)\n",
    "        bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=1)\n",
    "        model = bayes_opt_model_best_model[0]\n",
    "        model.save(modelPath)\n",
    "        # model = keras.models.load_model(modelPath)\n",
    "\n",
    "\n",
    "        pred_y_inv, test_y_inv = predict(model, test_X, test_y)\n",
    "        totalAvgMSE = mseForecast(test_y_inv, pred_y_inv)\n",
    "\n",
    "        validate_X, validate_y = getValidationData(\"Imputed Data.csv\", features, n_steps_in)\n",
    "        y_pred_inv, y_validate_inv = predict(model, validate_X, validate_y, True)\n",
    "        validationAvgMSE = mseForecast(y_validate_inv, y_pred_inv)\n",
    "\n",
    "        saveValidationResults(path, totalAvgMSE, validationAvgMSE, n_epochs, n_steps_in)\n",
    "\n",
    "        results.append((n_steps_in, n_epochs, round(totalAvgMSE, 4), round(validationAvgMSE, 4)))\n",
    "print(results)\n",
    "with open(\"Shifted Rainfall\\\\results list.json\", 'w') as f:\n",
    "    json.dumps(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "newFeaturesResults36 = [(36, 4, 117.4312, 122.5767), (36, 5, 56.0574, 52.9312), (36, 6, 58.1944, 56.2634), (36, 7, 58.5114, 55.2089), \n",
    "                        (36, 8, 55.7228, 55.9546)]\n",
    "newFeaturesResults48 = [(48, 4, 63.5259, 60.7421), (48, 5, 57.8375, 56.3667), (48, 6, 66.1357, 66.4636), (48, 7, 56.5258, 56.9665), \n",
    "                        (48, 8, 80.874, 82.2395), (48, 9, 59.1129, 62.2371), (48, 10, 59.7826, 56.803), \n",
    "                        (48, 12, 61.6223, 64.5385), (48, 15, 62.6599, 64.2141), (48, 18, 89.9026, 92.5337)]\n",
    "newFeaturesResults72 = [(72, 4, 61.5909, 56.5136), (72, 5, 55.7488, 55.6569), (72, 6, 59.8857, 56.9455), (72, 7, 63.0854, 62.7871)]\n",
    "\n",
    "# graphing epochs\n",
    "fig, ax = plt.subplots()\n",
    "for newFeaturesResults, label in zip([newFeaturesResults36, newFeaturesResults48[:len(newFeaturesResults36)], \n",
    "    newFeaturesResults72], [\"36 Hours\", \"48 Hours\", \"72 Hours\"]):\n",
    "    x = [tup[1] for tup in newFeaturesResults]     # number of epochs\n",
    "    y = [tup[3] for tup in newFeaturesResults]     # validation mse\n",
    "    ax.plot(x, y, label=label)\n",
    "    ax.scatter(x, y)\n",
    "ax.grid()\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_title(\"LSTM Average MSE on Validation Set\", fontsize=20)\n",
    "ax.set_xlabel(\"Epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"Average Validation MSE\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\natha\\\\Desktop\\\\Undergrad\\\\Spring2022\\\\MTH 596 PIC Math\\\\Project - Group 2\\\\Project\\\\Forecasting\"\n",
    "path += \"\\\\feature tuning\\\\subset 5\\\\keras_tuner_attempt1\\\\model\"\n",
    "model = keras.models.load_model(path)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18d1565e3dd2a1a1180dd629712b39ff168054eb513fda549cd851c01d6423bb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
