{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Small Gaps\n",
    "##### Note: make sure no column under the feature name starts with a null value\n",
    "##### HourlyPressureChange and HourlyPresureTendency are like that. Change to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def imputeArrValues(index, arr, width):\n",
    "    '''Helper function for imputeArr, which is used inside smallGapImputation.\n",
    "    '''\n",
    "    smallGapFailed = False      # represents if there's a small enough gap but was just not able to fill it in due to missing data on either side\n",
    "    if width < 5 and index + width + 1 < len(arr):\n",
    "        # interpolate data!\n",
    "        # want 6 values before and after gap for cubic spline, but more might be better\n",
    "        # technically only 4 points are needed, but more might help impute better\n",
    "        lenForwards = 1\n",
    "        while lenForwards < 10 and not np.isnan(arr[index + width + lenForwards]):\n",
    "            lenForwards += 1\n",
    "\n",
    "        lenBackwards = 1\n",
    "        while lenBackwards < 10 and not np.isnan(arr[index - lenBackwards]):\n",
    "            lenBackwards += 1\n",
    "\n",
    "        # getting values set up for imputation\n",
    "        nullRange = list(range(index, index + width, 1))\n",
    "        totalRange = list(range(index - lenBackwards + 1, index + width + lenForwards, 1))\n",
    "        x = [x for x in totalRange if x not in nullRange]       # impution data points\n",
    "        y = [arr[i] for i in x]                                 # function values of impution data points\n",
    "        imputionRange = list(range(index, index + width, 1))\n",
    "\n",
    "        if lenForwards > 5 and lenBackwards > 5:\n",
    "            # cubic spline impution\n",
    "            cspline = CubicSpline(x, y)\n",
    "            for i in imputionRange:\n",
    "                arr[i] = cspline(i)         # replacing null values in array with interpolated values\n",
    "        \n",
    "        elif (lenForwards > 5 and lenBackwards > 2) or (lenForwards > 2 and lenBackwards > 5):\n",
    "            # cubic spline but data lies mostly on one end\n",
    "            # handles cases such as [x1, x2, x3, x4, x5, x6, nan, nan, nan, x7, x8, x9]\n",
    "            cspline = CubicSpline(x, y)\n",
    "            for i in imputionRange:\n",
    "                arr[i] = cspline(i)         # replacing null values in array with interpolated values\n",
    "        \n",
    "        elif width < 3:                     # not enough values preceeding and succeeding null gap for cubic spline, but null gap is small\n",
    "            # linear impution\n",
    "            linInterplator = interp1d(x, y)\n",
    "            for i in imputionRange:\n",
    "                arr[i] = linInterplator(i)         # replacing null values in array with interpolated values\n",
    "        else:\n",
    "            smallGapFailed = True\n",
    "\n",
    "    return arr, smallGapFailed\n",
    "\n",
    "def imputeArr(arr):\n",
    "    '''Helper function for smallGapImputation.\n",
    "    '''\n",
    "    # looping through each index in the array to find nulls\n",
    "    index = 0\n",
    "    reimputeColumn = False\n",
    "    while index < len(arr):\n",
    "        # a null has been found\n",
    "        if np.isnan(arr[index]):\n",
    "            # finding how many consecutive nulls are present\n",
    "            width = 1\n",
    "            while index + width < len(arr) and np.isnan(arr[index + width]):    #not reach the end and is still null\n",
    "                width += 1\n",
    "\n",
    "            # imputation happens with this helper function\n",
    "            arr, smallGapFailed = imputeArrValues(index, arr, width)\n",
    "            if smallGapFailed:\n",
    "                reimputeColumn = True\n",
    "\n",
    "            # move index forward past gap, continue searching and imputing\n",
    "            index += width\n",
    "\n",
    "        # no null gap, so continue searching\n",
    "        else:\n",
    "            index += 1\n",
    "\n",
    "    return arr, reimputeColumn\n",
    "\n",
    "def smallGapImputation(df):\n",
    "    '''This function takes in a dataframe that has null values present.\n",
    "    For each column, this function will attempt to fill in null gaps of size 5\n",
    "    or less with cubic spline impution or, if that's not available and the gap is < 3,\n",
    "    linear impution.\n",
    "\n",
    "    Input: pandas dataframe whose columns have null values, first column is timestamp\n",
    "    Returns: df with imputed data\n",
    "    '''\n",
    "    for col in df.columns[1:]:\n",
    "        # getting an array from the dataframe\n",
    "        arr = np.array(df[col])\n",
    "\n",
    "        # to run through a column multiple times if necessary\n",
    "        count = 0\n",
    "        while count < 5:\n",
    "            arr, reimputeColumn = imputeArr(arr)\n",
    "            if reimputeColumn:\n",
    "                count += 1\n",
    "                # print(\"Reimputing {col}\".format(col=col))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # replacing arr in df with arr with interpolated values\n",
    "        df[col] = arr\n",
    "\n",
    "    return df\n",
    "\n",
    "def imputeSmallGaps():\n",
    "\n",
    "    # get original combined data with all null values\n",
    "    df = pd.read_csv(\"Joined Influent and Rainfall and Weather and Groundwater and Creek Gauge.csv\", parse_dates=[\"DateTime\"])\n",
    "    df[\"SWTP Total Influent Flow\"] = np.array([np.nan if x < 3.7 else x for x in df[\"SWTP Total Influent Flow\"]])\n",
    "\n",
    "    # imputing all small gaps with cubic splines and linear lines, gaps of size < 5\n",
    "    df = smallGapImputation(df)\n",
    "\n",
    "    # adding year, month, day, and hour columns\n",
    "    df[\"Year\"] = df[\"DateTime\"].dt.year\n",
    "    df[\"Month\"] = df[\"DateTime\"].dt.month\n",
    "    df[\"Week Day\"] = df[\"DateTime\"].dt.dayofweek\n",
    "    df[\"Hour\"] = df[\"DateTime\"].dt.hour\n",
    "    df[\"Week\"] = df[\"DateTime\"].dt.week\n",
    "\n",
    "    # saving imputed data\n",
    "    df.to_csv(\"Small Gap Imputed Data.csv\", index=False)\n",
    "\n",
    "imputeSmallGaps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Small Gap Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "def normalize(arr, maximum, minimum):\n",
    "    '''Array with range [a, b] scaled to [0, 1]'''\n",
    "    if maximum == minimum:\n",
    "        return arr\n",
    "    normArr = [(x - minimum) / (maximum - minimum) for x in arr]\n",
    "    return normArr\n",
    "\n",
    "def createIndicies(index, gapSize):\n",
    "    '''Creates a list of indicies centered around 'index' with size 'gapSize' to later turn null'''\n",
    "    indicies = []\n",
    "    if gapSize % 2 != 0:\n",
    "        maxVal = int(gapSize/2) + index\n",
    "        minVal = int(gapSize/2) * -1 + index\n",
    "        indicies = [i for i in range(minVal, maxVal + 1)]\n",
    "    else:\n",
    "        maxVal = int(gapSize/2) + index\n",
    "        minVal = int(gapSize/2) * -1 + index + 1\n",
    "        indicies = [i for i in range(minVal, maxVal + 1)]\n",
    "    return indicies\n",
    "\n",
    "def testSmallSpot(arr, index, length):\n",
    "    '''Tests if the spot is valid to turn into a small gap'''\n",
    "    if len(arr) - index > 25 and index > 25:    # not in last 25 or in first 25 indicies\n",
    "        # tests if 5 vals before and after index are null\n",
    "        # also if index is null\n",
    "        for i in range(length):\n",
    "            if np.isnan(arr[index + i]):\n",
    "                return False          \n",
    "            if np.isnan(arr[index - i]):\n",
    "                return False\n",
    "        return True                         # only if all are not null will this be hit\n",
    "    return False\n",
    "\n",
    "def testSmallGap(feature, count = 5, smallGapsPerTest = 30):\n",
    "    '''Performs small gap validation on a given feature.\n",
    "    Returns validation metrics'''\n",
    "    # getting data\n",
    "    df = pd.read_csv(\"Joined Influent and Rainfall and Weather and Groundwater and Creek Gauge.csv\", \n",
    "        usecols = [\"DateTime\", feature])\n",
    "    arr = np.array(df[feature])\n",
    "\n",
    "    # to remove sus values in a particular feature\n",
    "    if feature == \"SWTP Total Influent Flow\":\n",
    "        arr = np.array([np.nan if x < 3.7 else x for x in df[\"SWTP Total Influent Flow\"]])\n",
    "\n",
    "    # starting validation\n",
    "    totalR = 0\n",
    "    totalMSE = 0\n",
    "    totalNMSE = 0\n",
    "    breakCount = 0                              # in case not able to get as many desired spots\n",
    "    for i in range(count):                      # average of how many validation tests\n",
    "\n",
    "        spots = []                              # append all initial indicies to be turned null here\n",
    "        validationIndicies = []                 # append all indicies forced to null here\n",
    "        nullArr = deepcopy(arr)                 # will be adding null values to here for validation\n",
    "        \n",
    "        # getting the spots to make null\n",
    "        while len(spots) < smallGapsPerTest:\n",
    "            randIndex = np.random.randint(0, len(arr))\n",
    "\n",
    "            # testing if the randomly generated index is a valid spot\n",
    "            if testSmallSpot(nullArr, randIndex, 7):\n",
    "                spots.append(randIndex)\n",
    "                nullGapWidth = np.random.randint(1, 5) # either 1, 2, 3, or 4\n",
    "\n",
    "                # making a null gap where data was previously\n",
    "                indiciesToTurnNull = createIndicies(randIndex, nullGapWidth)\n",
    "                for i in indiciesToTurnNull:\n",
    "                    nullArr[i] = np.nan\n",
    "                    validationIndicies.append(i)\n",
    "\n",
    "            # in case while loop is infinite\n",
    "            if breakCount > 5000000:             # just some large number\n",
    "                raise NotImplementedError(\"Failed to create all small null gaps\")\n",
    "        \n",
    "            breakCount += 1\n",
    "        \n",
    "        # inputing new array with created null values\n",
    "        df[feature] = nullArr\n",
    "\n",
    "        # imputing and getting r2 values\n",
    "        df = smallGapImputation(df)\n",
    "        imputedArr = df[feature]\n",
    "\n",
    "        prevValues = [arr[i] for i in validationIndicies]\n",
    "        imputedValues = [imputedArr[i] for i in validationIndicies]\n",
    "        normPrevValues = normalize(prevValues, np.max(prevValues), np.min(prevValues))\n",
    "        normImputedValues = normalize(imputedValues, np.max(prevValues), np.min(prevValues))\n",
    "\n",
    "        totalR += r2_score(prevValues, imputedValues)\n",
    "        totalMSE += mean_squared_error(prevValues, imputedValues)\n",
    "        totalNMSE += mean_squared_error(normPrevValues, normImputedValues)\n",
    "\n",
    "    # print(\"Avg r^2 for {col} is: \\n{val}\".format(col = feature, val = totalR / count))\n",
    "\n",
    "    return totalR / count, totalMSE / count, totalNMSE / count\n",
    "\n",
    "def performSmallGapValidation(featDf):\n",
    "    '''Performs small gap validation'''\n",
    "    avgR2Vals = []\n",
    "    avgMSEVals = []\n",
    "    avgNMSEVals = []\n",
    "    allFeatures = np.array(featDf[\"Feature\"])\n",
    "    for feature in allFeatures:\n",
    "        # print(feature)\n",
    "        avgR2, avgMSE, avgNMSE = testSmallGap(feature, 10, 100)\n",
    "        avgR2Vals.append(avgR2)       # r2 is avg of 15 tests, 25 small null gaps created per test\n",
    "        avgMSEVals.append(avgMSE)\n",
    "        avgNMSEVals.append(avgNMSE)\n",
    "    featDf[\"Avg R2\"] = avgR2Vals\n",
    "    featDf[\"Avg MSE\"] = avgMSEVals\n",
    "    featDf[\"Avg NMSE\"] = avgNMSEVals\n",
    "    featDf.to_csv(\"Validated Features.csv\", index=False)\n",
    "    return featDf\n",
    "\n",
    "featDf = pd.read_csv(\"Features.csv\")\n",
    "featDf = performSmallGapValidation(featDf)\n",
    "print(featDf)\n",
    "featDf.to_csv(\"Small Gap Validation.csv\", index=False)\n",
    "\n",
    "# # df = pd.read_csv(\"Validated Features.csv\")\n",
    "# df = pd.read_csv(\"Small Gap Validation.csv\")\n",
    "# df.plot.bar(x=\"Feature\", y=\"Avg NMSE\", rot=37)\n",
    "# # df.plot.bar(x=\"Feature\", y=\"Avg R2\", rot=37)\n",
    "# # plt.bar([i for i in range(len(df[\"Feature\"]))], df[\"Avg R2\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Gap Imputation Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from dcor import distance_correlation\n",
    "%matplotlib qt\n",
    "\n",
    "def normalize(arr, maximum, minimum):\n",
    "    '''Used in finding normalized MSE'''\n",
    "    if maximum == minimum:\n",
    "        return np.zeros_like(arr)   # since all the same value, make all zeros\n",
    "    normArr = [(x - minimum) / (maximum - minimum) for x in arr]\n",
    "    return normArr\n",
    "\n",
    "def findNulls(arr):\n",
    "    index = 0\n",
    "    pairs = []                  # formatted like [(start index, num values)]\n",
    "    while index < len(arr):\n",
    "        if np.isnan(arr[index]):\n",
    "            width = 1\n",
    "            try:\n",
    "                while np.isnan(arr[index + width]):\n",
    "                    width += 1\n",
    "            except IndexError:  # means end of array is null\n",
    "                break\n",
    "            pairs.append((index, width))\n",
    "            index += width\n",
    "        else:\n",
    "            index += 1\n",
    "\n",
    "    # for pair in pairs:\n",
    "    #     print(\"Null values starting at index: {i}. {w} total nulls\".format(i=pair[0], w=pair[1]))\n",
    "    return pairs\n",
    "\n",
    "def createIndicies(index, gapSize):\n",
    "    indicies = []\n",
    "    if gapSize % 2 != 0:\n",
    "        maxVal = int(gapSize/2) + index\n",
    "        minVal = int(gapSize/2) * -1 + index\n",
    "        indicies = [i for i in range(minVal, maxVal + 1)]\n",
    "    else:\n",
    "        maxVal = int(gapSize/2) + index\n",
    "        minVal = int(gapSize/2) * -1 + index + 1\n",
    "        indicies = [i for i in range(minVal, maxVal + 1)]\n",
    "    return indicies\n",
    "\n",
    "def testLargeSpot(arr, index, length):\n",
    "    if len(arr) - index - 25 > length and index + 25 > length: \n",
    "        for i in range(length):\n",
    "            if np.isnan(arr[index + i]):\n",
    "                return False          \n",
    "            if np.isnan(arr[index - i]):\n",
    "                return False\n",
    "        return True                         # only if all are not null will this be hit\n",
    "    return False\n",
    " \n",
    "def createLargeGapIndicies(arr, arrWithNulls, test_size):\n",
    "    existingLargeNullGapLengths = [x[1] for x in findNulls(arrWithNulls) if x[1] > 5]\n",
    "    minGapSize = 5\n",
    "    maxGapSize = max(existingLargeNullGapLengths, default=150)\n",
    "    # print(\"Max gap size:\", maxGapSize)\n",
    "    spots = []                              # append all initial indicies to be turned null here\n",
    "    validationIndicies = []                 # append all indicies forced to null here\n",
    "    nullArr = deepcopy(arr)                     # will be adding null values to here for validation\n",
    "\n",
    "    # getting the spots to make null\n",
    "    breakout = 0\n",
    "    totalCreatedNulls = 0\n",
    "    hasLargestGap = False\n",
    "    while totalCreatedNulls / len(arr) < test_size and breakout < 500000:\n",
    "        # randomly getting index and how large of gap to create\n",
    "        randIndex = np.random.randint(0, len(arr))\n",
    "        randGapSize = np.random.randint(minGapSize, maxGapSize)\n",
    "        if not hasLargestGap:\n",
    "            randGapSize = maxGapSize\n",
    "            hasLargestGap = True\n",
    "\n",
    "        # testing if spot is valid\n",
    "        if testLargeSpot(nullArr, randIndex, randGapSize):\n",
    "            spots.append(randIndex)\n",
    "            totalCreatedNulls += randGapSize\n",
    "            \n",
    "            # making a null gap where data was previously\n",
    "            indiciesToTurnNull = createIndicies(randIndex, randGapSize)\n",
    "            for i in indiciesToTurnNull:\n",
    "                nullArr[i] = np.nan\n",
    "                validationIndicies.append(i)\n",
    "\n",
    "        breakout += 1\n",
    "    return validationIndicies\n",
    "\n",
    "def train_test_split_largeGap(data, target, targetWithNulls, test_size = 0.1):\n",
    "    trainX, trainY = deepcopy(data), deepcopy(target)\n",
    "    testX, testY = [], []\n",
    "    # targetWithNulls = []  # sometimes, the default of 150 will be better, rather than however large the max large gap in targetWithNulls is\n",
    "    validationIndicies = np.sort(np.array(createLargeGapIndicies(target, targetWithNulls, test_size)))\n",
    "    for index in validationIndicies:\n",
    "        testX.append(data[index])\n",
    "        testY.append(target[index])\n",
    "    testX, testY = np.array(testX), np.array(testY)\n",
    "    trainX = np.delete(trainX, validationIndicies, 0)\n",
    "    trainY = np.delete(trainY, validationIndicies)\n",
    "    return trainX, testX, trainY, testY, validationIndicies\n",
    "\n",
    "\n",
    "\n",
    "def saveForestPredictions(originFilename, predFilename, feature):\n",
    "    # building dataframes\n",
    "    originDf = pd.read_csv(originFilename, parse_dates=[\"DateTime\"])\n",
    "    predDf = pd.read_csv(predFilename, parse_dates=[\"DateTime\"])\n",
    "\n",
    "    # getting arrays from dataframes\n",
    "    predArr = np.array(predDf[predDf.columns[-1]])\n",
    "    predDates = np.array(predDf[\"DateTime\"])\n",
    "    originArr = np.array(originDf[feature])\n",
    "    originDates = np.array(originDf[\"DateTime\"])\n",
    "\n",
    "    # copying over predicted values\n",
    "    for i in range(len(predDates)):\n",
    "        locations = np.nonzero(originDates == predDates[i])\n",
    "        originArr[locations[0]] = predArr[i]\n",
    "\n",
    "    # replacing array in df and saving to csv\n",
    "    newDf = pd.read_csv(originFilename, usecols=[\"DateTime\"])\n",
    "    newDf[targetFeature] = originArr\n",
    "    newDf.to_csv(feature + \" imputed data.csv\", index=False)\n",
    "\n",
    "def findSharedNullValueFeatures(df, targetFeature, tol = 0.5):\n",
    "    # getting location of target nulls\n",
    "    target = np.array(df[targetFeature])\n",
    "    targetNulls = np.nonzero(np.isnan(target))[0]\n",
    "\n",
    "    # getting columns\n",
    "    columnList = [x for x in df.columns if x not in [\"DateTime\", targetFeature]]\n",
    "    badColumns = []\n",
    "    for col in columnList:\n",
    "        # getting how many null values are shared between target and each feature\n",
    "        arr = np.array(df[col])\n",
    "        arrNulls = np.nonzero(np.isnan(arr))[0]\n",
    "        sharedNullIncidiesCount = len(np.intersect1d(targetNulls, arrNulls))\n",
    "        \n",
    "        # if share tol% or more null features, then leave that feature out\n",
    "        if sharedNullIncidiesCount > tol * len(targetNulls):\n",
    "            badColumns.append(col)\n",
    "\n",
    "    return badColumns\n",
    "\n",
    "def separateDataIntoSets(target, data, dates):\n",
    "    # getting null locations from target feature array\n",
    "    targetNulls = np.nonzero(np.isnan(target))[0]\n",
    "\n",
    "    # getting data corresponding to where the target feature is null\n",
    "    testData = []\n",
    "    testDates = []\n",
    "    for i in targetNulls:\n",
    "        testData.append(data[i])\n",
    "        testDates.append(dates[i])\n",
    "    testData = np.array(testData)\n",
    "    testDates = np.array(testDates)\n",
    "\n",
    "    # deleting all indicies that are null from target and data\n",
    "    trainTarget = np.delete(target, targetNulls)\n",
    "    trainData = np.delete(data, targetNulls, 0)     # the 0 means delete a row\n",
    "\n",
    "    if len(testData) == 0:\n",
    "        return trainData, trainTarget, np.array([]), np.array([])\n",
    "\n",
    "    # finding where null values are present in the data that has a not null target corresponding to it\n",
    "    badIndicies = []\n",
    "    for col in range(len(trainData[0])):\n",
    "        for row in range(len(trainData)):\n",
    "            if np.isnan(trainData[row][col]):\n",
    "                badIndicies.append(row)\n",
    "\n",
    "    # removing those indicies so that rand forest can train\n",
    "    if len(badIndicies) > 0:\n",
    "        badIndicies = np.unique(np.array(badIndicies))\n",
    "        trainData = np.delete(trainData, badIndicies, 0)\n",
    "        trainTarget = np.delete(trainTarget, badIndicies)\n",
    "\n",
    "    # finding where null values are present in the data that has a null target corresponding to it\n",
    "    badIndicies = []\n",
    "    for col in range(len(testData[0])):\n",
    "        for row in range(len(testData)):\n",
    "            if np.isnan(testData[row][col]):\n",
    "                badIndicies.append(row)\n",
    "    \n",
    "    # removing those indicies so that rand forest can predict\n",
    "    if len(badIndicies) > 0:\n",
    "        badIndicies = np.unique(np.array(badIndicies))\n",
    "        testData = np.delete(testData, badIndicies, 0)\n",
    "        testDates = np.delete(testDates, badIndicies)\n",
    "    \n",
    "    return trainData, trainTarget, testData, testDates\n",
    "\n",
    "def scalePredictedValues(dates, fullTarget, predictedValues, predictedIndicies, scaleFactor):\n",
    "    # copying over predicted values\n",
    "    fullPredTarget = deepcopy(fullTarget)\n",
    "    validNullTarget = deepcopy(fullTarget)                 # used in linear scaling step to find where to scale\n",
    "    predDates = [dates[i] for i in predictedIndicies]\n",
    "    for i in range(len(predDates)):\n",
    "        locations = np.nonzero(dates == predDates[i])\n",
    "        try:\n",
    "            fullPredTarget[locations[0]] = predictedValues[i]\n",
    "        except:\n",
    "            print(len(fullTarget))\n",
    "            print(len(fullPredTarget))\n",
    "            print(len(dates))\n",
    "            print(len(predictedValues))\n",
    "            print(locations)\n",
    "            raise NotImplementedError(\"Blah\")\n",
    "        validNullTarget[locations[0]] = np.nan\n",
    "    \n",
    "    scalingSpots = findNulls(validNullTarget)\n",
    "    for tup in scalingSpots:\n",
    "        # getting points to make trendline, using 10 points\n",
    "        points = []\n",
    "        for i in range(1, 11):                   # at tup[0], target is null, so start with i = 1\n",
    "            xBefore = tup[0] - i\n",
    "            yBefore = fullTarget[xBefore]\n",
    "            xAfter = tup[0] + tup[1] - 1 + i\n",
    "            yAfter = fullTarget[xAfter]\n",
    "\n",
    "            # condition if cannot get all 10 points desired due to other null gaps close to current gap\n",
    "            if np.isnan(yBefore) or np.isnan(yAfter):\n",
    "                break\n",
    "            \n",
    "            # appending points\n",
    "            points.append((xBefore, yBefore))\n",
    "            points.append((xAfter, yAfter))\n",
    "\n",
    "        # creating trendline from points\n",
    "        trendlineCoeffs = np.polyfit(np.array([p[0] for p in points]), np.array([p[1] for p in points]), 1)\n",
    "        trendline = np.poly1d(trendlineCoeffs)\n",
    "\n",
    "        # scaling predicted values\n",
    "        for i in range(tup[1]):\n",
    "            fullPredTarget[tup[0] + i] = trendline(tup[0] + i) + scaleFactor * (fullPredTarget[tup[0] + i] - trendline(tup[0] + i))\n",
    "\n",
    "    # slicing out scaled prediced values\n",
    "    scaledPredictedValues = [fullPredTarget[i] for i in predictedIndicies]\n",
    "\n",
    "    return scaledPredictedValues\n",
    "\n",
    "def tuneForest(df, targetFeature):\n",
    "    # getting data to use\n",
    "    target = np.array(df[targetFeature])\n",
    "    dates = np.array(df[\"DateTime\"])\n",
    "    badFeaturesToUse = findSharedNullValueFeatures(df, targetFeature, 0)\n",
    "    badFeaturesToUse += [targetFeature, \"DateTime\"]\n",
    "    df = df.drop(columns=badFeaturesToUse)\n",
    "    data = df.to_numpy()\n",
    "\n",
    "    # splitting data up into respective datasets\n",
    "    validData, validTarget, nullData, nullDates = separateDataIntoSets(target, data, dates)\n",
    "\n",
    "    # setting up possible hyperparameter values\n",
    "    maxNumFeatures = list(range(5, int(len(df.columns)/1.5) + 1, 2))\n",
    "    maxDepths = [5, 7, 10]\n",
    "    numTrees = [75, 100]\n",
    "    scaleFactors = [.1, .2, .5, 1]\n",
    "    # numTrees = [75]\n",
    "    # maxNumFeatures = [11]\n",
    "    # maxDepths = [10]\n",
    "    # scaleFactors = [1]\n",
    "    # maxNumFeatures = list(range(3, 11, 2))\n",
    "    \n",
    "    \n",
    "    numValidations = 3\n",
    "    avgDict = {}\n",
    "    for n in range(numValidations):\n",
    "        # splitting up known data into training and validation sets\n",
    "        XTrain, XTest, YTrain, YTest, testIndicies = train_test_split_largeGap(validData, validTarget, target, test_size=0.20)\n",
    "\n",
    "        # grid searching for best combination\n",
    "        combos = []\n",
    "        for numFeats in maxNumFeatures:\n",
    "            for maxDepth in maxDepths:\n",
    "                for trees in numTrees:\n",
    "                    # only previous for loops impact the random forest's performance\n",
    "                    imputer = RandomForestRegressor(n_estimators=trees, max_depth=maxDepth, max_features=numFeats)\n",
    "                    imputer.fit(XTrain, YTrain)\n",
    "                    predictedValues = imputer.predict(XTest)\n",
    "                    # print(mean_squared_error(YTest, predictedValues))   # original mse without scaling\n",
    "\n",
    "                    for scale in scaleFactors:\n",
    "                        # linear trendline scaling\n",
    "                        scaledPredictedValues = scalePredictedValues(dates, validTarget, predictedValues, testIndicies, scale)\n",
    "                        mse = mean_squared_error(YTest, scaledPredictedValues)\n",
    "                        combos.append((mse, (numFeats, maxDepth, trees, scale)))\n",
    "        \n",
    "        # adding combos info to the average dictionary\n",
    "        for tup in combos:\n",
    "            if tup[1] in avgDict.keys():\n",
    "                avgDict[tup[1]] += tup[0]\n",
    "            else:\n",
    "                avgDict[tup[1]] = tup[0]\n",
    "    \n",
    "    # getting info out of avgDict and into a list to sort\n",
    "    combos = [(tup[1]/numValidations, tup[0]) for tup in avgDict.items()]\n",
    "\n",
    "    combos.sort(key=lambda a: a[0])\n",
    "    for tup in combos:\n",
    "        print(tup[0], \"with {f} features, max depth of {d}, {t} trees, and a scale factor of {s}\".format(\n",
    "            f = tup[1][0], d = tup[1][1], t = tup[1][2], s = tup[1][3]))\n",
    "\n",
    "    # creating best tree predictions\n",
    "    print(\"Best hyperparas are: {f} features, max depth of {d}, {t} trees, and a scale factor of {s}\".format(\n",
    "        f = combos[0][1][0], d = combos[0][1][1], t = combos[0][1][2], s = combos[0][1][3]))\n",
    "    print(\"With an MSE of: {m}\".format(m = combos[0][0]))\n",
    "    imputer = RandomForestRegressor(max_features=combos[0][1][0], max_depth=combos[0][1][1], n_estimators=combos[0][1][2])\n",
    "    # imputer = RandomForestRegressor(max_features=11, max_depth=10, n_estimators=75)\n",
    "    imputer.fit(validData, validTarget)\n",
    "    imputedValues = imputer.predict(nullData)\n",
    "    # imputedValues = scalePredictedValues(dates, target, imputedValues, \n",
    "    #     [i for i in range(len(target)) if np.isnan(target[i])], combos[0][1][3])\n",
    "\n",
    "    # saving to a dataframe\n",
    "    predData = np.array((np.array(nullDates), imputedValues)).T\n",
    "    newDf = pd.DataFrame(predData, columns=[\"DateTime\", \"Predicted Ozark Groundwater Depth (ft)\"])\n",
    "    newDf.to_csv(\"predicted forest.csv\", index=False)\n",
    "\n",
    "\n",
    "def getCorrelationPerFeature(df, targetFeature):\n",
    "    targetArr = np.array(df[targetFeature])\n",
    "    targetNullLocations = np.nonzero(np.isnan(targetArr))[0]\n",
    "\n",
    "    correlationList = []\n",
    "    cols = [col for col in df.columns if col not in [\"DateTime\", targetFeature]]\n",
    "    for col in cols:\n",
    "        # gettiing column as array and null locations\n",
    "        arr = np.array(df[col])\n",
    "        arrNullLocations = np.nonzero(np.isnan(arr))[0]\n",
    "        allNullLocations = np.unique(np.append(targetNullLocations, arrNullLocations))\n",
    "\n",
    "        # removing null indicies\n",
    "        currentTargetArr = np.delete(targetArr, allNullLocations)\n",
    "        arr = np.delete(arr, allNullLocations)\n",
    "\n",
    "        # computing \n",
    "        correlationValue = distance_correlation(currentTargetArr, arr)\n",
    "        correlationList.append((col, correlationValue))\n",
    "\n",
    "    correlationList.sort(key=lambda a: a[1])\n",
    "    correlationList = correlationList[::-1]\n",
    "    # corrDf = pd.DataFrame(np.array(correlationList), columns = [\"Feature\", \"Correlation with Target\"])\n",
    "    # print(corrDf)\n",
    "    \n",
    "    return correlationList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# have a list of all features needing large gap imputation here\n",
    "\n",
    "# features = ['SWTP Plant 1 Influent Flow', 'Wilsons Gauge Height (ft)', 'SW_Peak_Flow', \n",
    "#     'SWTP Plant 1 Gravity Flow', 'James Gauge Height (ft)', 'HourlyWetBulbTemperature', 'HourlyStationPressure', 'HourlyWindSpeed']\n",
    "# features = ['HourlyStationPressure', 'HourlyWindSpeed']\n",
    "features = ['HourlyPressureChange']\n",
    "# features = [\"HourlyPressureTendency\"]\n",
    "\n",
    "for targetFeature in features:\n",
    "    df = df = pd.read_csv(\"Small Gap Imputed Data.csv\")\n",
    "    df.drop(columns = [\"DateTime\"])\n",
    "    correlationList = getCorrelationPerFeature(df, targetFeature)\n",
    "    topCorrelatedFeatures = [x[0] for x in correlationList[:20]] + [\"DateTime\", targetFeature]\n",
    "    df = pd.read_csv(\"Small Gap Imputed Data.csv\", usecols=topCorrelatedFeatures)\n",
    "    # df = pd.read_csv(\"Imputed Data.csv\", usecols=topCorrelatedFeatures)\n",
    "    print(\"Imputing:\", targetFeature)\n",
    "    tuneForest(df, targetFeature)\n",
    "    saveForestPredictions(\"Small Gap Imputed Data.csv\", \"predicted forest.csv\", targetFeature)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# results\n",
    "# ozark: best was 9 features, 10 max depth, 75 trees, and a scale factor of 0.1 in full grid search with mse of 0.9284514998810384\n",
    "# springfield: best was 9 features, 10 max depth, 75 trees, and a scale factor of 0.1 in full grid search with mse of 0.02750132169140654\n",
    "# swtp total: best was 9 features, max depth of 10, 75 trees, and a scale factor of 1 with special features with mse of 28.72129856053067\n",
    "# swtp plant 2: best was 9 features, max depth of 10, 75 trees, and a scale factor of 0.5 in full grid search with mse of 24.51435301094517\n",
    "# swtp plant 1: best was 7 features, max depth of 10, 100 trees, and a scale factor of 0.5 in full grid search with mse of 8.780532754986643 \n",
    "# wilson gauge: best was 7 features, max depth of 10, 75 trees, and a scale factor of 0.5 in full grid search with mse of 0.1307122486409281 \n",
    "# peak flow: best was 7 features, max depth of 10, 75 trees, and a scale factor of 0.5 in full grid search with mse of 3.966058622312969\n",
    "# swtp gravity: best was 5 features, max depth of 10, 75 trees, and a scale factor of 0.5 in full grid search with mse of 4.519902798066215\n",
    "# james gauge: best was 7 features, max depth of 10, 75 trees, and a scale factor of 1 in full grid search with mse of 0.4629773919971907\n",
    "# wet bulb: best was 11 features, max depth of 10, 100 trees, and a scale factor of 1 in full grid search with mse of 0.08290534402734484\n",
    "# station pressure: best was 11 features, max depth of 10, 75 trees, and a scale factor of 1 in full grid search with mse of 6.434824542627539e-06\n",
    "# wind speed: best was 9 features, max depth of 10, 100 trees, and a scale factor of 1 in full grid search with mse of 19.531978664613003\n",
    "# pressure tend: best was 11 features, max depth of 10, 75 trees, and a scale factor of 1 in full grid search with mse of 4.365107909935982\n",
    "# pressure change: best was 9 features, max depth of 10, 100 trees, and a scale factor of 1 in full grid search with mse of 0.0008534304525081505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Gap Validation and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def saveJsonFile(filename, obj):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def readJsonFile(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        obj = json.load(f)\n",
    "    return obj\n",
    "\n",
    "hyperparameters = [('HourlyWetBulbTemperature', (11, 10, 100, 1)), ('HourlyStationPressure', (11, 10, 75, 1)), \n",
    "    ('HourlyWindSpeed', (9, 10, 100, 1)), ('HourlyPressureChange', (9, 10, 100, 1)), (\"HourlyPressureTendency\", (11, 10, 75, 1)),\n",
    "    ('SW_Peak_Flow', (7, 10, 75, 0.5)), ('SWTP Plant 1 Gravity Flow', (5, 10, 75, 0.5)),\n",
    "    ('Wilsons Gauge Height (ft)', (7, 10, 75, 0.5)), ('James Gauge Height (ft)', (7, 10, 75, 1)),\n",
    "    (\"Springfield Plateau Aquifer Depth to Water Level (ft)\", (9, 10, 75, 0.1)), (\"Ozark Aquifer Depth to Water Level (ft)\", (9, 10, 75, 0.1)),\n",
    "    ('SWTP Plant 1 Influent Flow', (7, 10, 100, 0.5)), ('SWTP Plant 2 Influent Flow', (9, 10, 75, 0.5)), \n",
    "    ('SWTP Total Influent Flow', (9, 10, 75, 1))]\n",
    "\n",
    "validationResults = []\n",
    "for tup in hyperparameters:\n",
    "    # getting data\n",
    "    targetFeature = tup[0]\n",
    "    df = pd.read_csv(\"Small Gap Imputed Data.csv\")\n",
    "    correlationPath = \"Correlation Lists/\" + targetFeature + \" correlation list.json\"\n",
    "    if os.path.exists(correlationPath):\n",
    "        correlationList = readJsonFile(correlationPath)\n",
    "    else:\n",
    "        correlationList = getCorrelationPerFeature(df, targetFeature)\n",
    "        saveJsonFile(correlationPath, correlationList)\n",
    "    if targetFeature == \"SWTP Total Influent Flow\":\n",
    "        correlationList = [('SWTP Plant 2 Influent Flow', 0.9559068108777935), ('Wilsons Gauge Height (ft)', 0.8137102534536554), \n",
    "                            ('James Gauge Height (ft)', 0.7535434901158702), ('SWTP Plant 1 Influent Flow', 0.6894131379860265), \n",
    "                            ('Fire 168 Hour Rainfall Aggregate', 0.6761004366376544), ('AT&T 168 Hour Rainfall Aggregate', 0.672011459843413), \n",
    "                            ('Field 168 Hour Rainfall Aggregate', 0.6708505528284172), ('Springfield Plateau Aquifer Depth to Water Level (ft)', 0.6505778879978816),\n",
    "                            ('Ozark Aquifer Depth to Water Level (ft)', 0.5220396032101949), ('Month', 0.34707577687495317),\n",
    "                            ('Hour', 0.16784404944993966), ('Month', 0.34707577687495317), ('Week', 0.34260778783378193), ('Year', 0.3153056413507134)]\n",
    "    \n",
    "    addedFeats = 0\n",
    "    topCorrelatedFeatures = [x[0] for x in correlationList[:20]] + [\"DateTime\", targetFeature]\n",
    "    temporalFeatures = [\"Month\", \"Hour\", \"Week\", \"Year\"]\n",
    "    if \"SW\" in targetFeature:\n",
    "        for feat in temporalFeatures:\n",
    "            if feat not in topCorrelatedFeatures:\n",
    "                topCorrelatedFeatures.append(feat)\n",
    "        addedFeats = 1\n",
    "\n",
    "    # data preprocessing\n",
    "    targetArrDf = pd.read_csv(\"Small Gap Imputed Data.csv\", usecols=[targetFeature])\n",
    "    df = pd.read_csv(\"Small Gap Imputed Data.csv\", usecols=topCorrelatedFeatures)\n",
    "    target = np.array(targetArrDf[targetFeature])\n",
    "    dates = np.array(df[\"DateTime\"])\n",
    "\n",
    "    badFeaturesToUse = findSharedNullValueFeatures(df, targetFeature, 0)\n",
    "    badFeaturesToUse += [targetFeature, \"DateTime\"]\n",
    "    df = df.drop(columns=badFeaturesToUse)\n",
    "    data = df.to_numpy()                   # matrix of top correlated features that can be used to impute the target feature\n",
    "\n",
    "    # splitting data up into respective datasets\n",
    "    validData, validTarget, nullData, nullDates = separateDataIntoSets(target, data, dates)\n",
    "\n",
    "    # starting validation\n",
    "    avgMSE = 0\n",
    "    avgR2 = 0\n",
    "    avgNMSE = 0\n",
    "    numValidations = 10                     # an average of this many validations\n",
    "    currentValidations = 0\n",
    "    validationFailedAttempts = 0\n",
    "    while currentValidations < numValidations:\n",
    "        try:                                                        # there's an index error that pops up (rarely) that I don't want to fix\n",
    "                                                                    # it's from the linear trendline scaling null index selection\n",
    "            if validationFailedAttempts > numValidations * 5:\n",
    "                raise RuntimeError(\"Too many failed validation attempts\")\n",
    "            \n",
    "            # getting datasets\n",
    "            XTrain, XTest, YTrain, YTest, testIndicies = train_test_split_largeGap(validData, validTarget, target, test_size=0.20)\n",
    "\n",
    "            # random forest model and predicting\n",
    "            imputer = RandomForestRegressor(n_estimators=tup[1][2], max_depth=tup[1][1], max_features=tup[1][0]+addedFeats)\n",
    "            imputer.fit(XTrain, YTrain)\n",
    "            predictedValues = imputer.predict(XTest)\n",
    "\n",
    "            # scaling random forest predictions and evaluation of predictions\n",
    "            scaledPredictedValues = scalePredictedValues(dates, validTarget, predictedValues, testIndicies, tup[1][3])\n",
    "            normScaledPredictedValues = normalize(scaledPredictedValues, np.max(YTest), np.min(YTest))\n",
    "            normYTest = normalize(YTest, np.max(YTest), np.min(YTest))\n",
    "            avgMSE += mean_squared_error(YTest, scaledPredictedValues)\n",
    "            avgNMSE += mean_squared_error(normYTest, normScaledPredictedValues)\n",
    "            avgR2 += r2_score(YTest, scaledPredictedValues)\n",
    "            currentValidations += 1\n",
    "\n",
    "        except NotImplementedError:\n",
    "            print(\"Failed:\", validationFailedAttempts)\n",
    "            validationFailedAttempts += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "    # getting dates of all training data in order\n",
    "    targetNulls = np.nonzero(np.isnan(target))[0]\n",
    "    trainTestDates = np.delete(dates, targetNulls)          # list of all dates that have a non-null value being regressed\n",
    "    testDates = np.array([trainTestDates[i] for i in testIndicies])\n",
    "\n",
    "    # saving test data to a dataframe for future comparison\n",
    "    predData = np.array((np.array(testDates), scaledPredictedValues)).T\n",
    "    newDf = pd.DataFrame(predData, columns=[\"DateTime\", targetFeature])\n",
    "    newDf.to_csv(\"validated forest.csv\", index=False)\n",
    "    \n",
    "\n",
    "\n",
    "    # if needed, imputing missing data again and saving\n",
    "    imputer = RandomForestRegressor(n_estimators=tup[1][2], max_depth=tup[1][1], max_features=tup[1][0])\n",
    "    imputer.fit(validData, validTarget)\n",
    "    imputedValues = imputer.predict(nullData)\n",
    "    imputedValues = scalePredictedValues(dates, target, imputedValues, [i for i in range(len(target)) if np.isnan(target[i])], tup[1][3])\n",
    "\n",
    "    # saving missing value imputation results\n",
    "    imputedData = np.array((np.array(nullDates), imputedValues)).T\n",
    "    newDf = pd.DataFrame(imputedData, columns=[\"DateTime\", targetFeature])\n",
    "    newDf.to_csv(\"predicted forest.csv\", index=False)\n",
    "    saveForestPredictions(\"Small Gap Imputed Data.csv\", \"predicted forest.csv\", targetFeature)\n",
    "    \n",
    "    # copying imputation results over into New Imputed Data.csv\n",
    "    doesExist = os.path.exists(\"New Imputed Data.csv\")\n",
    "    if not doesExist:\n",
    "        print(\"Creating file\")\n",
    "        dataframe = pd.read_csv(\"Small Gap Imputed Data.csv\")\n",
    "        dataframe.to_csv(\"New Imputed Data.csv\", index=False)  # creating file since doesn't exist yet\n",
    "\n",
    "    newDf = pd.read_csv(targetFeature + \" imputed data.csv\")\n",
    "    imputedDf = pd.read_csv(\"New Imputed Data.csv\")\n",
    "    imputedDf[targetFeature] = newDf[targetFeature]\n",
    "    imputedDf.to_csv(\"New Imputed Data.csv\", index=False)\n",
    "    os.remove(targetFeature + \" imputed data.csv\")\n",
    "\n",
    "\n",
    "    # validation results\n",
    "    validationList = [targetFeature, avgMSE / numValidations, avgNMSE / numValidations, avgR2 / numValidations, tup[1]]\n",
    "    validationResults.append(validationList)\n",
    "    print(\"For feature:\", targetFeature)\n",
    "    print(\"MSE:\", validationList[1])\n",
    "    print(\"NMSE:\", validationList[2])\n",
    "    print(\"R2:\", validationList[3])\n",
    "    print()\n",
    "    print(\"Imputed:\", targetFeature)\n",
    "    print()\n",
    "\n",
    "saveJsonFile(\"Validation Results.json\", validationResults)\n",
    "\n",
    "if os.path.exists(\"predicted forest.csv\"):\n",
    "    os.remove(\"predicted forest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationResults = readJsonFile(\"Validation Results.json\")\n",
    "df = pd.DataFrame(validationResults, columns = [\"Feature\", \"MSE\", \"NMSE\", \"R^2\", \"Hyperparameters\"])\n",
    "df = df.sort_values(by = \"NMSE\", ascending=True)\n",
    "df.to_csv(\"Large Gap Validation.csv\", index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Null Gap and Imputation Visualization\n",
    "##### Uncomment the feature wanted to be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def makeNullRects(dates, y):\n",
    "    '''This function returns a list of matplotlib.patches.Rectangles where\n",
    "    np.nan values are present in the y array. If values are consecutive,\n",
    "    the rectangles will widen as needed.\n",
    "    Note that this function is made for a figure with an x-axis of dates\n",
    "    Input:\n",
    "        dates: x axis date time values\n",
    "        y: y axis range values as np.array, contains np.nan values\n",
    "\n",
    "    Returns:\n",
    "        list of matplotlib.patches.Rectangles located where\n",
    "        y has np.nan values.\n",
    "\n",
    "    Rectangle Parameters in function:\n",
    "        opacityCoeff: how solid rectangles appear\n",
    "        longRectColor: the color of the rectangles with >=7 width\n",
    "        shortRectColor: the color of the rectanges with <7 width\n",
    "    '''\n",
    "    # setting up rectangle parameters\n",
    "    opacityCoeff = 0.5\n",
    "    longRectColor = \"red\"\n",
    "    shortRectColor = \"magenta\"\n",
    "\n",
    "    # prep work for creating rectangles for nan values\n",
    "    index = 0\n",
    "    yMax = np.nanmax(y)\n",
    "    yMin = np.nanmin(y)\n",
    "    rectHeight = yMax - yMin\n",
    "    yRectCoor = yMin\n",
    "    allRects = []   # this is what will be returned\n",
    "\n",
    "    # creating rectangle patches\n",
    "    while index < len(y):\n",
    "\n",
    "        # if nan exists, then need to create a rectangle patch\n",
    "        if np.isnan(y[index]):\n",
    "            xRectCoorIndex = index - 1\n",
    "\n",
    "            # condition for if first y value is nan\n",
    "            if index == 0:\n",
    "                xRectCoorIndex += 1\n",
    "            \n",
    "            # condition for if last y value is nan, assumes y is not len 2\n",
    "            elif index + 1 == len(y):\n",
    "                xRectCoor = mdates.date2num(dates[xRectCoorIndex])\n",
    "                coords = (xRectCoor, yRectCoor)\n",
    "                width = mdates.date2num(dates[xRectCoorIndex + 1]) - mdates.date2num(dates[xRectCoorIndex])\n",
    "                allRects.append(mpatches.Rectangle(coords, width, rectHeight, color=shortRectColor, alpha=opacityCoeff))\n",
    "                break\n",
    "                \n",
    "            # all other cases\n",
    "            xRectCoor = mdates.date2num(dates[xRectCoorIndex])\n",
    "\n",
    "            # checking finding how long the rectangle needs to be--how many consecutive null values\n",
    "            index += 1\n",
    "            while np.isnan(y[index]):\n",
    "                index += 1\n",
    "            rightEdgeIndex = mdates.date2num(dates[index])\n",
    "\n",
    "            # making rectangle\n",
    "            coords = (xRectCoor, yRectCoor)\n",
    "            width = rightEdgeIndex - xRectCoor\n",
    "            color = shortRectColor\n",
    "            if index - xRectCoorIndex > 5:\n",
    "                color = longRectColor\n",
    "            allRects.append(mpatches.Rectangle(coords, width, rectHeight, color=color, alpha=opacityCoeff))\n",
    "\n",
    "        else:\n",
    "            index += 1\n",
    "\n",
    "    return allRects\n",
    "\n",
    "def visualizeMissingValues(dates, arr, fig, ax, wantToMakeNullRects = True):\n",
    "    '''This function plots an array of values with datetime x axis values onto\n",
    "    a given axis, showing patches of null values if present.\n",
    "\n",
    "    Input:\n",
    "        dates: a numpy array of datetime objs that are the x-axis for the array with missing data to plot\n",
    "        arr: a numpy array that has missing data\n",
    "        fig: a matplotlib figure that contains the axis with the plot\n",
    "        ax: a matplotlib axis that will be plotted upon\n",
    "\n",
    "    Returns:\n",
    "        fig: edited matplotlib figure\n",
    "        ax: edited matplotlib axis\n",
    "    '''\n",
    "    ax.plot(dates, arr)\n",
    "\n",
    "    if wantToMakeNullRects:\n",
    "        rects = makeNullRects(dates, arr)\n",
    "        for rect in rects:\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    formatter = mdates.ConciseDateFormatter(ax.xaxis.get_major_locator(), formats=[\"%Y\", \"%Y-%b\", \"%b-%d\", \"%d %H:%M\", \"%d %H:%M\", \"%H:%M\"])\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    return fig, ax\n",
    "\n",
    "def plotImputedData(dates, nullArr, imputedArr, ax):\n",
    "    '''This graph plots imputed data as a green dashed line on a given\n",
    "    matplotlib axis.\n",
    "\n",
    "    Input:\n",
    "        dates: a numpy array of datetime objs that are the x-axis for the array with missing data to plot\n",
    "        nullArr: a numpy array that has missing data\n",
    "        imputedArr: a numpy array that has some of the missing values imputed\n",
    "        ax: a matplotlib axis that will be plotted upon\n",
    "    \n",
    "    Returns:\n",
    "        ax: edited matplotlib axis\n",
    "    '''\n",
    "    index = 0\n",
    "    while index < len(nullArr):                                 # looping through arr since it has the null values\n",
    "        if np.isnan(nullArr[index]):\n",
    "            # getting the width of the null area\n",
    "            lenForward = 0\n",
    "            while np.isnan(nullArr[index + lenForward]):\n",
    "                lenForward += 1\n",
    "\n",
    "            # domain to plot is [index-1, index+lenforward]\n",
    "            domain = list(range(index-1, index+lenForward+1))\n",
    "            datesToPlot = [dates[i] for i in domain]\n",
    "            pointsToPlot = [imputedArr[i] for i in domain]\n",
    "            ax.plot(datesToPlot, pointsToPlot, \"g--\")       # green dashed line\n",
    "\n",
    "            # moving index forward past null gap\n",
    "            index += lenForward\n",
    "        else:\n",
    "            index += 1\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "%matplotlib qt\n",
    "\n",
    "feature = \"SWTP Total Influent Flow\"\n",
    "# feature = \"SWTP Plant 2 Influent Flow\"\n",
    "# feature = \"SW_Peak_Flow\"\n",
    "# feature = \"SWTP Plant 1 Gravity Flow\"\n",
    "# feature = \"Total 168 Hour Rainfall Aggregate\"\n",
    "# feature = \"Ozark Aquifer Depth to Water Level (ft)\"\n",
    "# feature = \"Springfield Plateau Aquifer Depth to Water Level (ft)\"\n",
    "# feature = \"James Gauge Height (ft)\"\n",
    "# feature = \"Wilsons Gauge Height (ft)\"\n",
    "# feature = \"Fire 168 Hour Rainfall Aggregate\"\n",
    "# feature = \"HourlyPressureChange\"\n",
    "\n",
    "df = pd.read_csv(\"Joined Influent and Rainfall and Weather and Groundwater and Creek Gauge.csv\", parse_dates=[\"DateTime\"])\n",
    "# df = pd.read_csv(\"Small Gap Imputed Data.csv\", parse_dates=[\"DateTime\"])\n",
    "# df = pd.read_csv(\"Small Gap Imputed Data Editted.csv\", parse_dates=[\"DateTime\"])\n",
    "df[\"SWTP Total Influent Flow\"] = np.array([np.nan if x < 3.7 else x for x in df[\"SWTP Total Influent Flow\"]])\n",
    "\n",
    "\n",
    "# imputedDf = pd.read_csv(\"Small Gap Imputed Data.csv\")\n",
    "# imputedDf = pd.read_csv(\"Small Gap Imputed Data Editted.csv\")\n",
    "# imputedDf = pd.read_csv(\"test.csv\")\n",
    "# imputedDf = pd.read_csv(\"Imputed Data.csv\")\n",
    "imputedDf = pd.read_csv(\"New Imputed Data.csv\")\n",
    "\n",
    "\n",
    "dates = np.array(df[\"DateTime\"])\n",
    "imputedArr = np.array(imputedDf[feature])\n",
    "nullArr = deepcopy(np.array(df[feature]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig, ax = visualizeMissingValues(dates, nullArr, fig, ax)\n",
    "ax = plotImputedData(dates, nullArr, imputedArr, ax)\n",
    "ax.set_ylabel(feature, fontsize=20)\n",
    "ax.set_title(\"Missing Data in the \" + str(feature), fontsize=25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b104c07bfaa35248860e50ba1cfb46ca5e69d0db6474176440e1466f68c081"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
